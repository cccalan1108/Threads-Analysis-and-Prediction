{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6sEhoTFDEH3"
   },
   "source": [
    "## 安裝套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2V9shIFDDG3J",
    "outputId": "72d8024f-64c8-41fc-8ca0-00a5bffe1574"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (0.42.1)\n",
      "Requirement already satisfied: emoji in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (2.14.1)\n",
      "Requirement already satisfied: langdetect in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: datasets in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (2.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (0.15.1)\n",
      "Requirement already satisfied: packaging in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: responses<0.19 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.0)\n",
      "Requirement already satisfied: filelock in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2024.12.14)\n",
      "Requirement already satisfied: six in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: lingua-language-detector in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba\n",
    "!pip install emoji langdetect\n",
    "!pip install datasets\n",
    "!pip install lingua-language-detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhtPvgrLCUzp",
    "outputId": "1ecc2382-688c-437a-bc60-6b7a4ea45862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.42.1)\n",
      "Requirement already satisfied: emoji in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.14.1)\n",
      "Requirement already satisfied: langdetect in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.0.9)\n",
      "Requirement already satisfied: pytz in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2024.1)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: lingua-language-detector in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.1.5)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.31.1)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install jieba emoji langdetect pytz torch lingua-language-detector datasets openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HGttCcMDKE0"
   },
   "source": [
    "## 引入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JQ6rzOpBTI5K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "import langdetect\n",
    "from langdetect import detect\n",
    "from lingua import LanguageDetectorBuilder, Language, IsoCode639_1\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zM7PgPNTPXMd",
    "outputId": "ca72f2aa-3135-4a1f-857e-ea5f956efe1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 設定好路徑 (後面都是使用相對路徑)\n",
    "base_path = '/content/drive/My Drive/SMA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "xOIyJJ3KSNT9",
    "outputId": "e38e0262-6047-4c2a-c225-454cac6940d1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>post_time</th>\n",
       "      <th>topic</th>\n",
       "      <th>time_info</th>\n",
       "      <th>content</th>\n",
       "      <th>has_photo</th>\n",
       "      <th>has_video</th>\n",
       "      <th>like_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>repost_count</th>\n",
       "      <th>share_count</th>\n",
       "      <th>view_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>post_url</th>\n",
       "      <th>scrape_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>2025-04-29T10:58:39.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3小時</td>\n",
       "      <td>Thank you God for another day.</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>190</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3,197</td>\n",
       "      <td>141073</td>\n",
       "      <td>https://www.threads.net/@ayofvr/post/DJBymf8uTrK</td>\n",
       "      <td>2025-04-29T22:27:40.176749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-04-29T12:33:44.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1小時</td>\n",
       "      <td>百達翡麗？ 沒有下限的網路病態！</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>196</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>3 萬</td>\n",
       "      <td>77683</td>\n",
       "      <td>https://www.threads.net/@ban.mei.onnnnni/post/...</td>\n",
       "      <td>2025-04-29T22:27:54.964788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ribboworld2021</td>\n",
       "      <td>2025-04-29T04:39:46.000Z</td>\n",
       "      <td>小一日常</td>\n",
       "      <td>9小時</td>\n",
       "      <td>考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>75</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4,559</td>\n",
       "      <td>99</td>\n",
       "      <td>https://www.threads.net/@ribboworld2021/post/D...</td>\n",
       "      <td>2025-04-29T22:28:09.873641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>2025-04-29T11:25:22.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3小時</td>\n",
       "      <td>Just be strong. Confident. Hopeful. Intellectu...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>83</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1,967</td>\n",
       "      <td>141093</td>\n",
       "      <td>https://www.threads.net/@ayofvr/post/DJB1qP5OmzP</td>\n",
       "      <td>2025-04-29T22:28:24.726576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jose_ykc</td>\n",
       "      <td>2025-04-29T10:36:19.000Z</td>\n",
       "      <td>輔仁大學</td>\n",
       "      <td>3小時</td>\n",
       "      <td>日文輔系老師上課內容之一AiScReam 歌詞導讀</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>4,334</td>\n",
       "      <td>55</td>\n",
       "      <td>513</td>\n",
       "      <td>2,460</td>\n",
       "      <td>10 萬</td>\n",
       "      <td>65</td>\n",
       "      <td>https://www.threads.net/@jose_ykc/post/DJBvpGI...</td>\n",
       "      <td>2025-04-29T22:28:39.393706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>leighton.williams</td>\n",
       "      <td>2025年04月29日 07:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8小時</td>\n",
       "      <td>First tasting in California</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>726</td>\n",
       "      <td>30</td>\n",
       "      <td>https://www.threads.net/@leighton.williams/pos...</td>\n",
       "      <td>2025-04-29T16:23:40.328046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>cape__man</td>\n",
       "      <td>2025年04月29日 06:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9小時</td>\n",
       "      <td>I hoped it would have been better.</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1,291</td>\n",
       "      <td>114</td>\n",
       "      <td>https://www.threads.net/@cape__man/post/DJAdFd...</td>\n",
       "      <td>2025-04-29T16:23:55.063517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>simimoonlight</td>\n",
       "      <td>2025年04月29日 06:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9小時</td>\n",
       "      <td>I can’t wait to watch Beyoncé on TikTok tonigh...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>2,656</td>\n",
       "      <td>29</td>\n",
       "      <td>280</td>\n",
       "      <td>47</td>\n",
       "      <td>5.6 萬</td>\n",
       "      <td>65732</td>\n",
       "      <td>https://www.threads.net/@simimoonlight/post/DJ...</td>\n",
       "      <td>2025-04-29T16:24:39.870994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>other98</td>\n",
       "      <td>2025年04月29日 12:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3小時</td>\n",
       "      <td>Tonight, Canada just proved that they have a h...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0 萬</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>22</td>\n",
       "      <td>14 萬</td>\n",
       "      <td>217182</td>\n",
       "      <td>https://www.threads.net/@other98/post/DJBGV3NxiX_</td>\n",
       "      <td>2025-04-29T16:24:54.669936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>scottiebeam</td>\n",
       "      <td>2025年04月29日 01:33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14小時</td>\n",
       "      <td>Yall be fighting on here … i thought threads w...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>427</td>\n",
       "      <td>76</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1 萬</td>\n",
       "      <td>59249</td>\n",
       "      <td>https://www.threads.net/@scottiebeam/post/DI_6...</td>\n",
       "      <td>2025-04-29T16:25:09.396585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3101 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                 post_time topic time_info  \\\n",
       "0                ayofvr  2025-04-29T10:58:39.000Z   NaN       3小時   \n",
       "1                   NaN  2025-04-29T12:33:44.000Z   NaN       1小時   \n",
       "2        ribboworld2021  2025-04-29T04:39:46.000Z  小一日常       9小時   \n",
       "3                ayofvr  2025-04-29T11:25:22.000Z   NaN       3小時   \n",
       "4              jose_ykc  2025-04-29T10:36:19.000Z  輔仁大學       3小時   \n",
       "...                 ...                       ...   ...       ...   \n",
       "3096  leighton.williams         2025年04月29日 07:30   NaN       8小時   \n",
       "3097          cape__man         2025年04月29日 06:31   NaN       9小時   \n",
       "3098      simimoonlight         2025年04月29日 06:26   NaN       9小時   \n",
       "3099            other98         2025年04月29日 12:31   NaN       3小時   \n",
       "3100        scottiebeam         2025年04月29日 01:33   NaN      14小時   \n",
       "\n",
       "                                                content has_photo has_video  \\\n",
       "0                        Thank you God for another day.         N         N   \n",
       "1                                      百達翡麗？ 沒有下限的網路病態！         Y         N   \n",
       "2              考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」         N         N   \n",
       "3     Just be strong. Confident. Hopeful. Intellectu...         N         N   \n",
       "4                             日文輔系老師上課內容之一AiScReam 歌詞導讀         Y         Y   \n",
       "...                                                 ...       ...       ...   \n",
       "3096                        First tasting in California         N         N   \n",
       "3097                 I hoped it would have been better.         N         N   \n",
       "3098  I can’t wait to watch Beyoncé on TikTok tonigh...         N         N   \n",
       "3099  Tonight, Canada just proved that they have a h...         N         N   \n",
       "3100  Yall be fighting on here … i thought threads w...         N         N   \n",
       "\n",
       "     like_count reply_count repost_count share_count view_count  \\\n",
       "0           190           3           23         NaN      3,197   \n",
       "1           196          16          NaN           6        3 萬   \n",
       "2            75           6          NaN         NaN      4,559   \n",
       "3            83           5            3           1      1,967   \n",
       "4         4,334          55          513       2,460       10 萬   \n",
       "...         ...         ...          ...         ...        ...   \n",
       "3096          7         NaN          NaN           0        726   \n",
       "3097          2         NaN          NaN           0      1,291   \n",
       "3098      2,656          29          280          47      5.6 萬   \n",
       "3099      1.0 萬         214          214          22       14 萬   \n",
       "3100        427          76           28           1        1 萬   \n",
       "\n",
       "      followers_count                                           post_url  \\\n",
       "0              141073   https://www.threads.net/@ayofvr/post/DJBymf8uTrK   \n",
       "1               77683  https://www.threads.net/@ban.mei.onnnnni/post/...   \n",
       "2                  99  https://www.threads.net/@ribboworld2021/post/D...   \n",
       "3              141093   https://www.threads.net/@ayofvr/post/DJB1qP5OmzP   \n",
       "4                  65  https://www.threads.net/@jose_ykc/post/DJBvpGI...   \n",
       "...               ...                                                ...   \n",
       "3096               30  https://www.threads.net/@leighton.williams/pos...   \n",
       "3097              114  https://www.threads.net/@cape__man/post/DJAdFd...   \n",
       "3098            65732  https://www.threads.net/@simimoonlight/post/DJ...   \n",
       "3099           217182  https://www.threads.net/@other98/post/DJBGV3NxiX_   \n",
       "3100            59249  https://www.threads.net/@scottiebeam/post/DI_6...   \n",
       "\n",
       "                     scrape_time  \n",
       "0     2025-04-29T22:27:40.176749  \n",
       "1     2025-04-29T22:27:54.964788  \n",
       "2     2025-04-29T22:28:09.873641  \n",
       "3     2025-04-29T22:28:24.726576  \n",
       "4     2025-04-29T22:28:39.393706  \n",
       "...                          ...  \n",
       "3096  2025-04-29T16:23:40.328046  \n",
       "3097  2025-04-29T16:23:55.063517  \n",
       "3098  2025-04-29T16:24:39.870994  \n",
       "3099  2025-04-29T16:24:54.669936  \n",
       "3100  2025-04-29T16:25:09.396585  \n",
       "\n",
       "[3101 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 讀取資料（請確認你的 Excel 路徑）\n",
    "# df = pd.read_excel(base_path+\"/threads.xlsx\")\n",
    "df = pd.read_excel(\"threads.xlsx\", engine='openpyxl')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kIM9eoPtiDSC"
   },
   "outputs": [],
   "source": [
    "# === 語言偵測修正版===\n",
    "lingua_detector = LanguageDetectorBuilder.from_all_languages().with_preloaded_language_models().build()\n",
    "lingua_available = True\n",
    "def detect_lang_with_preprocessing_lingua(text):\n",
    "    original_text = text\n",
    "\n",
    "    # 若是 NaN 或空字串就回傳 \"unknown\"\n",
    "    if pd.isna(text):\n",
    "        return \"unknown\"\n",
    "    text = str(text).strip()\n",
    "    if not text:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    # 移除 URL、@標記、 #hashtag、emoji、多餘空白\n",
    "    try:\n",
    "      text_cleaned = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "      text_cleaned = re.sub(r'@\\w+', '', text_cleaned)\n",
    "      text_cleaned = re.sub(r'#\\w+', '', text_cleaned)\n",
    "      text_cleaned = emoji.replace_emoji(text_cleaned, replace='')\n",
    "      text_cleaned = re.sub(r'\\s+', ' ', text_cleaned).strip()\n",
    "    except Exception as e:\n",
    "      return \"error_state_preprocessing\"\n",
    "\n",
    "    # 若這些清理完後變成空字串\n",
    "    if not text_cleaned:\n",
    "      return \"empty_after_clean\"\n",
    "\n",
    "    # 若文字中超過 30% 是中文，就直接判定為 \"Ch\"（中文）\n",
    "    try:\n",
    "      chinese_chars = re.findall(r'[\\u4e00-\\u9fff]', text_cleaned)\n",
    "      text_len = len(text_cleaned)\n",
    "      ratio = len(chinese_chars) / max(text_len, 1)\n",
    "      chinese_threshold = 0.3\n",
    "      if ratio > chinese_threshold:\n",
    "        return \"Ch\"\n",
    "\n",
    "      # 呼叫 lingua 偵測語言\n",
    "      detected_language = lingua_detector.detect_language_of(text_cleaned)\n",
    "\n",
    "      # 若 lingua 判定是中文（'ZH'），則回傳 \"Ch\"，其餘語言以小寫的 ISO 639-1 回傳（如 en, ja, fr）\n",
    "      # 若無法偵測出語言，回傳 \"unknown\"\n",
    "      if detected_language is not None:\n",
    "        iso_code = detected_language.iso_code_639_1.name\n",
    "        if iso_code == 'ZH':\n",
    "          return \"Ch\"\n",
    "        else:\n",
    "          return iso_code.lower()\n",
    "      else:\n",
    "        return \"unknown\"\n",
    "\n",
    "    except Exception as e:\n",
    "      return \"unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_nqIl5HD2ia"
   },
   "source": [
    "## 清洗數據V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LsfgBIPFrsc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 處理完成，已輸出 threads_cleaned_v1.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>post_time</th>\n",
       "      <th>topic</th>\n",
       "      <th>time_info</th>\n",
       "      <th>content</th>\n",
       "      <th>has_photo</th>\n",
       "      <th>has_video</th>\n",
       "      <th>like_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>repost_count</th>\n",
       "      <th>...</th>\n",
       "      <th>scrape_time</th>\n",
       "      <th>emojis</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>scrape_time_origin</th>\n",
       "      <th>post_weekday</th>\n",
       "      <th>post_hour</th>\n",
       "      <th>viral</th>\n",
       "      <th>has_question</th>\n",
       "      <th>has_exclaim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>2025-04-29T10:58:39.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3小時</td>\n",
       "      <td>Thank you God for another day.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>190</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>2025年04月30日 06:27</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 06:27:40.176749+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-04-29T12:33:44.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1小時</td>\n",
       "      <td>百達翡麗？ 沒有下限的網路病態！</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>196</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2025年04月30日 06:27</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025-04-30 06:27:54.964788+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ribboworld2021</td>\n",
       "      <td>2025-04-29T04:39:46.000Z</td>\n",
       "      <td>小一日常</td>\n",
       "      <td>9小時</td>\n",
       "      <td>考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>75</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025-04-30 06:28:09.873641+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>2025-04-29T11:25:22.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3小時</td>\n",
       "      <td>Just be strong. Confident. Hopeful. Intellectu...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>83</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 06:28:24.726576+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jose_ykc</td>\n",
       "      <td>2025-04-29T10:36:19.000Z</td>\n",
       "      <td>輔仁大學</td>\n",
       "      <td>3小時</td>\n",
       "      <td>日文輔系老師上課內容之一AiScReam 歌詞導讀</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4334</td>\n",
       "      <td>55</td>\n",
       "      <td>513</td>\n",
       "      <td>...</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025-04-30 06:28:39.393706+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>leighton.williams</td>\n",
       "      <td>2025年04月29日 07:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8小時</td>\n",
       "      <td>First tasting in California</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2025年04月30日 00:23</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 00:23:40.328046+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>cape__man</td>\n",
       "      <td>2025年04月29日 06:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9小時</td>\n",
       "      <td>I hoped it would have been better.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2025年04月30日 00:23</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 00:23:55.063517+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>simimoonlight</td>\n",
       "      <td>2025年04月29日 06:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9小時</td>\n",
       "      <td>I can’t wait to watch Beyoncé on TikTok tonigh...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2656</td>\n",
       "      <td>29</td>\n",
       "      <td>280</td>\n",
       "      <td>...</td>\n",
       "      <td>2025年04月30日 00:24</td>\n",
       "      <td>🫶🏿🥹</td>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 00:24:39.870994+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>other98</td>\n",
       "      <td>2025年04月29日 12:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3小時</td>\n",
       "      <td>Tonight, Canada just proved that they have a h...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>10000</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>...</td>\n",
       "      <td>2025年04月30日 00:24</td>\n",
       "      <td>🙌</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 00:24:54.669936+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>scottiebeam</td>\n",
       "      <td>2025年04月29日 01:33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14小時</td>\n",
       "      <td>Yall be fighting on here … i thought threads w...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>427</td>\n",
       "      <td>76</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>2025年04月30日 00:25</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 00:25:09.396585+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3101 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                 post_time topic time_info  \\\n",
       "0                ayofvr  2025-04-29T10:58:39.000Z   NaN       3小時   \n",
       "1                   NaN  2025-04-29T12:33:44.000Z   NaN       1小時   \n",
       "2        ribboworld2021  2025-04-29T04:39:46.000Z  小一日常       9小時   \n",
       "3                ayofvr  2025-04-29T11:25:22.000Z   NaN       3小時   \n",
       "4              jose_ykc  2025-04-29T10:36:19.000Z  輔仁大學       3小時   \n",
       "...                 ...                       ...   ...       ...   \n",
       "3096  leighton.williams         2025年04月29日 07:30   NaN       8小時   \n",
       "3097          cape__man         2025年04月29日 06:31   NaN       9小時   \n",
       "3098      simimoonlight         2025年04月29日 06:26   NaN       9小時   \n",
       "3099            other98         2025年04月29日 12:31   NaN       3小時   \n",
       "3100        scottiebeam         2025年04月29日 01:33   NaN      14小時   \n",
       "\n",
       "                                                content  has_photo  has_video  \\\n",
       "0                        Thank you God for another day.      False      False   \n",
       "1                                      百達翡麗？ 沒有下限的網路病態！       True      False   \n",
       "2              考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」      False      False   \n",
       "3     Just be strong. Confident. Hopeful. Intellectu...      False      False   \n",
       "4                             日文輔系老師上課內容之一AiScReam 歌詞導讀       True       True   \n",
       "...                                                 ...        ...        ...   \n",
       "3096                        First tasting in California      False      False   \n",
       "3097                 I hoped it would have been better.      False      False   \n",
       "3098  I can’t wait to watch Beyoncé on TikTok tonigh...      False      False   \n",
       "3099  Tonight, Canada just proved that they have a h...      False      False   \n",
       "3100  Yall be fighting on here … i thought threads w...      False      False   \n",
       "\n",
       "      like_count  reply_count  repost_count  ...        scrape_time  emojis  \\\n",
       "0            190            3            23  ...  2025年04月30日 06:27           \n",
       "1            196           16             0  ...  2025年04月30日 06:27           \n",
       "2             75            6             0  ...  2025年04月30日 06:28           \n",
       "3             83            5             3  ...  2025年04月30日 06:28           \n",
       "4           4334           55           513  ...  2025年04月30日 06:28           \n",
       "...          ...          ...           ...  ...                ...     ...   \n",
       "3096           7            0             0  ...  2025年04月30日 00:23           \n",
       "3097           2            0             0  ...  2025年04月30日 00:23           \n",
       "3098        2656           29           280  ...  2025年04月30日 00:24     🫶🏿🥹   \n",
       "3099       10000          214           214  ...  2025年04月30日 00:24       🙌   \n",
       "3100         427           76            28  ...  2025年04月30日 00:25           \n",
       "\n",
       "      emoji_count lang               scrape_time_origin post_weekday  \\\n",
       "0               0   en 2025-04-30 06:27:40.176749+08:00    Wednesday   \n",
       "1               0   Ch 2025-04-30 06:27:54.964788+08:00    Wednesday   \n",
       "2               0   Ch 2025-04-30 06:28:09.873641+08:00    Wednesday   \n",
       "3               0   en 2025-04-30 06:28:24.726576+08:00    Wednesday   \n",
       "4               0   Ch 2025-04-30 06:28:39.393706+08:00    Wednesday   \n",
       "...           ...  ...                              ...          ...   \n",
       "3096            0   en 2025-04-30 00:23:40.328046+08:00    Wednesday   \n",
       "3097            0   en 2025-04-30 00:23:55.063517+08:00    Wednesday   \n",
       "3098            3   en 2025-04-30 00:24:39.870994+08:00    Wednesday   \n",
       "3099            1   en 2025-04-30 00:24:54.669936+08:00    Wednesday   \n",
       "3100            0   en 2025-04-30 00:25:09.396585+08:00    Wednesday   \n",
       "\n",
       "      post_hour viral has_question has_exclaim  \n",
       "0             6     0        False       False  \n",
       "1             6     1         True        True  \n",
       "2             6     0        False        True  \n",
       "3             6     0        False       False  \n",
       "4             6     1        False       False  \n",
       "...         ...   ...          ...         ...  \n",
       "3096          0     0        False       False  \n",
       "3097          0     0        False       False  \n",
       "3098          0     1        False       False  \n",
       "3099          0     1        False        True  \n",
       "3100          0     1        False       False  \n",
       "\n",
       "[3101 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === 數值欄位清洗（萬字、逗號格式處理）===\n",
    "def parse_count(value):\n",
    "    # 將文字數字（如 \"1,234\"、\"2.5萬\"）統一轉為整數（int）\n",
    "    if pd.isna(value): return 0\n",
    "    value = str(value).replace(\",\", \"\")\n",
    "    # \"萬\" 的部分會乘上 10,000 做轉換\n",
    "    # 無法處理的格式就回傳 0\n",
    "    if \"萬\" in value:\n",
    "        return int(float(value.replace(\"萬\", \"\")) * 10000)\n",
    "    try:\n",
    "        return int(float(value))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "for col in [\"like_count\", \"view_count\", \"share_count\", \"repost_count\", \"reply_count\"]:\n",
    "    df[col] = df[col].apply(parse_count)\n",
    "\n",
    "# === 布林欄位處理 ===\n",
    "# 將原始欄位（Y/N）轉換為 True/False\n",
    "# 處理過程會去除空白、轉成大寫\n",
    "df[\"has_photo\"] = df[\"has_photo\"].apply(lambda x: str(x).strip().upper() == \"Y\")\n",
    "df[\"has_video\"] = df[\"has_video\"].apply(lambda x: str(x).strip().upper() == \"Y\")\n",
    "\n",
    "# === emoji 萃取與統計 ===\n",
    "# 檢查是否為文字型別，如果是文字，從中萃取出所有 emoji 字元並串接成字串回傳\n",
    "def extract_emojis(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return \"\".join([ch for ch in text if ch in emoji.EMOJI_DATA])\n",
    "\n",
    "df[\"emojis\"] = df[\"content\"].apply(extract_emojis)\n",
    "df[\"emoji_count\"] = df[\"emojis\"].apply(len)\n",
    "\n",
    "# # === 語言偵測修正版===\n",
    "# def detect_lang_custom(text):\n",
    "#     try:\n",
    "#         text = str(text)\n",
    "#         chinese_chars = re.findall(r'[\\u4e00-\\u9fff]', text)\n",
    "#         if len(chinese_chars) / max(len(text), 1) > 0.3:\n",
    "#             return \"Ch\"\n",
    "#         return detect(text)\n",
    "#     except:\n",
    "#         return \"unknown\"\n",
    "\n",
    "# 使用先前定義好的語言偵測函數 detect_lang_with_preprocessing_lingua()，處理每篇文章的語言判定\n",
    "df[\"lang\"] = df[\"content\"].apply(detect_lang_with_preprocessing_lingua)\n",
    "\n",
    "# === scrape_time 處理（轉換時區 + 抽取星期與小時）===\n",
    "# 將時間欄位轉為台北時區，額外抽出格式化後的時間字串、星期幾、小時(0–23）\n",
    "df[\"scrape_time_origin\"] = pd.to_datetime(df[\"scrape_time\"], utc=True).dt.tz_convert(\"Asia/Taipei\")\n",
    "df[\"scrape_time\"]  = df[\"scrape_time_origin\"].dt.strftime(\"%Y年%m月%d日 %H:%M\")\n",
    "df[\"post_weekday\"] = df[\"scrape_time_origin\"].dt.day_name()\n",
    "df[\"post_hour\"] = df[\"scrape_time_origin\"].dt.hour\n",
    "\n",
    "# === 是否為高流量文章（破萬）===\n",
    "# 超過等於 10,000 瀏覽為 1，其餘為 0\n",
    "df[\"viral\"] = (df[\"view_count\"] >= 10000).astype(int)\n",
    "\n",
    "# === 是否使用問號、驚嘆號 ===\n",
    "df[\"has_question\"] = df[\"content\"].apply(lambda x: \"？\" in str(x) or \"?\" in str(x))\n",
    "df[\"has_exclaim\"] = df[\"content\"].apply(lambda x: \"！\" in str(x) or \"!\" in str(x))\n",
    "\n",
    "# === 儲存結果 ===\n",
    "df.to_csv(\"threads_cleaned_v1.csv\",encoding='utf_8_sig', index=False)\n",
    "print(\"✅ 處理完成，已輸出 threads_cleaned_v1.csv\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTVtegcwEfb1"
   },
   "source": [
    "## 清洗數據V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_a9vHZ0p9o2u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 處理完成，已輸出 threads_cleaned_v2.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>content_length</th>\n",
       "      <th>lang</th>\n",
       "      <th>scrape_time</th>\n",
       "      <th>post_weekday</th>\n",
       "      <th>post_hour</th>\n",
       "      <th>post_period</th>\n",
       "      <th>viral</th>\n",
       "      <th>post_time</th>\n",
       "      <th>...</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>post_url</th>\n",
       "      <th>emojis</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>scrape_time_origin</th>\n",
       "      <th>has_question</th>\n",
       "      <th>has_exclaim</th>\n",
       "      <th>has_url</th>\n",
       "      <th>has_mention</th>\n",
       "      <th>has_hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>Thank you God for another day.</td>\n",
       "      <td>30</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 06:27</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T10:58:39.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>141073</td>\n",
       "      <td>https://www.threads.net/@ayofvr/post/DJBymf8uTrK</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:27:40.176749+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>百達翡麗？ 沒有下限的網路病態！</td>\n",
       "      <td>16</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 06:27</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-29T12:33:44.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>77683</td>\n",
       "      <td>https://www.threads.net/@ban.mei.onnnnni/post/...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:27:54.964788+08:00</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ribboworld2021</td>\n",
       "      <td>考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」</td>\n",
       "      <td>40</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T04:39:46.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>https://www.threads.net/@ribboworld2021/post/D...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:09.873641+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>Just be strong. Confident. Hopeful. Intellectu...</td>\n",
       "      <td>69</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T11:25:22.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>141093</td>\n",
       "      <td>https://www.threads.net/@ayofvr/post/DJB1qP5OmzP</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:24.726576+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jose_ykc</td>\n",
       "      <td>日文輔系老師上課內容之一AiScReam 歌詞導讀</td>\n",
       "      <td>25</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-29T10:36:19.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>https://www.threads.net/@jose_ykc/post/DJBvpGI...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:39.393706+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>leighton.williams</td>\n",
       "      <td>First tasting in California</td>\n",
       "      <td>27</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:23</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025年04月29日 07:30</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>https://www.threads.net/@leighton.williams/pos...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:23:40.328046+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>cape__man</td>\n",
       "      <td>I hoped it would have been better.</td>\n",
       "      <td>34</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:23</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025年04月29日 06:31</td>\n",
       "      <td>...</td>\n",
       "      <td>114</td>\n",
       "      <td>https://www.threads.net/@cape__man/post/DJAdFd...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:23:55.063517+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>simimoonlight</td>\n",
       "      <td>I can’t wait to watch Beyoncé on TikTok tonigh...</td>\n",
       "      <td>51</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:24</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025年04月29日 06:26</td>\n",
       "      <td>...</td>\n",
       "      <td>65732</td>\n",
       "      <td>https://www.threads.net/@simimoonlight/post/DJ...</td>\n",
       "      <td>🫶🏿🥹</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-04-30 00:24:39.870994+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>other98</td>\n",
       "      <td>Tonight, Canada just proved that they have a h...</td>\n",
       "      <td>77</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:24</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025年04月29日 12:31</td>\n",
       "      <td>...</td>\n",
       "      <td>217182</td>\n",
       "      <td>https://www.threads.net/@other98/post/DJBGV3NxiX_</td>\n",
       "      <td>🙌</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-30 00:24:54.669936+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>scottiebeam</td>\n",
       "      <td>Yall be fighting on here … i thought threads w...</td>\n",
       "      <td>57</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:25</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025年04月29日 01:33</td>\n",
       "      <td>...</td>\n",
       "      <td>59249</td>\n",
       "      <td>https://www.threads.net/@scottiebeam/post/DI_6...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:25:09.396585+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3101 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                                            content  \\\n",
       "0                ayofvr                     Thank you God for another day.   \n",
       "1                   NaN                                   百達翡麗？ 沒有下限的網路病態！   \n",
       "2        ribboworld2021           考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」   \n",
       "3                ayofvr  Just be strong. Confident. Hopeful. Intellectu...   \n",
       "4              jose_ykc                          日文輔系老師上課內容之一AiScReam 歌詞導讀   \n",
       "...                 ...                                                ...   \n",
       "3096  leighton.williams                        First tasting in California   \n",
       "3097          cape__man                 I hoped it would have been better.   \n",
       "3098      simimoonlight  I can’t wait to watch Beyoncé on TikTok tonigh...   \n",
       "3099            other98  Tonight, Canada just proved that they have a h...   \n",
       "3100        scottiebeam  Yall be fighting on here … i thought threads w...   \n",
       "\n",
       "      content_length lang        scrape_time post_weekday  post_hour  \\\n",
       "0                 30   en  2025年04月30日 06:27    Wednesday          6   \n",
       "1                 16   Ch  2025年04月30日 06:27    Wednesday          6   \n",
       "2                 40   Ch  2025年04月30日 06:28    Wednesday          6   \n",
       "3                 69   en  2025年04月30日 06:28    Wednesday          6   \n",
       "4                 25   Ch  2025年04月30日 06:28    Wednesday          6   \n",
       "...              ...  ...                ...          ...        ...   \n",
       "3096              27   en  2025年04月30日 00:23    Wednesday          0   \n",
       "3097              34   en  2025年04月30日 00:23    Wednesday          0   \n",
       "3098              51   en  2025年04月30日 00:24    Wednesday          0   \n",
       "3099              77   en  2025年04月30日 00:24    Wednesday          0   \n",
       "3100              57   en  2025年04月30日 00:25    Wednesday          0   \n",
       "\n",
       "     post_period  viral                 post_time  ... followers_count  \\\n",
       "0        morning      0  2025-04-29T10:58:39.000Z  ...          141073   \n",
       "1        morning      1  2025-04-29T12:33:44.000Z  ...           77683   \n",
       "2        morning      0  2025-04-29T04:39:46.000Z  ...              99   \n",
       "3        morning      0  2025-04-29T11:25:22.000Z  ...          141093   \n",
       "4        morning      1  2025-04-29T10:36:19.000Z  ...              65   \n",
       "...          ...    ...                       ...  ...             ...   \n",
       "3096       night      0         2025年04月29日 07:30  ...              30   \n",
       "3097       night      0         2025年04月29日 06:31  ...             114   \n",
       "3098       night      1         2025年04月29日 06:26  ...           65732   \n",
       "3099       night      1         2025年04月29日 12:31  ...          217182   \n",
       "3100       night      1         2025年04月29日 01:33  ...           59249   \n",
       "\n",
       "                                               post_url  emojis  emoji_count  \\\n",
       "0      https://www.threads.net/@ayofvr/post/DJBymf8uTrK                    0   \n",
       "1     https://www.threads.net/@ban.mei.onnnnni/post/...                    0   \n",
       "2     https://www.threads.net/@ribboworld2021/post/D...                    0   \n",
       "3      https://www.threads.net/@ayofvr/post/DJB1qP5OmzP                    0   \n",
       "4     https://www.threads.net/@jose_ykc/post/DJBvpGI...                    0   \n",
       "...                                                 ...     ...          ...   \n",
       "3096  https://www.threads.net/@leighton.williams/pos...                    0   \n",
       "3097  https://www.threads.net/@cape__man/post/DJAdFd...                    0   \n",
       "3098  https://www.threads.net/@simimoonlight/post/DJ...     🫶🏿🥹            3   \n",
       "3099  https://www.threads.net/@other98/post/DJBGV3NxiX_       🙌            1   \n",
       "3100  https://www.threads.net/@scottiebeam/post/DI_6...                    0   \n",
       "\n",
       "                   scrape_time_origin  has_question  has_exclaim  has_url  \\\n",
       "0    2025-04-30 06:27:40.176749+08:00         False        False    False   \n",
       "1    2025-04-30 06:27:54.964788+08:00          True         True    False   \n",
       "2    2025-04-30 06:28:09.873641+08:00         False         True    False   \n",
       "3    2025-04-30 06:28:24.726576+08:00         False        False    False   \n",
       "4    2025-04-30 06:28:39.393706+08:00         False        False    False   \n",
       "...                               ...           ...          ...      ...   \n",
       "3096 2025-04-30 00:23:40.328046+08:00         False        False    False   \n",
       "3097 2025-04-30 00:23:55.063517+08:00         False        False    False   \n",
       "3098 2025-04-30 00:24:39.870994+08:00         False        False    False   \n",
       "3099 2025-04-30 00:24:54.669936+08:00         False         True    False   \n",
       "3100 2025-04-30 00:25:09.396585+08:00         False        False    False   \n",
       "\n",
       "      has_mention  has_hashtag  \n",
       "0           False        False  \n",
       "1           False        False  \n",
       "2           False        False  \n",
       "3           False        False  \n",
       "4           False        False  \n",
       "...           ...          ...  \n",
       "3096        False        False  \n",
       "3097        False        False  \n",
       "3098        False        False  \n",
       "3099        False        False  \n",
       "3100        False        False  \n",
       "\n",
       "[3101 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 文章長度 ---\n",
    "df[\"content_length\"] = df[\"content\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "# --- 是否包含網址 ---\n",
    "df[\"has_url\"] = df[\"content\"].apply(lambda x: \"http\" in str(x) or \"www.\" in str(x))\n",
    "\n",
    "# --- 是否包含 @標記他人 ---\n",
    "df[\"has_mention\"] = df[\"content\"].apply(lambda x: \"@\" in str(x))\n",
    "\n",
    "# --- 是否使用 Hashtag ---\n",
    "df[\"has_hashtag\"] = df[\"content\"].apply(lambda x: \"#\" in str(x))\n",
    "\n",
    "# 貼文主題字詞提取（可後續做 TF-IDF 或主題建模）\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=100, stop_words='english')\n",
    "word_matrix = vectorizer.fit_transform(df['content'].astype(str))\n",
    "\n",
    "# 將常見詞語提取出來\n",
    "keywords = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 是否為深夜或白天貼文（時間段分類）\n",
    "def time_period(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return \"morning\"\n",
    "    elif 12 <= hour < 17:\n",
    "        return \"afternoon\"\n",
    "    elif 17 <= hour < 22:\n",
    "        return \"evening\"\n",
    "    else:\n",
    "        return \"night\"\n",
    "\n",
    "df[\"post_period\"] = df[\"post_hour\"].apply(time_period)\n",
    "\n",
    "cols_to_show_first = ['author', 'content', 'content_length', 'lang', 'scrape_time', 'post_weekday', 'post_hour', 'post_period', 'viral']\n",
    "df = df[cols_to_show_first + [col for col in df.columns if col not in cols_to_show_first]]\n",
    "df.to_csv(\"threads_cleaned_v2.csv\",encoding='utf_8_sig',index=False)\n",
    "print(\"✅ 處理完成，已輸出 threads_cleaned_v2.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention 模組處理文字資料（測試中 因未寫完可先跳過）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/tx1m33l955b0h85n6dlwm4440000gn/T/ipykernel_1391/3744166298.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"semantic_text\"] = df.apply(build_semantic_text, axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>content_length</th>\n",
       "      <th>lang</th>\n",
       "      <th>scrape_time</th>\n",
       "      <th>post_weekday</th>\n",
       "      <th>post_hour</th>\n",
       "      <th>post_period</th>\n",
       "      <th>viral</th>\n",
       "      <th>post_time</th>\n",
       "      <th>...</th>\n",
       "      <th>post_url</th>\n",
       "      <th>emojis</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>scrape_time_origin</th>\n",
       "      <th>has_question</th>\n",
       "      <th>has_exclaim</th>\n",
       "      <th>has_url</th>\n",
       "      <th>has_mention</th>\n",
       "      <th>has_hashtag</th>\n",
       "      <th>semantic_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>Thank you God for another day.</td>\n",
       "      <td>30</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 06:27</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T10:58:39.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@ayofvr/post/DJBymf8uTrK</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:27:40.176749+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>author_id: ayofvr emojis:  content: Thank you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>百達翡麗？ 沒有下限的網路病態！</td>\n",
       "      <td>16</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 06:27</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-29T12:33:44.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@ban.mei.onnnnni/post/...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:27:54.964788+08:00</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>emojis:  content: 百達翡麗？ 沒有下限的網路病態！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ribboworld2021</td>\n",
       "      <td>考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」</td>\n",
       "      <td>40</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T04:39:46.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@ribboworld2021/post/D...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:09.873641+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>author_id: ribboworld2021 topic: 小一日常 emojis: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>Just be strong. Confident. Hopeful. Intellectu...</td>\n",
       "      <td>69</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T11:25:22.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@ayofvr/post/DJB1qP5OmzP</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:24.726576+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>author_id: ayofvr emojis:  content: Just be st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jose_ykc</td>\n",
       "      <td>日文輔系老師上課內容之一AiScReam 歌詞導讀</td>\n",
       "      <td>25</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-29T10:36:19.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@jose_ykc/post/DJBvpGI...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:39.393706+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>author_id: jose_ykc topic: 輔仁大學 emojis:  conte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>leighton.williams</td>\n",
       "      <td>First tasting in California</td>\n",
       "      <td>27</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:23</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025年04月29日 07:30</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@leighton.williams/pos...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:23:40.328046+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>author_id: leighton.williams emojis:  content:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>cape__man</td>\n",
       "      <td>I hoped it would have been better.</td>\n",
       "      <td>34</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:23</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025年04月29日 06:31</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@cape__man/post/DJAdFd...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:23:55.063517+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>author_id: cape__man emojis:  content: I hoped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>simimoonlight</td>\n",
       "      <td>I can’t wait to watch Beyoncé on TikTok tonigh...</td>\n",
       "      <td>51</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:24</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025年04月29日 06:26</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@simimoonlight/post/DJ...</td>\n",
       "      <td>🫶🏿🥹</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-04-30 00:24:39.870994+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>author_id: simimoonlight emojis: 🫶 🏿 🥹 content...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>other98</td>\n",
       "      <td>Tonight, Canada just proved that they have a h...</td>\n",
       "      <td>77</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:24</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025年04月29日 12:31</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@other98/post/DJBGV3NxiX_</td>\n",
       "      <td>🙌</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-30 00:24:54.669936+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>author_id: other98 emojis: 🙌 content: Tonight,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>scottiebeam</td>\n",
       "      <td>Yall be fighting on here … i thought threads w...</td>\n",
       "      <td>57</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:25</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025年04月29日 01:33</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@scottiebeam/post/DI_6...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:25:09.396585+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>author_id: scottiebeam emojis:  content: Yall ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3101 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                                            content  \\\n",
       "0                ayofvr                     Thank you God for another day.   \n",
       "1                   NaN                                   百達翡麗？ 沒有下限的網路病態！   \n",
       "2        ribboworld2021           考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」   \n",
       "3                ayofvr  Just be strong. Confident. Hopeful. Intellectu...   \n",
       "4              jose_ykc                          日文輔系老師上課內容之一AiScReam 歌詞導讀   \n",
       "...                 ...                                                ...   \n",
       "3096  leighton.williams                        First tasting in California   \n",
       "3097          cape__man                 I hoped it would have been better.   \n",
       "3098      simimoonlight  I can’t wait to watch Beyoncé on TikTok tonigh...   \n",
       "3099            other98  Tonight, Canada just proved that they have a h...   \n",
       "3100        scottiebeam  Yall be fighting on here … i thought threads w...   \n",
       "\n",
       "      content_length lang        scrape_time post_weekday  post_hour  \\\n",
       "0                 30   en  2025年04月30日 06:27    Wednesday          6   \n",
       "1                 16   Ch  2025年04月30日 06:27    Wednesday          6   \n",
       "2                 40   Ch  2025年04月30日 06:28    Wednesday          6   \n",
       "3                 69   en  2025年04月30日 06:28    Wednesday          6   \n",
       "4                 25   Ch  2025年04月30日 06:28    Wednesday          6   \n",
       "...              ...  ...                ...          ...        ...   \n",
       "3096              27   en  2025年04月30日 00:23    Wednesday          0   \n",
       "3097              34   en  2025年04月30日 00:23    Wednesday          0   \n",
       "3098              51   en  2025年04月30日 00:24    Wednesday          0   \n",
       "3099              77   en  2025年04月30日 00:24    Wednesday          0   \n",
       "3100              57   en  2025年04月30日 00:25    Wednesday          0   \n",
       "\n",
       "     post_period  viral                 post_time  ...  \\\n",
       "0        morning      0  2025-04-29T10:58:39.000Z  ...   \n",
       "1        morning      1  2025-04-29T12:33:44.000Z  ...   \n",
       "2        morning      0  2025-04-29T04:39:46.000Z  ...   \n",
       "3        morning      0  2025-04-29T11:25:22.000Z  ...   \n",
       "4        morning      1  2025-04-29T10:36:19.000Z  ...   \n",
       "...          ...    ...                       ...  ...   \n",
       "3096       night      0         2025年04月29日 07:30  ...   \n",
       "3097       night      0         2025年04月29日 06:31  ...   \n",
       "3098       night      1         2025年04月29日 06:26  ...   \n",
       "3099       night      1         2025年04月29日 12:31  ...   \n",
       "3100       night      1         2025年04月29日 01:33  ...   \n",
       "\n",
       "                                               post_url emojis  emoji_count  \\\n",
       "0      https://www.threads.net/@ayofvr/post/DJBymf8uTrK                   0   \n",
       "1     https://www.threads.net/@ban.mei.onnnnni/post/...                   0   \n",
       "2     https://www.threads.net/@ribboworld2021/post/D...                   0   \n",
       "3      https://www.threads.net/@ayofvr/post/DJB1qP5OmzP                   0   \n",
       "4     https://www.threads.net/@jose_ykc/post/DJBvpGI...                   0   \n",
       "...                                                 ...    ...          ...   \n",
       "3096  https://www.threads.net/@leighton.williams/pos...                   0   \n",
       "3097  https://www.threads.net/@cape__man/post/DJAdFd...                   0   \n",
       "3098  https://www.threads.net/@simimoonlight/post/DJ...    🫶🏿🥹            3   \n",
       "3099  https://www.threads.net/@other98/post/DJBGV3NxiX_      🙌            1   \n",
       "3100  https://www.threads.net/@scottiebeam/post/DI_6...                   0   \n",
       "\n",
       "                   scrape_time_origin  has_question  has_exclaim  has_url  \\\n",
       "0    2025-04-30 06:27:40.176749+08:00         False        False    False   \n",
       "1    2025-04-30 06:27:54.964788+08:00          True         True    False   \n",
       "2    2025-04-30 06:28:09.873641+08:00         False         True    False   \n",
       "3    2025-04-30 06:28:24.726576+08:00         False        False    False   \n",
       "4    2025-04-30 06:28:39.393706+08:00         False        False    False   \n",
       "...                               ...           ...          ...      ...   \n",
       "3096 2025-04-30 00:23:40.328046+08:00         False        False    False   \n",
       "3097 2025-04-30 00:23:55.063517+08:00         False        False    False   \n",
       "3098 2025-04-30 00:24:39.870994+08:00         False        False    False   \n",
       "3099 2025-04-30 00:24:54.669936+08:00         False         True    False   \n",
       "3100 2025-04-30 00:25:09.396585+08:00         False        False    False   \n",
       "\n",
       "      has_mention  has_hashtag  \\\n",
       "0           False        False   \n",
       "1           False        False   \n",
       "2           False        False   \n",
       "3           False        False   \n",
       "4           False        False   \n",
       "...           ...          ...   \n",
       "3096        False        False   \n",
       "3097        False        False   \n",
       "3098        False        False   \n",
       "3099        False        False   \n",
       "3100        False        False   \n",
       "\n",
       "                                          semantic_text  \n",
       "0     author_id: ayofvr emojis:  content: Thank you ...  \n",
       "1                    emojis:  content: 百達翡麗？ 沒有下限的網路病態！  \n",
       "2     author_id: ribboworld2021 topic: 小一日常 emojis: ...  \n",
       "3     author_id: ayofvr emojis:  content: Just be st...  \n",
       "4     author_id: jose_ykc topic: 輔仁大學 emojis:  conte...  \n",
       "...                                                 ...  \n",
       "3096  author_id: leighton.williams emojis:  content:...  \n",
       "3097  author_id: cape__man emojis:  content: I hoped...  \n",
       "3098  author_id: simimoonlight emojis: 🫶 🏿 🥹 content...  \n",
       "3099  author_id: other98 emojis: 🙌 content: Tonight,...  \n",
       "3100  author_id: scottiebeam emojis:  content: Yall ...  \n",
       "\n",
       "[3101 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_semantic_text(row):\n",
    "    parts = []\n",
    "\n",
    "    # 作者資訊加上前綴\n",
    "    if pd.notna(row[\"author\"]):\n",
    "        parts.append(f\"author_id: {row['author']}\")\n",
    "\n",
    "    # 主題分類加上前綴\n",
    "    if pd.notna(row[\"topic\"]):\n",
    "        parts.append(f\"topic: {row['topic']}\")\n",
    "\n",
    "    # Emoji 當作情感符號，合併成一串文字\n",
    "    if pd.notna(row[\"emojis\"]):\n",
    "        emoji_text = \" \".join(row[\"emojis\"])\n",
    "        parts.append(f\"emojis: {emoji_text}\")\n",
    "\n",
    "    # 貼文正文\n",
    "    if pd.notna(row[\"content\"]):\n",
    "        parts.append(f\"content: {row['content']}\")\n",
    "\n",
    "    return \" \".join(parts)\n",
    "\n",
    "# 應用到 DataFrame\n",
    "df[\"semantic_text\"] = df.apply(build_semantic_text, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. 載入 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "# 2. 自訂 Dataset 類別\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, max_len=128):\n",
    "        self.encodings = tokenizer(\n",
    "            texts, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=max_len, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx]\n",
    "        }\n",
    "\n",
    "# 3. 建立 dataset 和 dataloader\n",
    "texts = df[\"content\"].astype(str).tolist()\n",
    "text_dataset = TextDataset(texts)\n",
    "text_loader = DataLoader(text_dataset, batch_size=32)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttentionEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=768, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, embed_dim)  # 可選加強轉換\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.embedding(input_ids)  # [B, T, D]\n",
    "        attn_output, _ = self.attention(x, x, x, key_padding_mask=~attention_mask.bool())\n",
    "        x = self.norm(attn_output + x)\n",
    "        x = self.fc(x)\n",
    "        cls_rep = x[:, 0, :] \n",
    "        return cls_rep\n",
    "\n",
    "# 初始化模型\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model = SelfAttentionEncoder(vocab_size)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# 提取語意向量 Z_text\n",
    "Z_text_list = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in text_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        Z_text = model(input_ids, attention_mask)  # [B, 768]\n",
    "        Z_text_list.append(Z_text.cpu())\n",
    "\n",
    "Z_text_tensor = torch.cat(Z_text_list, dim=0)  # [N, 768]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.185373</td>\n",
       "      <td>0.116477</td>\n",
       "      <td>0.122377</td>\n",
       "      <td>0.404507</td>\n",
       "      <td>-0.339129</td>\n",
       "      <td>-0.347968</td>\n",
       "      <td>-0.377152</td>\n",
       "      <td>-0.266511</td>\n",
       "      <td>0.258730</td>\n",
       "      <td>-0.805666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.437772</td>\n",
       "      <td>1.143777</td>\n",
       "      <td>-0.130535</td>\n",
       "      <td>0.552972</td>\n",
       "      <td>0.508016</td>\n",
       "      <td>-0.661004</td>\n",
       "      <td>0.648708</td>\n",
       "      <td>0.508159</td>\n",
       "      <td>0.471049</td>\n",
       "      <td>0.812747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.118061</td>\n",
       "      <td>0.158701</td>\n",
       "      <td>0.106780</td>\n",
       "      <td>0.251318</td>\n",
       "      <td>-0.336016</td>\n",
       "      <td>-0.447374</td>\n",
       "      <td>-0.266132</td>\n",
       "      <td>-0.184926</td>\n",
       "      <td>0.391666</td>\n",
       "      <td>-0.695012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470352</td>\n",
       "      <td>1.147053</td>\n",
       "      <td>-0.160061</td>\n",
       "      <td>0.309530</td>\n",
       "      <td>0.539303</td>\n",
       "      <td>-0.772830</td>\n",
       "      <td>0.498029</td>\n",
       "      <td>0.462456</td>\n",
       "      <td>0.497207</td>\n",
       "      <td>0.803436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.160336</td>\n",
       "      <td>0.090955</td>\n",
       "      <td>0.141805</td>\n",
       "      <td>0.271038</td>\n",
       "      <td>-0.268028</td>\n",
       "      <td>-0.443410</td>\n",
       "      <td>-0.325646</td>\n",
       "      <td>-0.121320</td>\n",
       "      <td>0.362589</td>\n",
       "      <td>-0.728238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423751</td>\n",
       "      <td>1.035433</td>\n",
       "      <td>-0.356109</td>\n",
       "      <td>0.497018</td>\n",
       "      <td>0.418673</td>\n",
       "      <td>-0.695262</td>\n",
       "      <td>0.535084</td>\n",
       "      <td>0.468965</td>\n",
       "      <td>0.534107</td>\n",
       "      <td>0.722016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.076843</td>\n",
       "      <td>0.105067</td>\n",
       "      <td>0.052723</td>\n",
       "      <td>0.331179</td>\n",
       "      <td>-0.202970</td>\n",
       "      <td>-0.429289</td>\n",
       "      <td>-0.354241</td>\n",
       "      <td>-0.151234</td>\n",
       "      <td>0.392680</td>\n",
       "      <td>-0.755461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332896</td>\n",
       "      <td>1.001550</td>\n",
       "      <td>-0.155210</td>\n",
       "      <td>0.619413</td>\n",
       "      <td>0.462634</td>\n",
       "      <td>-0.595787</td>\n",
       "      <td>0.625809</td>\n",
       "      <td>0.477340</td>\n",
       "      <td>0.576133</td>\n",
       "      <td>0.758163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.138116</td>\n",
       "      <td>0.084057</td>\n",
       "      <td>0.129734</td>\n",
       "      <td>0.177018</td>\n",
       "      <td>-0.317661</td>\n",
       "      <td>-0.374959</td>\n",
       "      <td>-0.301814</td>\n",
       "      <td>-0.134798</td>\n",
       "      <td>0.365255</td>\n",
       "      <td>-0.658869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398198</td>\n",
       "      <td>1.032260</td>\n",
       "      <td>-0.304353</td>\n",
       "      <td>0.398853</td>\n",
       "      <td>0.552792</td>\n",
       "      <td>-0.693314</td>\n",
       "      <td>0.558638</td>\n",
       "      <td>0.415872</td>\n",
       "      <td>0.535430</td>\n",
       "      <td>0.751466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>1.089460</td>\n",
       "      <td>0.230556</td>\n",
       "      <td>0.096753</td>\n",
       "      <td>0.355270</td>\n",
       "      <td>-0.262501</td>\n",
       "      <td>-0.373350</td>\n",
       "      <td>-0.388324</td>\n",
       "      <td>-0.285172</td>\n",
       "      <td>0.496985</td>\n",
       "      <td>-0.821012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.436395</td>\n",
       "      <td>1.199498</td>\n",
       "      <td>-0.107245</td>\n",
       "      <td>0.401726</td>\n",
       "      <td>0.564576</td>\n",
       "      <td>-0.815372</td>\n",
       "      <td>0.613472</td>\n",
       "      <td>0.482972</td>\n",
       "      <td>0.583095</td>\n",
       "      <td>0.766825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>1.079693</td>\n",
       "      <td>0.206142</td>\n",
       "      <td>0.059271</td>\n",
       "      <td>0.246766</td>\n",
       "      <td>-0.208253</td>\n",
       "      <td>-0.328518</td>\n",
       "      <td>-0.396139</td>\n",
       "      <td>-0.210764</td>\n",
       "      <td>0.441348</td>\n",
       "      <td>-0.776673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388379</td>\n",
       "      <td>0.950016</td>\n",
       "      <td>-0.213405</td>\n",
       "      <td>0.464987</td>\n",
       "      <td>0.501455</td>\n",
       "      <td>-0.790398</td>\n",
       "      <td>0.626000</td>\n",
       "      <td>0.466310</td>\n",
       "      <td>0.540599</td>\n",
       "      <td>0.711200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>1.129379</td>\n",
       "      <td>0.108640</td>\n",
       "      <td>0.122049</td>\n",
       "      <td>0.300188</td>\n",
       "      <td>-0.298384</td>\n",
       "      <td>-0.332469</td>\n",
       "      <td>-0.461424</td>\n",
       "      <td>-0.200608</td>\n",
       "      <td>0.412536</td>\n",
       "      <td>-0.822279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.437023</td>\n",
       "      <td>1.012208</td>\n",
       "      <td>-0.170115</td>\n",
       "      <td>0.577867</td>\n",
       "      <td>0.450494</td>\n",
       "      <td>-0.554175</td>\n",
       "      <td>0.630714</td>\n",
       "      <td>0.495547</td>\n",
       "      <td>0.548192</td>\n",
       "      <td>0.719363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>1.063507</td>\n",
       "      <td>0.102142</td>\n",
       "      <td>0.152797</td>\n",
       "      <td>0.225523</td>\n",
       "      <td>-0.329202</td>\n",
       "      <td>-0.329178</td>\n",
       "      <td>-0.355302</td>\n",
       "      <td>-0.101184</td>\n",
       "      <td>0.401613</td>\n",
       "      <td>-0.827614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340202</td>\n",
       "      <td>1.069342</td>\n",
       "      <td>-0.156227</td>\n",
       "      <td>0.479461</td>\n",
       "      <td>0.446085</td>\n",
       "      <td>-0.610415</td>\n",
       "      <td>0.554650</td>\n",
       "      <td>0.437024</td>\n",
       "      <td>0.644774</td>\n",
       "      <td>0.715786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>1.107659</td>\n",
       "      <td>0.133675</td>\n",
       "      <td>0.157146</td>\n",
       "      <td>0.307179</td>\n",
       "      <td>-0.363217</td>\n",
       "      <td>-0.364786</td>\n",
       "      <td>-0.397355</td>\n",
       "      <td>-0.150967</td>\n",
       "      <td>0.458223</td>\n",
       "      <td>-0.829984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371882</td>\n",
       "      <td>1.058214</td>\n",
       "      <td>-0.190540</td>\n",
       "      <td>0.491995</td>\n",
       "      <td>0.454371</td>\n",
       "      <td>-0.690442</td>\n",
       "      <td>0.558352</td>\n",
       "      <td>0.457464</td>\n",
       "      <td>0.483374</td>\n",
       "      <td>0.741767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3101 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     1.185373  0.116477  0.122377  0.404507 -0.339129 -0.347968 -0.377152   \n",
       "1     1.118061  0.158701  0.106780  0.251318 -0.336016 -0.447374 -0.266132   \n",
       "2     1.160336  0.090955  0.141805  0.271038 -0.268028 -0.443410 -0.325646   \n",
       "3     1.076843  0.105067  0.052723  0.331179 -0.202970 -0.429289 -0.354241   \n",
       "4     1.138116  0.084057  0.129734  0.177018 -0.317661 -0.374959 -0.301814   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3096  1.089460  0.230556  0.096753  0.355270 -0.262501 -0.373350 -0.388324   \n",
       "3097  1.079693  0.206142  0.059271  0.246766 -0.208253 -0.328518 -0.396139   \n",
       "3098  1.129379  0.108640  0.122049  0.300188 -0.298384 -0.332469 -0.461424   \n",
       "3099  1.063507  0.102142  0.152797  0.225523 -0.329202 -0.329178 -0.355302   \n",
       "3100  1.107659  0.133675  0.157146  0.307179 -0.363217 -0.364786 -0.397355   \n",
       "\n",
       "           7         8         9    ...       758       759       760  \\\n",
       "0    -0.266511  0.258730 -0.805666  ...  0.437772  1.143777 -0.130535   \n",
       "1    -0.184926  0.391666 -0.695012  ...  0.470352  1.147053 -0.160061   \n",
       "2    -0.121320  0.362589 -0.728238  ...  0.423751  1.035433 -0.356109   \n",
       "3    -0.151234  0.392680 -0.755461  ...  0.332896  1.001550 -0.155210   \n",
       "4    -0.134798  0.365255 -0.658869  ...  0.398198  1.032260 -0.304353   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3096 -0.285172  0.496985 -0.821012  ...  0.436395  1.199498 -0.107245   \n",
       "3097 -0.210764  0.441348 -0.776673  ...  0.388379  0.950016 -0.213405   \n",
       "3098 -0.200608  0.412536 -0.822279  ...  0.437023  1.012208 -0.170115   \n",
       "3099 -0.101184  0.401613 -0.827614  ...  0.340202  1.069342 -0.156227   \n",
       "3100 -0.150967  0.458223 -0.829984  ...  0.371882  1.058214 -0.190540   \n",
       "\n",
       "           761       762       763       764       765       766       767  \n",
       "0     0.552972  0.508016 -0.661004  0.648708  0.508159  0.471049  0.812747  \n",
       "1     0.309530  0.539303 -0.772830  0.498029  0.462456  0.497207  0.803436  \n",
       "2     0.497018  0.418673 -0.695262  0.535084  0.468965  0.534107  0.722016  \n",
       "3     0.619413  0.462634 -0.595787  0.625809  0.477340  0.576133  0.758163  \n",
       "4     0.398853  0.552792 -0.693314  0.558638  0.415872  0.535430  0.751466  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3096  0.401726  0.564576 -0.815372  0.613472  0.482972  0.583095  0.766825  \n",
       "3097  0.464987  0.501455 -0.790398  0.626000  0.466310  0.540599  0.711200  \n",
       "3098  0.577867  0.450494 -0.554175  0.630714  0.495547  0.548192  0.719363  \n",
       "3099  0.479461  0.446085 -0.610415  0.554650  0.437024  0.644774  0.715786  \n",
       "3100  0.491995  0.454371 -0.690442  0.558352  0.457464  0.483374  0.741767  \n",
       "\n",
       "[3101 rows x 768 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Z_text_df = pd.DataFrame(Z_text_tensor.numpy())\n",
    "Z_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9GcFmuNEsVn"
   },
   "source": [
    "## 清洗數據embbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5H23LR9aF0ny"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb0ed594ba34a05b52791752297863f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3093 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d3f5fdafc9421dba334466a3702baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3093 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 全部處理完成，已輸出 threads_with_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# =============== BERT 向量嵌入 ===============\n",
    "df = df.dropna(subset=['content']) #要先處理content空值才能embedding\n",
    "# --- 載入 tokenizer & model ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "# 選擇硬體設備（MPS、CUDA、CPU），自動判斷是否可用 GPU（M1/M2 晶片上的 MPS 或 CUDA），否則 fallback 到 CPU\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# --- 建立 HuggingFace Dataset ---\n",
    "hf_dataset = Dataset.from_pandas(df[[\"content\"]])\n",
    "\n",
    "# --- tokenize function ---\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['content'], truncation=True, padding='max_length', max_length=128)\n",
    "tokenized_dataset = hf_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# --- 取得 [CLS] 向量 ---\n",
    "def extract_embeddings(batch):\n",
    "    inputs = {k: torch.tensor(v).to(model.device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
    "    return {\"embeddings\": embeddings}\n",
    "\n",
    "# --- 批次轉換為 embeddings ---\n",
    "batch_size = 64\n",
    "embeddings_dataset = tokenized_dataset.map(extract_embeddings, batched=True, batch_size=batch_size)\n",
    "\n",
    "# =============== 匯出最終結果 ===============\n",
    "# embeddings_dataset[\"embeddings\"] 是 list of 768-dim vectors\n",
    "embedding_df = pd.DataFrame(embeddings_dataset[\"embeddings\"])\n",
    "final_df = pd.concat([df.reset_index(drop=True), embedding_df], axis=1)\n",
    "\n",
    "# 儲存\n",
    "# final_df.to_csv(\"C:/Users/User/Desktop/louis/threads_with_embeddings.csv\",encoding='utf_8_sig', index=False)\n",
    "final_df.to_csv(\"threads_with_embeddings.csv\",encoding='utf_8_sig', index=False)\n",
    "print(\"✅ 全部處理完成，已輸出 threads_with_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WxP6bcNEywY"
   },
   "source": [
    "## 分詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SovtoK069ox_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/d4/dpw_wc9n69zg41k6pqzpwzcr0000gn/T/jieba.cache\n",
      "Loading model cost 0.258 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'考完 期中考 ， 成績 都 還沒出 來 ， 小一 女兒 就 自信 對 我 說 ： 「 我 真羨慕 妳 生 一個 天才 ！ 」'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = set(['的', '了', '在', '是', '和', '也', '與', '有', '為', '等'])\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    words = jieba.cut(text)\n",
    "    words_filtered = [word for word in words if word not in stopwords]\n",
    "    return ' '.join(words_filtered)\n",
    "\n",
    "\n",
    "df['processed_content'] = df['content'].apply(tokenize_and_remove_stopwords)\n",
    "df['processed_content'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwCtvYh4E3vU"
   },
   "source": [
    "## 機器學習建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eFdTaLb79ovp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 33647 stored elements and shape (3093, 12887)>\n",
      "  Coords\tValues\n",
      "  (0, 2810)\t0.4662798616255825\n",
      "  (0, 3199)\t0.2941078724734809\n",
      "  (0, 1316)\t0.4428271644129856\n",
      "  (0, 1220)\t0.31033983594608366\n",
      "  (0, 422)\t0.5030766701843938\n",
      "  (0, 902)\t0.3880603079298958\n",
      "  (1, 9706)\t0.44772643118600786\n",
      "  (1, 10484)\t0.44772643118600786\n",
      "  (1, 8824)\t0.25695349239444143\n",
      "  (1, 3502)\t0.425969976146039\n",
      "  (1, 10379)\t0.3887770705954321\n",
      "  (1, 9624)\t0.44772643118600786\n",
      "  (2, 10526)\t0.31144587501432597\n",
      "  (2, 8426)\t0.32887477885054034\n",
      "  (2, 7301)\t0.285391148181221\n",
      "  (2, 12083)\t0.3586696867544276\n",
      "  (2, 6469)\t0.3586696867544276\n",
      "  (2, 6055)\t0.2990798709466531\n",
      "  (2, 10693)\t0.32887477885054034\n",
      "  (2, 9857)\t0.3586696867544276\n",
      "  (2, 3232)\t0.1812649560447855\n",
      "  (2, 5971)\t0.31144587501432597\n",
      "  (3, 1638)\t0.26167811005929115\n",
      "  (3, 548)\t0.23061394459754261\n",
      "  (3, 2710)\t0.3728557459198426\n",
      "  :\t:\n",
      "  (3090, 3045)\t0.4140018744676121\n",
      "  (3090, 3075)\t0.3876142688527415\n",
      "  (3090, 2883)\t0.3657390732966238\n",
      "  (3091, 1638)\t0.23035513728612972\n",
      "  (3091, 702)\t0.20796259097802208\n",
      "  (3091, 2813)\t0.1625400466880004\n",
      "  (3091, 2812)\t0.21359102767160001\n",
      "  (3091, 1405)\t0.22665008839036188\n",
      "  (3091, 2823)\t0.23302350706955696\n",
      "  (3091, 1442)\t0.3163304413126057\n",
      "  (3091, 2883)\t0.2590135719616217\n",
      "  (3091, 2809)\t0.2876720066371137\n",
      "  (3091, 2300)\t0.34498887598809774\n",
      "  (3091, 1570)\t0.34498887598809774\n",
      "  (3091, 2986)\t0.34498887598809774\n",
      "  (3091, 1729)\t0.34498887598809774\n",
      "  (3092, 548)\t0.24995784453429756\n",
      "  (3092, 2838)\t0.2748792874614182\n",
      "  (3092, 2184)\t0.404130889547885\n",
      "  (3092, 3069)\t0.2836276724999312\n",
      "  (3092, 2112)\t0.25162707680971147\n",
      "  (3092, 1179)\t0.404130889547885\n",
      "  (3092, 1431)\t0.32219919619252974\n",
      "  (3092, 3177)\t0.36884482985647593\n",
      "  (3092, 2832)\t0.38948585157415855\n"
     ]
    }
   ],
   "source": [
    "# 計算 TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_content'])\n",
    "\n",
    "# 計算 TF\n",
    "tf_vectorizer = CountVectorizer()\n",
    "tf_matrix = tf_vectorizer.fit_transform(df['processed_content'])\n",
    "\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wvN8ioFE8Cz"
   },
   "source": [
    "# 多模型分類實驗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cROM4E4v9otD"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>content_length</th>\n",
       "      <th>lang</th>\n",
       "      <th>scrape_time</th>\n",
       "      <th>post_weekday</th>\n",
       "      <th>post_hour</th>\n",
       "      <th>post_period</th>\n",
       "      <th>viral</th>\n",
       "      <th>post_time</th>\n",
       "      <th>...</th>\n",
       "      <th>post_url</th>\n",
       "      <th>emojis</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>scrape_time_origin</th>\n",
       "      <th>has_question</th>\n",
       "      <th>has_exclaim</th>\n",
       "      <th>has_url</th>\n",
       "      <th>has_mention</th>\n",
       "      <th>has_hashtag</th>\n",
       "      <th>processed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>Thank you God for another day.</td>\n",
       "      <td>30</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 06:27</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T10:58:39.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@ayofvr/post/DJBymf8uTrK</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:27:40.176749+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Thank   you   God   for   another   day .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>百達翡麗？ 沒有下限的網路病態！</td>\n",
       "      <td>16</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 06:27</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-29T12:33:44.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@ban.mei.onnnnni/post/...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:27:54.964788+08:00</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>百達 翡麗 ？   沒有 下限 網路 病態 ！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ribboworld2021</td>\n",
       "      <td>考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」</td>\n",
       "      <td>40</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T04:39:46.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@ribboworld2021/post/D...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:09.873641+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>考完 期中考 ， 成績 都 還沒出 來 ， 小一 女兒 就 自信 對 我 說 ： 「 我 真...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>Just be strong. Confident. Hopeful. Intellectu...</td>\n",
       "      <td>69</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T11:25:22.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@ayofvr/post/DJB1qP5OmzP</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:24.726576+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Just   be   strong .   Confident .   Hopeful ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jose_ykc</td>\n",
       "      <td>日文輔系老師上課內容之一AiScReam 歌詞導讀</td>\n",
       "      <td>25</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-29T10:36:19.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@jose_ykc/post/DJBvpGI...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:39.393706+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>日文 輔系 老師 上 課內容 之一 AiScReam   歌詞 導讀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>leighton.williams</td>\n",
       "      <td>First tasting in California</td>\n",
       "      <td>27</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:23</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025年04月29日 07:30</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@leighton.williams/pos...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:23:40.328046+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>First   tasting   in   California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>cape__man</td>\n",
       "      <td>I hoped it would have been better.</td>\n",
       "      <td>34</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:23</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025年04月29日 06:31</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@cape__man/post/DJAdFd...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:23:55.063517+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>I   hoped   it   would   have   been   better .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>simimoonlight</td>\n",
       "      <td>I can’t wait to watch Beyoncé on TikTok tonigh...</td>\n",
       "      <td>51</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:24</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025年04月29日 06:26</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@simimoonlight/post/DJ...</td>\n",
       "      <td>🫶🏿🥹</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-04-30 00:24:39.870994+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>I   can ’ t   wait   to   watch   Beyonc é   o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>other98</td>\n",
       "      <td>Tonight, Canada just proved that they have a h...</td>\n",
       "      <td>77</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:24</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025年04月29日 12:31</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@other98/post/DJBGV3NxiX_</td>\n",
       "      <td>🙌</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-30 00:24:54.669936+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Tonight ,   Canada   just   proved   that   th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>scottiebeam</td>\n",
       "      <td>Yall be fighting on here … i thought threads w...</td>\n",
       "      <td>57</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:25</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025年04月29日 01:33</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@scottiebeam/post/DI_6...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:25:09.396585+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Yall   be   fighting   on   here   …   i   tho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3093 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                                            content  \\\n",
       "0                ayofvr                     Thank you God for another day.   \n",
       "1                   NaN                                   百達翡麗？ 沒有下限的網路病態！   \n",
       "2        ribboworld2021           考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」   \n",
       "3                ayofvr  Just be strong. Confident. Hopeful. Intellectu...   \n",
       "4              jose_ykc                          日文輔系老師上課內容之一AiScReam 歌詞導讀   \n",
       "...                 ...                                                ...   \n",
       "3096  leighton.williams                        First tasting in California   \n",
       "3097          cape__man                 I hoped it would have been better.   \n",
       "3098      simimoonlight  I can’t wait to watch Beyoncé on TikTok tonigh...   \n",
       "3099            other98  Tonight, Canada just proved that they have a h...   \n",
       "3100        scottiebeam  Yall be fighting on here … i thought threads w...   \n",
       "\n",
       "      content_length lang        scrape_time post_weekday  post_hour  \\\n",
       "0                 30   en  2025年04月30日 06:27    Wednesday          6   \n",
       "1                 16   Ch  2025年04月30日 06:27    Wednesday          6   \n",
       "2                 40   Ch  2025年04月30日 06:28    Wednesday          6   \n",
       "3                 69   en  2025年04月30日 06:28    Wednesday          6   \n",
       "4                 25   Ch  2025年04月30日 06:28    Wednesday          6   \n",
       "...              ...  ...                ...          ...        ...   \n",
       "3096              27   en  2025年04月30日 00:23    Wednesday          0   \n",
       "3097              34   en  2025年04月30日 00:23    Wednesday          0   \n",
       "3098              51   en  2025年04月30日 00:24    Wednesday          0   \n",
       "3099              77   en  2025年04月30日 00:24    Wednesday          0   \n",
       "3100              57   en  2025年04月30日 00:25    Wednesday          0   \n",
       "\n",
       "     post_period  viral                 post_time  ...  \\\n",
       "0        morning      0  2025-04-29T10:58:39.000Z  ...   \n",
       "1        morning      1  2025-04-29T12:33:44.000Z  ...   \n",
       "2        morning      0  2025-04-29T04:39:46.000Z  ...   \n",
       "3        morning      0  2025-04-29T11:25:22.000Z  ...   \n",
       "4        morning      1  2025-04-29T10:36:19.000Z  ...   \n",
       "...          ...    ...                       ...  ...   \n",
       "3096       night      0         2025年04月29日 07:30  ...   \n",
       "3097       night      0         2025年04月29日 06:31  ...   \n",
       "3098       night      1         2025年04月29日 06:26  ...   \n",
       "3099       night      1         2025年04月29日 12:31  ...   \n",
       "3100       night      1         2025年04月29日 01:33  ...   \n",
       "\n",
       "                                               post_url emojis  emoji_count  \\\n",
       "0      https://www.threads.net/@ayofvr/post/DJBymf8uTrK                   0   \n",
       "1     https://www.threads.net/@ban.mei.onnnnni/post/...                   0   \n",
       "2     https://www.threads.net/@ribboworld2021/post/D...                   0   \n",
       "3      https://www.threads.net/@ayofvr/post/DJB1qP5OmzP                   0   \n",
       "4     https://www.threads.net/@jose_ykc/post/DJBvpGI...                   0   \n",
       "...                                                 ...    ...          ...   \n",
       "3096  https://www.threads.net/@leighton.williams/pos...                   0   \n",
       "3097  https://www.threads.net/@cape__man/post/DJAdFd...                   0   \n",
       "3098  https://www.threads.net/@simimoonlight/post/DJ...    🫶🏿🥹            3   \n",
       "3099  https://www.threads.net/@other98/post/DJBGV3NxiX_      🙌            1   \n",
       "3100  https://www.threads.net/@scottiebeam/post/DI_6...                   0   \n",
       "\n",
       "                   scrape_time_origin  has_question  has_exclaim  has_url  \\\n",
       "0    2025-04-30 06:27:40.176749+08:00         False        False    False   \n",
       "1    2025-04-30 06:27:54.964788+08:00          True         True    False   \n",
       "2    2025-04-30 06:28:09.873641+08:00         False         True    False   \n",
       "3    2025-04-30 06:28:24.726576+08:00         False        False    False   \n",
       "4    2025-04-30 06:28:39.393706+08:00         False        False    False   \n",
       "...                               ...           ...          ...      ...   \n",
       "3096 2025-04-30 00:23:40.328046+08:00         False        False    False   \n",
       "3097 2025-04-30 00:23:55.063517+08:00         False        False    False   \n",
       "3098 2025-04-30 00:24:39.870994+08:00         False        False    False   \n",
       "3099 2025-04-30 00:24:54.669936+08:00         False         True    False   \n",
       "3100 2025-04-30 00:25:09.396585+08:00         False        False    False   \n",
       "\n",
       "      has_mention  has_hashtag  \\\n",
       "0           False        False   \n",
       "1           False        False   \n",
       "2           False        False   \n",
       "3           False        False   \n",
       "4           False        False   \n",
       "...           ...          ...   \n",
       "3096        False        False   \n",
       "3097        False        False   \n",
       "3098        False        False   \n",
       "3099        False        False   \n",
       "3100        False        False   \n",
       "\n",
       "                                      processed_content  \n",
       "0             Thank   you   God   for   another   day .  \n",
       "1                               百達 翡麗 ？   沒有 下限 網路 病態 ！  \n",
       "2     考完 期中考 ， 成績 都 還沒出 來 ， 小一 女兒 就 自信 對 我 說 ： 「 我 真...  \n",
       "3     Just   be   strong .   Confident .   Hopeful ....  \n",
       "4                    日文 輔系 老師 上 課內容 之一 AiScReam   歌詞 導讀  \n",
       "...                                                 ...  \n",
       "3096                  First   tasting   in   California  \n",
       "3097    I   hoped   it   would   have   been   better .  \n",
       "3098  I   can ’ t   wait   to   watch   Beyonc é   o...  \n",
       "3099  Tonight ,   Canada   just   proved   that   th...  \n",
       "3100  Yall   be   fighting   on   here   …   i   tho...  \n",
       "\n",
       "[3093 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, random_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 8\n",
    "num_epochs = 5\n",
    "# ========== 參數設定 ==========\n",
    "model_tokenizer_map = {\n",
    "    \"FusionMacBERT\": \"hfl/chinese-macbert-base\",\n",
    "    \"PureMacBERT\": \"hfl/chinese-macbert-base\",\n",
    "    \"NumericOnly\": None,\n",
    "    \"BiLSTMWithNumeric\": \"bert-base-chinese\",\n",
    "    \"MacBERTWithGRU\": \"hfl/chinese-macbert-base\",\n",
    "    \"MacBERTMLPFusion\": \"hfl/chinese-macbert-base\",\n",
    "    \"TextCNNMacBERT\": \"hfl/chinese-macbert-base\",\n",
    "    \"RoBERTa\": \"hfl/chinese-roberta-wwm-ext\",\n",
    "    \"BERTwwmExt\": \"hfl/chinese-bert-wwm-ext\",\n",
    "    \"ERNIE\": \"nghuyong/ernie-3.0-base-zh\",\n",
    "    \"ConvBERT\": \"YituTech/conv-bert-base\"\n",
    "}\n",
    "\n",
    "#tokenizer\n",
    "default_tokenizer_name = model_tokenizer_map[\"FusionMacBERT\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(default_tokenizer_name)\n",
    "\n",
    "#載入資料\n",
    "# df = pd.read_csv(\"C:/Users/User/Desktop/louis/threads_cleaned_v2.csv\", encoding='utf_8_sig')\n",
    "#df = df.dropna(subset=['content', 'view_count']).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "To77Vfki2Jzf"
   },
   "source": [
    "# Label 分群 (用四分位數分三群)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Df9cfTOt9oqs"
   },
   "outputs": [],
   "source": [
    "# Label 分群 ：標籤轉換（按瀏覽數進行分群）\n",
    "# 取「瀏覽數」的第 80 百分位作為高人氣門檻（q_high）、第 20 百分位作為低人氣門檻（q_low）\n",
    "# 把每筆資料的「view_count」劃分為三類：0 高人氣 (high)、1 中人氣 (medium)、2 低人氣 (low)\n",
    "q_high = df['view_count'].quantile(0.80)\n",
    "q_low = df['view_count'].quantile(0.20)\n",
    "df['view_class'] = df['view_count'].apply(lambda x: \"high\" if x >= q_high else (\"low\" if x <= q_low else \"medium\"))\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['view_class'])\n",
    "\n",
    "#做 oversampling 類別資料平衡 ：資料增強（針對 high / low 類別 oversample）\n",
    "df_high = df[df['view_class'] == 'high']\n",
    "df_low = df[df['view_class'] == 'low']\n",
    "df_medium = df[df['view_class'] == 'medium']\n",
    "\n",
    "# 分別取出三個分類的樣本：對 high 與 low 分類做「過採樣」，各自複製三次，讓資料數量接近 medium\n",
    "# 再對整個資料表做隨機打散 (shuffle），避免模型學到資料順序的偏誤\n",
    "df_high_oversampled = pd.concat([df_high] * 3, ignore_index=True)\n",
    "df_low_oversampled = pd.concat([df_low] * 3, ignore_index=True)\n",
    "df = pd.concat([df_medium, df_high_oversampled, df_low_oversampled], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-d1A_HlMu8Ae"
   },
   "outputs": [],
   "source": [
    "q_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOiJNmCmu_uf"
   },
   "outputs": [],
   "source": [
    "q_low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibbctyH9CUzu"
   },
   "source": [
    "## Label 分群 （1000以下、1000~10000、10000~100000、100000以上)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5yL13rSfCUzu"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>content_length</th>\n",
       "      <th>lang</th>\n",
       "      <th>scrape_time</th>\n",
       "      <th>post_weekday</th>\n",
       "      <th>post_hour</th>\n",
       "      <th>post_period</th>\n",
       "      <th>viral</th>\n",
       "      <th>post_time</th>\n",
       "      <th>...</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>scrape_time_origin</th>\n",
       "      <th>has_question</th>\n",
       "      <th>has_exclaim</th>\n",
       "      <th>has_url</th>\n",
       "      <th>has_mention</th>\n",
       "      <th>has_hashtag</th>\n",
       "      <th>processed_content</th>\n",
       "      <th>view_class</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>031758_wu</td>\n",
       "      <td>老實說我現在還是搞不懂一場演唱會搶這麼多次票的意義在哪裡💧</td>\n",
       "      <td>29</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 08:30</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>8</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T12:59:30.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-30 08:30:58.388029+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>老實 說 我現 還是 搞不懂 一場 演唱 會 搶 這麼 多次 票 意義在 哪裡 💧</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yuii_1.6</td>\n",
       "      <td>吳海嫄🫧說能看到很多恩瑟很開心還說台北的應援法太讚了 ㅠㅠ</td>\n",
       "      <td>29</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月29日 02:17</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025年04月28日 02:23</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-29 02:17:59.586251+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>吳海 嫄 🫧 說 能 看到 很多 恩瑟 很 開心 還說 台北 應援法 太 讚   ㅠ ㅠ</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rejiya1575</td>\n",
       "      <td>Tahajjud is life changer. ♥</td>\n",
       "      <td>27</td>\n",
       "      <td>lg</td>\n",
       "      <td>2025年04月28日 05:19</td>\n",
       "      <td>Monday</td>\n",
       "      <td>5</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025年04月27日 18:08</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-28 05:19:53.327702+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Tahajjud   is   life   changer .   ♥</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jonblta</td>\n",
       "      <td>i dont wanna hear Warriors fans complaining ab...</td>\n",
       "      <td>87</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年05月03日 19:29</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>19</td>\n",
       "      <td>evening</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-03T03:09:05.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-03 19:29:20.954467+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>i   dont   wanna   hear   Warriors   fans   co...</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_lync.m</td>\n",
       "      <td>Trải nghiệm đầu tiên khi đu Em Xinh: Yeolan và...</td>\n",
       "      <td>80</td>\n",
       "      <td>vi</td>\n",
       "      <td>2025年05月03日 08:40</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>8</td>\n",
       "      <td>morning</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-05-02T11:43:24.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-03 08:40:12.796089+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Tr ả i   nghi ệ m   đ ầ u   ti ê n   khi   đ u...</td>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6222</th>\n",
       "      <td>jauyiwu</td>\n",
       "      <td>連續三天走在路上，聽到不同的人說要去白沙屯。￼我想，這已經變成台灣人一個新的時間體感，就好像...</td>\n",
       "      <td>57</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年05月01日 17:40</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>17</td>\n",
       "      <td>evening</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-01T00:46:32.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-01 17:40:50.710474+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>連續 三天 走 路上 ， 聽到 不同 人 說 要 去 白沙 屯 。 ￼ 我 想 ， 這已 經...</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6223</th>\n",
       "      <td>__mi_achool</td>\n",
       "      <td>收到vip 了 原價讓一張 A2 2XX</td>\n",
       "      <td>20</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月29日 07:25</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>7</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025年04月28日 17:58</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29 07:25:02.893557+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>收到 vip     原價 讓 一張   A2   2XX</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6224</th>\n",
       "      <td>lhs_7632</td>\n",
       "      <td>分手第十八天 偶然看見你的限時動態，這是十八天內換的第二個男生了，打開了你追蹤名單 不管是我...</td>\n",
       "      <td>124</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 07:03</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>7</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-28T22:51:13.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 07:03:46.817921+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>分手 第十八天   偶然 看見 你 限時 動態 ， 這是 十八天 內換 第二 個 男生 ， ...</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6225</th>\n",
       "      <td>taro_puree_05</td>\n",
       "      <td>我有說我要去嗎 你就給我第一排的票 虫合❓</td>\n",
       "      <td>21</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年05月01日 03:09</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>3</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30T05:43:31.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-05-01 03:09:51.931280+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>我 說 我 要 去 嗎   你 就給 我 第一排 票   虫合 ❓</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6226</th>\n",
       "      <td>xuan._0603</td>\n",
       "      <td>演唱會遇到最靠北的事： 錄到最喜歡的part手機跳儲存空間不足。</td>\n",
       "      <td>32</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月29日 02:44</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025年04月28日 12:45</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29 02:44:28.814612+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>演唱 會 遇到 最 靠北 事 ：   錄到 最 喜歡 part 手機 跳 儲存 空間 不足 。</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6227 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             author                                            content  \\\n",
       "0         031758_wu                      老實說我現在還是搞不懂一場演唱會搶這麼多次票的意義在哪裡💧   \n",
       "1          yuii_1.6                      吳海嫄🫧說能看到很多恩瑟很開心還說台北的應援法太讚了 ㅠㅠ   \n",
       "2        rejiya1575                        Tahajjud is life changer. ♥   \n",
       "3           jonblta  i dont wanna hear Warriors fans complaining ab...   \n",
       "4           _lync.m  Trải nghiệm đầu tiên khi đu Em Xinh: Yeolan và...   \n",
       "...             ...                                                ...   \n",
       "6222        jauyiwu  連續三天走在路上，聽到不同的人說要去白沙屯。￼我想，這已經變成台灣人一個新的時間體感，就好像...   \n",
       "6223    __mi_achool                               收到vip 了 原價讓一張 A2 2XX   \n",
       "6224       lhs_7632  分手第十八天 偶然看見你的限時動態，這是十八天內換的第二個男生了，打開了你追蹤名單 不管是我...   \n",
       "6225  taro_puree_05                              我有說我要去嗎 你就給我第一排的票 虫合❓   \n",
       "6226     xuan._0603                   演唱會遇到最靠北的事： 錄到最喜歡的part手機跳儲存空間不足。   \n",
       "\n",
       "      content_length lang        scrape_time post_weekday  post_hour  \\\n",
       "0                 29   Ch  2025年04月30日 08:30    Wednesday          8   \n",
       "1                 29   Ch  2025年04月29日 02:17      Tuesday          2   \n",
       "2                 27   lg  2025年04月28日 05:19       Monday          5   \n",
       "3                 87   en  2025年05月03日 19:29     Saturday         19   \n",
       "4                 80   vi  2025年05月03日 08:40     Saturday          8   \n",
       "...              ...  ...                ...          ...        ...   \n",
       "6222              57   Ch  2025年05月01日 17:40     Thursday         17   \n",
       "6223              20   Ch  2025年04月29日 07:25      Tuesday          7   \n",
       "6224             124   Ch  2025年04月30日 07:03    Wednesday          7   \n",
       "6225              21   Ch  2025年05月01日 03:09     Thursday          3   \n",
       "6226              32   Ch  2025年04月29日 02:44      Tuesday          2   \n",
       "\n",
       "     post_period  viral                 post_time  ... emoji_count  \\\n",
       "0        morning      0  2025-04-29T12:59:30.000Z  ...           1   \n",
       "1          night      0         2025年04月28日 02:23  ...           1   \n",
       "2        morning      0         2025年04月27日 18:08  ...           1   \n",
       "3        evening      0  2025-05-03T03:09:05.000Z  ...           0   \n",
       "4        morning      1  2025-05-02T11:43:24.000Z  ...           0   \n",
       "...          ...    ...                       ...  ...         ...   \n",
       "6222     evening      0  2025-05-01T00:46:32.000Z  ...           0   \n",
       "6223     morning      0         2025年04月28日 17:58  ...           0   \n",
       "6224     morning      0  2025-04-28T22:51:13.000Z  ...           0   \n",
       "6225       night      0  2025-04-30T05:43:31.000Z  ...           1   \n",
       "6226       night      0         2025年04月28日 12:45  ...           0   \n",
       "\n",
       "                   scrape_time_origin  has_question  has_exclaim  has_url  \\\n",
       "0    2025-04-30 08:30:58.388029+08:00         False        False    False   \n",
       "1    2025-04-29 02:17:59.586251+08:00         False        False    False   \n",
       "2    2025-04-28 05:19:53.327702+08:00         False        False    False   \n",
       "3    2025-05-03 19:29:20.954467+08:00         False        False    False   \n",
       "4    2025-05-03 08:40:12.796089+08:00         False        False    False   \n",
       "...                               ...           ...          ...      ...   \n",
       "6222 2025-05-01 17:40:50.710474+08:00         False        False    False   \n",
       "6223 2025-04-29 07:25:02.893557+08:00         False        False    False   \n",
       "6224 2025-04-30 07:03:46.817921+08:00         False        False    False   \n",
       "6225 2025-05-01 03:09:51.931280+08:00         False        False    False   \n",
       "6226 2025-04-29 02:44:28.814612+08:00         False        False    False   \n",
       "\n",
       "      has_mention  has_hashtag  \\\n",
       "0           False        False   \n",
       "1           False        False   \n",
       "2           False        False   \n",
       "3           False        False   \n",
       "4           False        False   \n",
       "...           ...          ...   \n",
       "6222        False        False   \n",
       "6223        False        False   \n",
       "6224        False        False   \n",
       "6225        False        False   \n",
       "6226        False        False   \n",
       "\n",
       "                                      processed_content  view_class  label  \n",
       "0             老實 說 我現 還是 搞不懂 一場 演唱 會 搶 這麼 多次 票 意義在 哪裡 💧         low      1  \n",
       "1          吳海 嫄 🫧 說 能 看到 很多 恩瑟 很 開心 還說 台北 應援法 太 讚   ㅠ ㅠ      medium      2  \n",
       "2                  Tahajjud   is   life   changer .   ♥         low      1  \n",
       "3     i   dont   wanna   hear   Warriors   fans   co...         low      1  \n",
       "4     Tr ả i   nghi ệ m   đ ầ u   ti ê n   khi   đ u...        high      0  \n",
       "...                                                 ...         ...    ...  \n",
       "6222  連續 三天 走 路上 ， 聽到 不同 人 說 要 去 白沙 屯 。 ￼ 我 想 ， 這已 經...      medium      2  \n",
       "6223                      收到 vip     原價 讓 一張   A2   2XX         low      1  \n",
       "6224  分手 第十八天   偶然 看見 你 限時 動態 ， 這是 十八天 內換 第二 個 男生 ， ...      medium      2  \n",
       "6225                  我 說 我 要 去 嗎   你 就給 我 第一排 票   虫合 ❓      medium      2  \n",
       "6226    演唱 會 遇到 最 靠北 事 ：   錄到 最 喜歡 part 手機 跳 儲存 空間 不足 。         low      1  \n",
       "\n",
       "[6227 rows x 32 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將 view_count 分成四類：\n",
    "# 0: 小於 1000\n",
    "# 1: 1000 ~ 9999\n",
    "# 2: 10000 ~ 99999\n",
    "# 3: 100000 以上\n",
    "\n",
    "def map_view_class(x):\n",
    "    if x < 1000:\n",
    "        return 'low'\n",
    "    elif x < 10000:\n",
    "        return 'medium'\n",
    "    elif x < 100000:\n",
    "        return 'high'\n",
    "    else:\n",
    "        return 'very_high'\n",
    "\n",
    "df['view_class'] = df['view_count'].apply(map_view_class)\n",
    "\n",
    "# 編碼成數字 label\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['view_class'])\n",
    "\n",
    "df_high = df[df['view_class'] == 'very_high']\n",
    "df_medium = df[df['view_class'] == 'high']\n",
    "df_low = df[df['view_class'] == 'medium']\n",
    "df_very_low = df[df['view_class'] == 'low']\n",
    "\n",
    "# 針對較少的類別進行擴增（假設 high 和 very_low 比較少）\n",
    "df_high_oversampled = pd.concat([df_high] * 3, ignore_index=True)\n",
    "df_very_low_oversampled = pd.concat([df_very_low] * 3, ignore_index=True)\n",
    "\n",
    "# 合併並打亂\n",
    "df = pd.concat([df_medium, df_low, df_high_oversampled, df_very_low_oversampled], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j32CLLcICUzu"
   },
   "outputs": [],
   "source": [
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df['input_ids'].tolist()\n",
    "        self.attention_mask = df['attention_mask'].tolist()\n",
    "        self.numerics = df[num_cols].values\n",
    "        self.targets = df['target'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'numerics': torch.tensor(self.numerics[idx], dtype=torch.float),\n",
    "            'target': torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeqEslof2OBB"
   },
   "source": [
    "# Normalization 數值特徵標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "N3pJMu-g9on2"
   },
   "outputs": [],
   "source": [
    "# Normalization 數值特徵標準化\n",
    "base_num_cols = ['like_count', 'share_count', 'repost_count', 'reply_count', 'emoji_count', 'has_photo', 'has_video', 'has_question', 'has_exclaim', 'has_mention', 'has_url', 'has_hashtag', 'content_length']\n",
    "# 找出 one-hot 編碼的欄位（語言類型、發文時段、星期幾等類別欄位）\n",
    "# 使用 StandardScaler 將數值欄位轉換為「標準常態分布」（mean=0, std=1），有助於模型學習穩定。\n",
    "onehot_cols = [col for col in df.columns if col.startswith('lang_') or col.startswith('post_period_') or col.startswith('post_weekday_')]\n",
    "num_cols = base_num_cols + onehot_cols\n",
    "scaler = StandardScaler()\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "# Tokenizer 文本編碼 ：使用事先定義好的 tokenizer（例如 MacBERT、RoBERTa）對貼文進行斷詞、編碼\n",
    "# 將編碼後的結果儲存到 df 中，這兩個欄位會作為 BERT 模型的輸入\n",
    "encodings = tokenizer(df['content'].tolist(), truncation=True, padding='max_length', max_length=128)\n",
    "df['input_ids'] = encodings['input_ids']  # 斷詞後對應的詞彙 ID\n",
    "df['attention_mask'] = encodings['attention_mask']  # 對應位置是否是 padding（0）或實際內容（1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsA6q_qf9ojJ"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OBPv79a10W0"
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df['input_ids'].tolist()\n",
    "        self.attention_mask = df['attention_mask'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.numerics = df[num_cols].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'numerics': torch.tensor(self.numerics[idx], dtype=torch.float),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_iVSfG82Gpt"
   },
   "source": [
    "#模型架構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "um-YKfVt10Uf"
   },
   "outputs": [],
   "source": [
    "#模型架構\n",
    "# 1. FusionMacBERT：BERT + 數值特徵 concat\n",
    "class FusionMacBERTModel(nn.Module):\n",
    "    def __init__(self, model_name, num_numeric_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size + 64, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        cls_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        num_out = torch.relu(self.numeric_fc(numerics))\n",
    "        combined = torch.cat((cls_output, num_out), dim=1)\n",
    "        return self.classifier(self.dropout(combined))\n",
    "\n",
    "# 2. PureMacBERT：只有文字\n",
    "class PureMacBERTModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics=None):\n",
    "        cls_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        return self.classifier(self.dropout(cls_output))\n",
    "\n",
    "# 3. NumericOnly：只有數值特徵\n",
    "class NumericOnlyModel(nn.Module):\n",
    "    def __init__(self, num_numeric_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_numeric_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, numerics=None):\n",
    "        return self.classifier(numerics)\n",
    "\n",
    "# 4. BiLSTMWithNumeric：LSTM 處理詞嵌入 + 數值特徵\n",
    "class BiLSTMWithNumeric(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_numeric_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2 + 64, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        x = self.embedding(input_ids)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        pooled = lstm_out[:, -1, :]\n",
    "        num_out = torch.relu(self.numeric_fc(numerics))\n",
    "        combined = torch.cat((pooled, num_out), dim=1)\n",
    "        return self.classifier(self.dropout(combined))\n",
    "\n",
    "# 5. MacBERTWithGRU：BERT + GRU + 數值特徵\n",
    "class MacBERTWithGRU(nn.Module):\n",
    "    def __init__(self, model_name, num_numeric_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.gru = nn.GRU(self.bert.config.hidden_size, 128, batch_first=True, bidirectional=True)\n",
    "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
    "        self.classifier = nn.Linear(128*2 + 64, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        gru_out, _ = self.gru(bert_out)\n",
    "        pooled = gru_out[:, -1, :]\n",
    "        num_out = torch.relu(self.numeric_fc(numerics))\n",
    "        combined = torch.cat((pooled, num_out), dim=1)\n",
    "        return self.classifier(self.dropout(combined))\n",
    "\n",
    "# 6. MacBERTMLPFusion：BERT + 數值特徵 -> MLP\n",
    "class MacBERTMLPFusion(nn.Module):\n",
    "    def __init__(self, model_name, num_numeric_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size + num_numeric_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        cls_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        combined = torch.cat((cls_output, numerics), dim=1)\n",
    "        return self.fc(combined)\n",
    "\n",
    "# 7. TextCNNMacBERT：BERT 輸出卷積後 + 數值特徵\n",
    "class TextCNNMacBERT(nn.Module):\n",
    "    def __init__(self, model_name, num_numeric_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, 64, (k, self.bert.config.hidden_size)) for k in [2, 3, 4]])\n",
    "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
    "        self.classifier = nn.Linear(64 * len([2, 3, 4]) + 64, num_classes)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = torch.relu(conv(x)).squeeze(3)\n",
    "        x = torch.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        x = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state.unsqueeze(1)\n",
    "        cnn_out = torch.cat([self.conv_and_pool(x, conv) for conv in self.convs], 1)\n",
    "        num_out = torch.relu(self.numeric_fc(numerics))\n",
    "        combined = torch.cat((cnn_out, num_out), dim=1)\n",
    "        return self.classifier(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "De3g_-Qs2Vux"
   },
   "outputs": [],
   "source": [
    "#訓練與評估\n",
    "def train_and_eval(model, name, preview_count=10):\n",
    "    model = model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    loss_fn = FocalLoss()\n",
    "    # loss_fn = nn.MSELoss()\n",
    "\n",
    "    # 訓練階段\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            output = model(input_ids, attention_mask, numerics)\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # 評估階段\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            output = model(input_ids, attention_mask, numerics)\n",
    "            preds = output.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    mae = mean_absolute_error(all_targets, all_preds)\n",
    "    print(f\"[{name} 評估結果] MSE: {mse:.2f} | MAE: {mae:.2f}\")\n",
    "    print(f\"\\n{name} 評估結果：\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "    '''\n",
    "    preview_shown = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            output = model(input_ids, attention_mask, numerics)\n",
    "            preds = output.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            #印出前幾筆的預測、真實值\n",
    "            if preview_shown < preview_count:\n",
    "                batch_size = input_ids.shape[0]\n",
    "                for i in range(batch_size):\n",
    "                    if preview_shown >= preview_count:\n",
    "                        break\n",
    "                    input_id = input_ids[i].cpu().numpy()\n",
    "                    text = tokenizer.decode(input_id, skip_special_tokens=True)\n",
    "                    print(f\"\\n[{name} 預測] 第 {preview_shown+1} 筆\")\n",
    "                    print(f\"Text: {text}\")\n",
    "                    print(f\"Predicted: {label_encoder.inverse_transform([preds[i]])[0]}\")\n",
    "                    print(f\"Actual:    {label_encoder.inverse_transform([labels[i].cpu().item()])[0]}\")\n",
    "                    preview_shown += 1\n",
    "\n",
    "    print(f\"\\n{name} 評估結果：\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wqejxS910SJ"
   },
   "outputs": [],
   "source": [
    "# 資料分割：資料集切分與取樣\n",
    "dataset = CustomDataset(df)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, len(dataset)-train_size])\n",
    "train_labels = [train_dataset[i]['labels'].item() for i in range(len(train_dataset))]\n",
    "class_counts = pd.Series(train_labels).value_counts().to_dict()\n",
    "weights = [1.0 / class_counts[label] for label in train_labels]\n",
    "sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, sampler=sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# 執行多模型訓練\n",
    "model_variants = {\n",
    "    \"FusionMacRegressor\": FusionMacBERTRegressor(\"hfl/chinese-macbert-base\", len(num_cols), 3),\n",
    "    \"FusionMacBERT\": FusionMacBERTModel(\"hfl/chinese-macbert-base\", len(num_cols), 3),\n",
    "    \"PureMacBERT\": PureMacBERTModel(\"hfl/chinese-macbert-base\", 3),\n",
    "    \"NumericOnly\": NumericOnlyModel(len(num_cols), 3),\n",
    "    \"BiLSTMWithNumeric\": BiLSTMWithNumeric(tokenizer.vocab_size, 128, 128, len(num_cols), 3),\n",
    "    \"MacBERTWithGRU\": MacBERTWithGRU(\"hfl/chinese-macbert-base\", len(num_cols), 3),\n",
    "    \"MacBERTMLPFusion\": MacBERTMLPFusion(\"hfl/chinese-macbert-base\", len(num_cols), 3),\n",
    "    \"TextCNNMacBERT\": TextCNNMacBERT(\"hfl/chinese-macbert-base\", len(num_cols), 3),\n",
    "    \"RoBERTa\": FusionMacBERTModel(\"hfl/chinese-roberta-wwm-ext\", len(num_cols), 3),\n",
    "    \"BERTwwmExt\": FusionMacBERTModel(\"hfl/chinese-bert-wwm-ext\", len(num_cols), 3),\n",
    "    \"ERNIE\": FusionMacBERTModel(\"nghuyong/ernie-3.0-base-zh\", len(num_cols), 3),\n",
    "    \"ConvBERT\": FusionMacBERTModel(\"YituTech/conv-bert-base\", len(num_cols), 3)\n",
    "}\n",
    "\n",
    "# 逐個模型訓練與輸出結果\n",
    "for name, model in model_variants.items():\n",
    "    tokenizer_name = model_tokenizer_map.get(name, default_tokenizer_name)\n",
    "    if tokenizer_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        encodings = tokenizer(df['content'].tolist(), truncation=True, padding='max_length', max_length=128)\n",
    "        df['input_ids'] = encodings['input_ids']\n",
    "        df['attention_mask'] = encodings['attention_mask']\n",
    "    train_and_eval(model, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WP2MxFtLUhR"
   },
   "source": [
    "1. FusionMacBERT ✅文字 + ✅數值\n",
    "BERT： 使用 MacBERT\n",
    "架構： 把 [CLS] 向量與數值特徵經過 MLP 融合\n",
    "用途： 做為 baseline 融合模型\n",
    "優點： 同時考慮內容語義與貼文統計資料（如按讚數、是否有 hashtag）\n",
    "\n",
    "2. PureMacBERT ✅文字 + ❌數值\n",
    "BERT： 使用 MacBERT\n",
    "架構： 單純使用 [CLS]，後接 linear 層分類\n",
    "用途： 純語言模型 baseline\n",
    "對照： 可用來比較是否有數值輔助提升效果\n",
    "\n",
    "3. NumericOnly ❌文字 + ✅數值\n",
    "模型類型： 只有數值輸入，經過 MLP 做分類\n",
    "用途： 測試「只靠貼文統計資料」能否達到合理分類\n",
    "對照： 可與文字模型或融合模型對比效果\n",
    "\n",
    "4. BiLSTMWithNumeric ✅文字（Embedding+LSTM）+ ✅數值\n",
    "嵌入方式： 使用 nn.Embedding + BiLSTM 處理文字（不是 BERT）\n",
    "融合方式： 將 LSTM 最後時間步 + 數值特徵拼接\n",
    "特別點： 測試「非 Transformer 模型」是否仍具競爭力\n",
    "\n",
    "5. MacBERTWithGRU ✅文字（MacBERT）+ ✅數值\n",
    "文字處理： MacBERT 之後再串 GRU\n",
    "融合方式： GRU 輸出最後一步拼接數值特徵\n",
    "意圖： 想看 BERT+RNN 的表現 vs. 傳統 BERT\n",
    "\n",
    "6. MacBERTMLPFusion ✅文字 + ✅數值\n",
    "處理方式： 文字與數值直接拼接後進入 MLP\n",
    "不同於 FusionMacBERT：\n",
    "沒有額外處理數值特徵（如沒有經過 nn.Linear)\n",
    "更單純的融合設計（屬於 Early Fusion）\n",
    "\n",
    "7. TextCNNMacBERT ✅文字 + ✅數值\n",
    "模型組合：\n",
    "使用 BERT 編碼後丟進 CNN filter (TextCNN)\n",
    "再與數值特徵融合\n",
    "用途： 測試 BERT 結合 CNN 特徵提取是否提升效果\n",
    "有趣點： 有些短文模型（如微博、Threads）對 CNN 特徵抓取敏感\n",
    "\n",
    "8. RoBERTa ✅文字 + ✅數值\n",
    "BERT 替代品： 改用 RoBERTa（中文版本）\n",
    "融合方式： 同 FusionMacBERT\n",
    "實驗目的： 測試不同語言模型對結果的影響（語言模型 ablation）\n",
    "\n",
    "\n",
    "9. BERTwwmExt ✅文字 + ✅數值\n",
    "BERT： 使用 Chinese BERT whole-word-masking 擴展版\n",
    "比較目的： 同上，用於測試不同語言模型特性的影響\n",
    "\n",
    "10. ERNIE ✅文字 + ✅數值\n",
    "BERT： 改用百度的 ERNIE（引入知識增強）\n",
    "適用場景： 當文本與常識有關（如話題、用語）\n",
    "目的： 評估知識型語言模型在社群文本分類的效果\n",
    "\n",
    "11. ConvBERT ✅文字 + ✅數值\n",
    "模型特色： 使用 Convolution + Self-Attention 混合架構的 BERT\n",
    "實驗意義： 試驗非傳統 Self-Attention 模型是否有優勢\n",
    "\n",
    "\n",
    "| 模型名稱              | 說明               | 是否融合 | 文本處理法         | 特殊處理       |\n",
    "| ----------------- | ---------------- | ---- | ------------- | ---------- |\n",
    "| FusionMacBERT     | BERT + 數值特徵      | ✅    | MacBERT       | 自製融合層      |\n",
    "| PureMacBERT       | 純文本模型            | ❌    | MacBERT       | baseline   |\n",
    "| NumericOnly       | 純統計數值            | ❌    | 無             | MLP only   |\n",
    "| BiLSTMWithNumeric | LSTM + 數值        | ✅    | nn.Embedding  | 不使用 BERT   |\n",
    "| MacBERTWithGRU    | BERT + GRU + 數值  | ✅    | MacBERT + GRU | 時序特徵強化     |\n",
    "| MacBERTMLPFusion  | BERT + 數值        | ✅    | MacBERT       | 拼接後進 MLP   |\n",
    "| TextCNNMacBERT    | BERT + CNN + 數值  | ✅    | MacBERT + CNN | 模仿 TextCNN |\n",
    "| RoBERTa           | 換 BERT backbone  | ✅    | RoBERTa       | 模型比較       |\n",
    "| BERTwwmExt        | 換 BERT backbone  | ✅    | BERT-wwm      | 模型比較       |\n",
    "| ERNIE             | 引入知識的 BERT       | ✅    | ERNIE         | 模型比較       |\n",
    "| ConvBERT          | 混合卷積 + 注意力的 BERT | ✅    | ConvBERT      | 模型比較       |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBCtDWKlz6m8"
   },
   "outputs": [],
   "source": [
    "# 資料分割：資料集切分與取樣\n",
    "dataset = CustomDataset(df)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, len(dataset)-train_size])\n",
    "train_labels = [train_dataset[i]['labels'].item() for i in range(len(train_dataset))]\n",
    "class_counts = pd.Series(train_labels).value_counts().to_dict()\n",
    "weights = [1.0 / class_counts[label] for label in train_labels]\n",
    "sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, sampler=sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# 執行多模型訓練\n",
    "model_variants = {\n",
    "    \"FusionMacBERT\": FusionMacBERTModel(\"hfl/chinese-macbert-base\", len(num_cols), 3),\n",
    "    \"PureMacBERT\": PureMacBERTModel(\"hfl/chinese-macbert-base\", 3),\n",
    "    \"NumericOnly\": NumericOnlyModel(len(num_cols), 3),\n",
    "    \"BiLSTMWithNumeric\": BiLSTMWithNumeric(tokenizer.vocab_size, 128, 128, len(num_cols), 3),\n",
    "    \"MacBERTWithGRU\": MacBERTWithGRU(\"hfl/chinese-macbert-base\", len(num_cols), 3),\n",
    "    \"MacBERTMLPFusion\": MacBERTMLPFusion(\"hfl/chinese-macbert-base\", len(num_cols), 3),\n",
    "    \"TextCNNMacBERT\": TextCNNMacBERT(\"hfl/chinese-macbert-base\", len(num_cols), 3),\n",
    "    \"RoBERTa\": FusionMacBERTModel(\"hfl/chinese-roberta-wwm-ext\", len(num_cols), 3),\n",
    "    \"BERTwwmExt\": FusionMacBERTModel(\"hfl/chinese-bert-wwm-ext\", len(num_cols), 3),\n",
    "    \"ERNIE\": FusionMacBERTModel(\"nghuyong/ernie-3.0-base-zh\", len(num_cols), 3),\n",
    "    \"ConvBERT\": FusionMacBERTModel(\"YituTech/conv-bert-base\", len(num_cols), 3)\n",
    "}\n",
    "\n",
    "# 逐個模型訓練與輸出結果\n",
    "for name, model in model_variants.items():\n",
    "    tokenizer_name = model_tokenizer_map.get(name, default_tokenizer_name)\n",
    "    if tokenizer_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        encodings = tokenizer(df['content'].tolist(), truncation=True, padding='max_length', max_length=128)\n",
    "        df['input_ids'] = encodings['input_ids']\n",
    "        df['attention_mask'] = encodings['attention_mask']\n",
    "    train_and_eval(model, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sd8HLHR6VL1"
   },
   "source": [
    "# 迴歸預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "291iyZhHxpj9"
   },
   "outputs": [],
   "source": [
    "# 原本這樣分類（要拿掉）\n",
    "# df['view_class'] = ...\n",
    "# df['label'] = ...\n",
    "\n",
    "# 直接用原始 view_count 作為 regression target\n",
    "df = df.dropna(subset=[\"content\", \"view_count\"])\n",
    "df['target'] = df['view_count'].apply(parse_count)  # 如果 view_count 不是數字要先轉換\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "dedliSNv7UGU"
   },
   "outputs": [],
   "source": [
    "def train_and_eval_regression(model, name):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # 訓練\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            targets = batch['labels'].float().to(device)  # 重要：labels 必須是 float\n",
    "            preds = model(input_ids, attention_mask, numerics)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # 評估\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            targets = batch['labels'].float().to(device)\n",
    "            preds = model(input_ids, attention_mask, numerics)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    mae = mean_absolute_error(all_targets, all_preds)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "    print(f\"\\n{name}  MSE: {mse:.2f} | MAE: {mae:.2f}| R2: {r2:.2f}\")\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "    return all_targets, all_preds\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5c7CQi1v6wwI"
   },
   "outputs": [],
   "source": [
    "class FusionMacBERTRegressor(nn.Module):\n",
    "    def __init__(self, model_name, num_numeric_features, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
    "        self.regressor = nn.Linear(self.bert.config.hidden_size + 64, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        cls_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        num_out = torch.relu(self.numeric_fc(numerics))\n",
    "        combined = torch.cat((cls_output, num_out), dim=1)\n",
    "        return self.regressor(self.dropout(combined)).squeeze(1)  # (batch,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kTncOradCUz8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "rm -rf ~/.cache/huggingface/transformers/hfl__chinese-macbert-base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ptY9GnoACUz8"
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "'''\n",
    "class CustomDatasetRegression(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df['input_ids'].tolist()\n",
    "        self.attention_mask = df['attention_mask'].tolist()\n",
    "        self.labels = df['label'].astype(float).values   # ← 為回歸任務需轉成 float\n",
    "        self.numerics = df[num_cols].astype(float).values  # ← 確保為 float array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'numerics': torch.tensor(self.numerics[idx], dtype=torch.float),  # ← 修正為 float\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.float)       # ← 修正為 float\n",
    "        }\n",
    "'''\n",
    "class CustomDatasetRegression(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df['input_ids'].tolist()\n",
    "        self.attention_mask = df['attention_mask'].tolist()\n",
    "        self.labels = df['target'].astype(float).values\n",
    "        self.numerics = df[num_cols].astype(float).values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'numerics': torch.tensor(self.numerics[idx], dtype=torch.float),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Rlezwnry2K0h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FusionMacBERTRegressor  MSE: 4410895810.43 | MAE: 10676.62 | R²: -0.03\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = CustomDatasetRegression(df)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "\n",
    "class FusionMacBERTRegressor(nn.Module):\n",
    "    def __init__(self, model_name, num_numeric_features, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
    "        self.regressor = nn.Linear(self.bert.config.hidden_size + 64, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        cls_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        num_out = torch.relu(self.numeric_fc(numerics))\n",
    "        combined = torch.cat((cls_output, num_out), dim=1)\n",
    "        return self.regressor(self.dropout(combined)).squeeze(1)\n",
    "\n",
    "\n",
    "def train_and_eval_regression(model, name):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            targets = batch['labels'].float().to(device)\n",
    "\n",
    "            preds = model(input_ids, attention_mask, numerics)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            targets = batch['labels'].float().to(device)\n",
    "\n",
    "            preds = model(input_ids, attention_mask, numerics)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    mae = mean_absolute_error(all_targets, all_preds)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f\"\\n{name}  MSE: {mse:.2f} | MAE: {mae:.2f} | R²: {r2:.2f}\")\n",
    "    return all_targets, all_preds\n",
    "\n",
    "\n",
    "model = FusionMacBERTRegressor(\"hfl/chinese-macbert-base\", len(num_cols))\n",
    "all_targets, all_preds = train_and_eval_regression(model, \"FusionMacBERTRegressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQwvt6LQ10Pi"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RF-tbCHU10Ea"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmUJQfroFBqt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEyKqKjE9oZy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OeqnHopFJuT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0DyH1z7FJm3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnoabF2fFJgR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaucnuMEFCV-"
   },
   "source": [
    "#**下面都是舊的東西而已~~~~**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvuTn85bUTA5"
   },
   "outputs": [],
   "source": [
    "stopwords = set(['的', '了', '在', '是', '和', '也', '與', '有', '為', '等'])\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    words = jieba.cut(text)\n",
    "    words_filtered = [word for word in words if word not in stopwords]\n",
    "    return ' '.join(words_filtered)\n",
    "\n",
    "df['processed_content'] = df['Content'].apply(tokenize_and_remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nzhynZN7IYy"
   },
   "source": [
    "new_article = \"IC 設計大廠聯發科 (2454-TW) 副董事長暨執行長蔡力行今 (26) 日獲頒潘文淵獎，會後受訪表示，聯發科 3 奈米會在台積電 (2330-TW)(TSM-US) 做，且由於先進製程技術相當複雜，不論要採用或更換都非常困難，雙方會持續緊密合作。外界今日提問不論是輝達 (NVDA-US)、蘋果 (AAPL-US) 等都表示尋求多元的晶圓代工方案，蔡力行回應，聯發科在先進製程持續與台積電緊密合作，英特爾 (INTC-US) 則負責 16 奈米蔡力行也強調，聯發科不會只停在採用 4 奈米，也會採用 3 奈米製程，此外，由於電晶體微縮速度趨緩，儘管技術上可行，但不一定符合經濟效益，因此技術也逐步從平面變成 2D、2.5D，甚至 3D 等，先進封裝的重要性比以前增加。至於跟輝達合作，蔡力行重申，雙方合作仍以汽車為主，輝達布局車用比聯發科早，主要著墨在智慧座艙與 ADAS 系統，雙方有很好的配合，其中，輝達主攻高階、聯發科則瞄準中階，雙方正密切合作開會。\"\n",
    "processed_new_article = tokenize_and_remove_stopwords(new_article)\n",
    "\n",
    "# Random Forest 模型訓練與預測\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 使用相同的數據分割方式\n",
    "X_train_rf, X_val_rf, y_train_rf, y_val_rf = train_test_split(tfidf_matrix, y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train_rf, X_val_rf, y_train_rf, y_val_rf = train_test_split(X_train_rf, y_train_rf, test_size=0.1, random_state=42)\n",
    "\n",
    "# 創建隨機森林模型\n",
    "rand_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 訓練隨機森林模型\n",
    "rand_forest_model.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "# 預測\n",
    "y_val_pred_rf = rand_forest_model.predict(X_val_rf)\n",
    "y_test_pred_rf = rand_forest_model.predict(X_test)\n",
    "\n",
    "# 分類報告\n",
    "print(\"驗證集 Validation Classification Report:\")\n",
    "print(classification_report(y_val_rf, y_val_pred_rf))\n",
    "\n",
    "print(\"\\n測試集 Test Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_rf))\n",
    "\n",
    "# 對照表\n",
    "result_df_val_rf = pd.DataFrame({'Actual': y_val_rf, 'Predicted': y_val_pred_rf})\n",
    "result_df_test_rf = pd.DataFrame({'Actual': y_test, 'Predicted': y_test_pred_rf})\n",
    "\n",
    "print(\"驗證集 Validation Result Comparison:\")\n",
    "print(result_df_val_rf)\n",
    "\n",
    "print(\"\\n測試集 Test Result Comparison:\")\n",
    "print(result_df_test_rf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTn9buro9xlD"
   },
   "outputs": [],
   "source": [
    "new_article = \"IC 設計大廠聯發科 (2454-TW) 副董事長暨執行長蔡力行今 (26) 日獲頒潘文淵獎，會後受訪表示，聯發科 3 奈米會在台積電 (2330-TW)(TSM-US) 做，且由於先進製程技術相當複雜，不論要採用或更換都非常困難，雙方會持續緊密合作。外界今日提問不論是輝達 (NVDA-US)、蘋果 (AAPL-US) 等都表示尋求多元的晶圓代工方案，蔡力行回應，聯發科在先進製程持續與台積電緊密合作，英特爾 (INTC-US) 則負責 16 奈米蔡力行也強調，聯發科不會只停在採用 4 奈米，也會採用 3 奈米製程，此外，由於電晶體微縮速度趨緩，儘管技術上可行，但不一定符合經濟效益，因此技術也逐步從平面變成 2D、2.5D，甚至 3D 等，先進封裝的重要性比以前增加。至於跟輝達合作，蔡力行重申，雙方合作仍以汽車為主，輝達布局車用比聯發科早，主要著墨在智慧座艙與 ADAS 系統，雙方有很好的配合，其中，輝達主攻高階、聯發科則瞄準中階，雙方正密切合作開會。\"\n",
    "\n",
    "processed_new_article = tokenize_and_remove_stopwords(new_article)\n",
    "\n",
    "print(processed_new_article)\n",
    "\n",
    "# 將新文章轉換為 TF-IDF 表示形式\n",
    "new_article_tfidf = tfidf_vectorizer.transform([processed_new_article])\n",
    "\n",
    "# 使用投票分類器進行預測\n",
    "predicted_label_ensemble = voting_classifier.predict(new_article_tfidf)\n",
    "\n",
    "print(f\"新文章預測結果: {predicted_label_ensemble}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjsKVnf0-iiU"
   },
   "outputs": [],
   "source": [
    "# 使用 inverse_transform 將預測的數字編碼轉換回原始標籤\n",
    "predicted_label_original = label_encoder.inverse_transform(predicted_label_ensemble)\n",
    "\n",
    "print(f\"新文章預測結果（原始標籤）: {predicted_label_original}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMJI_upxwRgP"
   },
   "source": [
    "## 實際預測（研究）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gbdm9r7_qFq"
   },
   "outputs": [],
   "source": [
    "# 獲取所有標籤對應的編碼\n",
    "all_labels = label_encoder.classes_\n",
    "\n",
    "print(\"所有標籤對應的編碼:\")\n",
    "for label_code, label in enumerate(all_labels):\n",
    "    print(f\"編碼 {label_code}: 標籤 {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTk9I4ZtEPBP"
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "# 定義停用詞\n",
    "stopwords = set(['的', '了', '在', '是', '和', '也', '與', '有', '為', '等'])\n",
    "\n",
    "# 定義分詞並去除停用詞的函數\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    words = jieba.cut(text)  # 使用 jieba 进行分词\n",
    "    words_filtered = [word for word in words if word not in stopwords]  # 去除停用词\n",
    "    return ' '.join(words_filtered)\n",
    "\n",
    "# 將處理後的內容加入 DataFrame 中\n",
    "df['processed_content'] = df['Content'].apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "# 新文章\n",
    "new_article = \"台股守穩季線，週線三連紅。（資料照） 〔財經頻道／綜合報導〕美國CPI略高於市場預期，美股漲勢暫歇，本週以來，台股經過兩日大漲後，今（13）日指數震盪走低，終場下跌43.34點，以16782.57點作收，守住季線關卡，成交量為2986.08億元，週線上漲262點，呈現三連紅，緯創失守百元大關，AI族群普遍都是收黑，電子類股以矽光子、網通等次族群比較有表現，傳產輪動到營建、造紙、百貨等接棒演出。 前10大成交額個股漲多跌少，除了AI族群收黑，其他都是紅盤居多，廣達跌12元，收226元，成交額182.32億元，排名第1；台積電終場漲3元，收553元，成交額171.29億元，排名第2；矽統終場漲2.75元，收47.7元，成交額146.25億元，排名第3；定穎投控漲3.3元，收103元，成交額95.92億元，排名第4；緯創跌3.4元，收99.1元，成交額93.89億元，排名第5。 請繼續往下閱讀...  技嘉跌13.5元，收271元，成交額89.05億元，排名第6；創意收1695元平盤，成交額86.78億元，排名第7；聯發科上漲27元，收842元，成交額81.63億元，排名第8；裕隆漲1.1元，收85.1元，成交額66.51億元，排名第9；材料-KY漲 5元，收1185元，成交額63.34億元，排名第10。 一手掌握經濟脈動點我訂閱自由財經Youtube頻道 不用抽 不用搶 現在用APP看新聞 保證天天中獎點我下載APP按我看活動辦法 相關新聞\"\n",
    "\n",
    "# 處理新文章\n",
    "processed_new_article = tokenize_and_remove_stopwords(new_article)\n",
    "\n",
    "# 輸出處理後的文章\n",
    "print(processed_new_article)\n",
    "\n",
    "# 將新文章轉換為 TF-IDF 表示形式\n",
    "new_article_tfidf = tfidf_vectorizer.transform([processed_new_article])\n",
    "\n",
    "# 使用投票分類器進行預測\n",
    "predicted_label_ensemble = voting_classifier.predict(new_article_tfidf)\n",
    "\n",
    "# 輸出預測結果\n",
    "print(f\"新文章預測結果: {predicted_label_ensemble}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzB1yP1GFFki"
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "# 定義停用詞\n",
    "stopwords = set(['的', '了', '在', '是', '和', '也', '與', '有', '為', '等'])\n",
    "\n",
    "# 定義分詞並去除停用詞的函數\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    words = jieba.cut(text)  # 使用 jieba 进行分词\n",
    "    words_filtered = [word for word in words if word not in stopwords]  # 去除停用词\n",
    "    return ' '.join(words_filtered)\n",
    "\n",
    "# 將處理後的內容加入 DataFrame 中\n",
    "df['processed_content'] = df['Content'].apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "# 新文章\n",
    "new_article = \"小漲\"\n",
    "\n",
    "# 處理新文章\n",
    "processed_new_article = tokenize_and_remove_stopwords(new_article)\n",
    "\n",
    "# 輸出處理後的文章\n",
    "print(processed_new_article)\n",
    "\n",
    "# 將新文章轉換為 TF-IDF 表示形式\n",
    "new_article_tfidf = tfidf_vectorizer.transform([processed_new_article])\n",
    "\n",
    "# 使用投票分類器進行預測\n",
    "predicted_label_ensemble = voting_classifier.predict(new_article_tfidf)\n",
    "\n",
    "# 輸出預測結果\n",
    "print(f\"新文章預測結果: {predicted_label_ensemble}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
