{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6sEhoTFDEH3"
   },
   "source": [
    "## å®‰è£å¥—ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2V9shIFDDG3J",
    "outputId": "72d8024f-64c8-41fc-8ca0-00a5bffe1574"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (0.42.1)\n",
      "Requirement already satisfied: emoji in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (2.14.1)\n",
      "Requirement already satisfied: langdetect in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: datasets in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (2.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (0.15.1)\n",
      "Requirement already satisfied: packaging in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: responses<0.19 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.0)\n",
      "Requirement already satisfied: filelock in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2024.12.14)\n",
      "Requirement already satisfied: six in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: lingua-language-detector in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba\n",
    "!pip install emoji langdetect\n",
    "!pip install datasets\n",
    "!pip install lingua-language-detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhtPvgrLCUzp",
    "outputId": "1ecc2382-688c-437a-bc60-6b7a4ea45862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.42.1)\n",
      "Requirement already satisfied: emoji in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.14.1)\n",
      "Requirement already satisfied: langdetect in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.0.9)\n",
      "Requirement already satisfied: pytz in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2024.1)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: lingua-language-detector in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.1.5)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.31.1)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install jieba emoji langdetect pytz torch lingua-language-detector datasets openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HGttCcMDKE0"
   },
   "source": [
    "## å¼•å…¥å¥—ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JQ6rzOpBTI5K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "import langdetect\n",
    "from langdetect import detect\n",
    "from lingua import LanguageDetectorBuilder, Language, IsoCode639_1\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zM7PgPNTPXMd",
    "outputId": "ca72f2aa-3135-4a1f-857e-ea5f956efe1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# è¨­å®šå¥½è·¯å¾‘ (å¾Œé¢éƒ½æ˜¯ä½¿ç”¨ç›¸å°è·¯å¾‘)\n",
    "base_path = '/content/drive/My Drive/SMA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "xOIyJJ3KSNT9",
    "outputId": "e38e0262-6047-4c2a-c225-454cac6940d1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>post_time</th>\n",
       "      <th>topic</th>\n",
       "      <th>time_info</th>\n",
       "      <th>content</th>\n",
       "      <th>has_photo</th>\n",
       "      <th>has_video</th>\n",
       "      <th>like_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>repost_count</th>\n",
       "      <th>share_count</th>\n",
       "      <th>view_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>post_url</th>\n",
       "      <th>scrape_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>2025-04-29T10:58:39.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3å°æ™‚</td>\n",
       "      <td>Thank you God for another day.</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>190</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3,197</td>\n",
       "      <td>141073</td>\n",
       "      <td>https://www.threads.net/@ayofvr/post/DJBymf8uTrK</td>\n",
       "      <td>2025-04-29T22:27:40.176749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-04-29T12:33:44.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1å°æ™‚</td>\n",
       "      <td>ç™¾é”ç¿¡éº—ï¼Ÿ æ²’æœ‰ä¸‹é™çš„ç¶²è·¯ç—…æ…‹ï¼</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>196</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>3Â è¬</td>\n",
       "      <td>77683</td>\n",
       "      <td>https://www.threads.net/@ban.mei.onnnnni/post/...</td>\n",
       "      <td>2025-04-29T22:27:54.964788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ribboworld2021</td>\n",
       "      <td>2025-04-29T04:39:46.000Z</td>\n",
       "      <td>å°ä¸€æ—¥å¸¸</td>\n",
       "      <td>9å°æ™‚</td>\n",
       "      <td>è€ƒå®ŒæœŸä¸­è€ƒï¼Œæˆç¸¾éƒ½é‚„æ²’å‡ºä¾†ï¼Œå°ä¸€å¥³å…’å°±è‡ªä¿¡çš„å°æˆ‘èªªï¼šã€Œæˆ‘çœŸç¾¨æ…•å¦³ç”Ÿäº†ä¸€å€‹å¤©æ‰ï¼ã€</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>75</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4,559</td>\n",
       "      <td>99</td>\n",
       "      <td>https://www.threads.net/@ribboworld2021/post/D...</td>\n",
       "      <td>2025-04-29T22:28:09.873641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>2025-04-29T11:25:22.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3å°æ™‚</td>\n",
       "      <td>Just be strong. Confident. Hopeful. Intellectu...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>83</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1,967</td>\n",
       "      <td>141093</td>\n",
       "      <td>https://www.threads.net/@ayofvr/post/DJB1qP5OmzP</td>\n",
       "      <td>2025-04-29T22:28:24.726576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jose_ykc</td>\n",
       "      <td>2025-04-29T10:36:19.000Z</td>\n",
       "      <td>è¼”ä»å¤§å­¸</td>\n",
       "      <td>3å°æ™‚</td>\n",
       "      <td>æ—¥æ–‡è¼”ç³»è€å¸«ä¸Šèª²å…§å®¹ä¹‹ä¸€AiScReam æ­Œè©å°è®€</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>4,334</td>\n",
       "      <td>55</td>\n",
       "      <td>513</td>\n",
       "      <td>2,460</td>\n",
       "      <td>10Â è¬</td>\n",
       "      <td>65</td>\n",
       "      <td>https://www.threads.net/@jose_ykc/post/DJBvpGI...</td>\n",
       "      <td>2025-04-29T22:28:39.393706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>leighton.williams</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 07:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8å°æ™‚</td>\n",
       "      <td>First tasting in California</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>726</td>\n",
       "      <td>30</td>\n",
       "      <td>https://www.threads.net/@leighton.williams/pos...</td>\n",
       "      <td>2025-04-29T16:23:40.328046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>cape__man</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 06:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9å°æ™‚</td>\n",
       "      <td>I hoped it would have been better.</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1,291</td>\n",
       "      <td>114</td>\n",
       "      <td>https://www.threads.net/@cape__man/post/DJAdFd...</td>\n",
       "      <td>2025-04-29T16:23:55.063517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>simimoonlight</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 06:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9å°æ™‚</td>\n",
       "      <td>I canâ€™t wait to watch BeyoncÃ© on TikTok tonigh...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>2,656</td>\n",
       "      <td>29</td>\n",
       "      <td>280</td>\n",
       "      <td>47</td>\n",
       "      <td>5.6Â è¬</td>\n",
       "      <td>65732</td>\n",
       "      <td>https://www.threads.net/@simimoonlight/post/DJ...</td>\n",
       "      <td>2025-04-29T16:24:39.870994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>other98</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 12:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3å°æ™‚</td>\n",
       "      <td>Tonight, Canada just proved that they have a h...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0Â è¬</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>22</td>\n",
       "      <td>14Â è¬</td>\n",
       "      <td>217182</td>\n",
       "      <td>https://www.threads.net/@other98/post/DJBGV3NxiX_</td>\n",
       "      <td>2025-04-29T16:24:54.669936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>scottiebeam</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 01:33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14å°æ™‚</td>\n",
       "      <td>Yall be fighting on here â€¦ i thought threads w...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>427</td>\n",
       "      <td>76</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1Â è¬</td>\n",
       "      <td>59249</td>\n",
       "      <td>https://www.threads.net/@scottiebeam/post/DI_6...</td>\n",
       "      <td>2025-04-29T16:25:09.396585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3101 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                 post_time topic time_info  \\\n",
       "0                ayofvr  2025-04-29T10:58:39.000Z   NaN       3å°æ™‚   \n",
       "1                   NaN  2025-04-29T12:33:44.000Z   NaN       1å°æ™‚   \n",
       "2        ribboworld2021  2025-04-29T04:39:46.000Z  å°ä¸€æ—¥å¸¸       9å°æ™‚   \n",
       "3                ayofvr  2025-04-29T11:25:22.000Z   NaN       3å°æ™‚   \n",
       "4              jose_ykc  2025-04-29T10:36:19.000Z  è¼”ä»å¤§å­¸       3å°æ™‚   \n",
       "...                 ...                       ...   ...       ...   \n",
       "3096  leighton.williams         2025å¹´04æœˆ29æ—¥ 07:30   NaN       8å°æ™‚   \n",
       "3097          cape__man         2025å¹´04æœˆ29æ—¥ 06:31   NaN       9å°æ™‚   \n",
       "3098      simimoonlight         2025å¹´04æœˆ29æ—¥ 06:26   NaN       9å°æ™‚   \n",
       "3099            other98         2025å¹´04æœˆ29æ—¥ 12:31   NaN       3å°æ™‚   \n",
       "3100        scottiebeam         2025å¹´04æœˆ29æ—¥ 01:33   NaN      14å°æ™‚   \n",
       "\n",
       "                                                content has_photo has_video  \\\n",
       "0                        Thank you God for another day.         N         N   \n",
       "1                                      ç™¾é”ç¿¡éº—ï¼Ÿ æ²’æœ‰ä¸‹é™çš„ç¶²è·¯ç—…æ…‹ï¼         Y         N   \n",
       "2              è€ƒå®ŒæœŸä¸­è€ƒï¼Œæˆç¸¾éƒ½é‚„æ²’å‡ºä¾†ï¼Œå°ä¸€å¥³å…’å°±è‡ªä¿¡çš„å°æˆ‘èªªï¼šã€Œæˆ‘çœŸç¾¨æ…•å¦³ç”Ÿäº†ä¸€å€‹å¤©æ‰ï¼ã€         N         N   \n",
       "3     Just be strong. Confident. Hopeful. Intellectu...         N         N   \n",
       "4                             æ—¥æ–‡è¼”ç³»è€å¸«ä¸Šèª²å…§å®¹ä¹‹ä¸€AiScReam æ­Œè©å°è®€         Y         Y   \n",
       "...                                                 ...       ...       ...   \n",
       "3096                        First tasting in California         N         N   \n",
       "3097                 I hoped it would have been better.         N         N   \n",
       "3098  I canâ€™t wait to watch BeyoncÃ© on TikTok tonigh...         N         N   \n",
       "3099  Tonight, Canada just proved that they have a h...         N         N   \n",
       "3100  Yall be fighting on here â€¦ i thought threads w...         N         N   \n",
       "\n",
       "     like_count reply_count repost_count share_count view_count  \\\n",
       "0           190           3           23         NaN      3,197   \n",
       "1           196          16          NaN           6        3Â è¬   \n",
       "2            75           6          NaN         NaN      4,559   \n",
       "3            83           5            3           1      1,967   \n",
       "4         4,334          55          513       2,460       10Â è¬   \n",
       "...         ...         ...          ...         ...        ...   \n",
       "3096          7         NaN          NaN           0        726   \n",
       "3097          2         NaN          NaN           0      1,291   \n",
       "3098      2,656          29          280          47      5.6Â è¬   \n",
       "3099      1.0Â è¬         214          214          22       14Â è¬   \n",
       "3100        427          76           28           1        1Â è¬   \n",
       "\n",
       "      followers_count                                           post_url  \\\n",
       "0              141073   https://www.threads.net/@ayofvr/post/DJBymf8uTrK   \n",
       "1               77683  https://www.threads.net/@ban.mei.onnnnni/post/...   \n",
       "2                  99  https://www.threads.net/@ribboworld2021/post/D...   \n",
       "3              141093   https://www.threads.net/@ayofvr/post/DJB1qP5OmzP   \n",
       "4                  65  https://www.threads.net/@jose_ykc/post/DJBvpGI...   \n",
       "...               ...                                                ...   \n",
       "3096               30  https://www.threads.net/@leighton.williams/pos...   \n",
       "3097              114  https://www.threads.net/@cape__man/post/DJAdFd...   \n",
       "3098            65732  https://www.threads.net/@simimoonlight/post/DJ...   \n",
       "3099           217182  https://www.threads.net/@other98/post/DJBGV3NxiX_   \n",
       "3100            59249  https://www.threads.net/@scottiebeam/post/DI_6...   \n",
       "\n",
       "                     scrape_time  \n",
       "0     2025-04-29T22:27:40.176749  \n",
       "1     2025-04-29T22:27:54.964788  \n",
       "2     2025-04-29T22:28:09.873641  \n",
       "3     2025-04-29T22:28:24.726576  \n",
       "4     2025-04-29T22:28:39.393706  \n",
       "...                          ...  \n",
       "3096  2025-04-29T16:23:40.328046  \n",
       "3097  2025-04-29T16:23:55.063517  \n",
       "3098  2025-04-29T16:24:39.870994  \n",
       "3099  2025-04-29T16:24:54.669936  \n",
       "3100  2025-04-29T16:25:09.396585  \n",
       "\n",
       "[3101 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è®€å–è³‡æ–™ï¼ˆè«‹ç¢ºèªä½ çš„ Excel è·¯å¾‘ï¼‰\n",
    "# df = pd.read_excel(base_path+\"/threads.xlsx\")\n",
    "df = pd.read_excel(\"threads.xlsx\", engine='openpyxl')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kIM9eoPtiDSC"
   },
   "outputs": [],
   "source": [
    "# === èªè¨€åµæ¸¬ä¿®æ­£ç‰ˆ===\n",
    "lingua_detector = LanguageDetectorBuilder.from_all_languages().with_preloaded_language_models().build()\n",
    "lingua_available = True\n",
    "def detect_lang_with_preprocessing_lingua(text):\n",
    "    original_text = text\n",
    "\n",
    "    # è‹¥æ˜¯ NaN æˆ–ç©ºå­—ä¸²å°±å›å‚³ \"unknown\"\n",
    "    if pd.isna(text):\n",
    "        return \"unknown\"\n",
    "    text = str(text).strip()\n",
    "    if not text:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    # ç§»é™¤ URLã€@æ¨™è¨˜ã€ #hashtagã€emojiã€å¤šé¤˜ç©ºç™½\n",
    "    try:\n",
    "      text_cleaned = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "      text_cleaned = re.sub(r'@\\w+', '', text_cleaned)\n",
    "      text_cleaned = re.sub(r'#\\w+', '', text_cleaned)\n",
    "      text_cleaned = emoji.replace_emoji(text_cleaned, replace='')\n",
    "      text_cleaned = re.sub(r'\\s+', ' ', text_cleaned).strip()\n",
    "    except Exception as e:\n",
    "      return \"error_state_preprocessing\"\n",
    "\n",
    "    # è‹¥é€™äº›æ¸…ç†å®Œå¾Œè®Šæˆç©ºå­—ä¸²\n",
    "    if not text_cleaned:\n",
    "      return \"empty_after_clean\"\n",
    "\n",
    "    # è‹¥æ–‡å­—ä¸­è¶…é 30% æ˜¯ä¸­æ–‡ï¼Œå°±ç›´æ¥åˆ¤å®šç‚º \"Ch\"ï¼ˆä¸­æ–‡ï¼‰\n",
    "    try:\n",
    "      chinese_chars = re.findall(r'[\\u4e00-\\u9fff]', text_cleaned)\n",
    "      text_len = len(text_cleaned)\n",
    "      ratio = len(chinese_chars) / max(text_len, 1)\n",
    "      chinese_threshold = 0.3\n",
    "      if ratio > chinese_threshold:\n",
    "        return \"Ch\"\n",
    "\n",
    "      # å‘¼å« lingua åµæ¸¬èªè¨€\n",
    "      detected_language = lingua_detector.detect_language_of(text_cleaned)\n",
    "\n",
    "      # è‹¥ lingua åˆ¤å®šæ˜¯ä¸­æ–‡ï¼ˆ'ZH'ï¼‰ï¼Œå‰‡å›å‚³ \"Ch\"ï¼Œå…¶é¤˜èªè¨€ä»¥å°å¯«çš„ ISO 639-1 å›å‚³ï¼ˆå¦‚ en, ja, frï¼‰\n",
    "      # è‹¥ç„¡æ³•åµæ¸¬å‡ºèªè¨€ï¼Œå›å‚³ \"unknown\"\n",
    "      if detected_language is not None:\n",
    "        iso_code = detected_language.iso_code_639_1.name\n",
    "        if iso_code == 'ZH':\n",
    "          return \"Ch\"\n",
    "        else:\n",
    "          return iso_code.lower()\n",
    "      else:\n",
    "        return \"unknown\"\n",
    "\n",
    "    except Exception as e:\n",
    "      return \"unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_nqIl5HD2ia"
   },
   "source": [
    "## æ¸…æ´—æ•¸æ“šV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LsfgBIPFrsc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è™•ç†å®Œæˆï¼Œå·²è¼¸å‡º threads_cleaned_v1.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>post_time</th>\n",
       "      <th>topic</th>\n",
       "      <th>time_info</th>\n",
       "      <th>content</th>\n",
       "      <th>has_photo</th>\n",
       "      <th>has_video</th>\n",
       "      <th>like_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>repost_count</th>\n",
       "      <th>...</th>\n",
       "      <th>scrape_time</th>\n",
       "      <th>emojis</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>scrape_time_origin</th>\n",
       "      <th>post_weekday</th>\n",
       "      <th>post_hour</th>\n",
       "      <th>viral</th>\n",
       "      <th>has_question</th>\n",
       "      <th>has_exclaim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>2025-04-29T10:58:39.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3å°æ™‚</td>\n",
       "      <td>Thank you God for another day.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>190</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 06:27</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 06:27:40.176749+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-04-29T12:33:44.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1å°æ™‚</td>\n",
       "      <td>ç™¾é”ç¿¡éº—ï¼Ÿ æ²’æœ‰ä¸‹é™çš„ç¶²è·¯ç—…æ…‹ï¼</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>196</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 06:27</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025-04-30 06:27:54.964788+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ribboworld2021</td>\n",
       "      <td>2025-04-29T04:39:46.000Z</td>\n",
       "      <td>å°ä¸€æ—¥å¸¸</td>\n",
       "      <td>9å°æ™‚</td>\n",
       "      <td>è€ƒå®ŒæœŸä¸­è€ƒï¼Œæˆç¸¾éƒ½é‚„æ²’å‡ºä¾†ï¼Œå°ä¸€å¥³å…’å°±è‡ªä¿¡çš„å°æˆ‘èªªï¼šã€Œæˆ‘çœŸç¾¨æ…•å¦³ç”Ÿäº†ä¸€å€‹å¤©æ‰ï¼ã€</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>75</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 06:28</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025-04-30 06:28:09.873641+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>2025-04-29T11:25:22.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3å°æ™‚</td>\n",
       "      <td>Just be strong. Confident. Hopeful. Intellectu...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>83</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 06:28</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 06:28:24.726576+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jose_ykc</td>\n",
       "      <td>2025-04-29T10:36:19.000Z</td>\n",
       "      <td>è¼”ä»å¤§å­¸</td>\n",
       "      <td>3å°æ™‚</td>\n",
       "      <td>æ—¥æ–‡è¼”ç³»è€å¸«ä¸Šèª²å…§å®¹ä¹‹ä¸€AiScReam æ­Œè©å°è®€</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4334</td>\n",
       "      <td>55</td>\n",
       "      <td>513</td>\n",
       "      <td>...</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 06:28</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025-04-30 06:28:39.393706+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>leighton.williams</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 07:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8å°æ™‚</td>\n",
       "      <td>First tasting in California</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:23</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 00:23:40.328046+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>cape__man</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 06:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9å°æ™‚</td>\n",
       "      <td>I hoped it would have been better.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:23</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 00:23:55.063517+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>simimoonlight</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 06:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9å°æ™‚</td>\n",
       "      <td>I canâ€™t wait to watch BeyoncÃ© on TikTok tonigh...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2656</td>\n",
       "      <td>29</td>\n",
       "      <td>280</td>\n",
       "      <td>...</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:24</td>\n",
       "      <td>ğŸ«¶ğŸ¿ğŸ¥¹</td>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 00:24:39.870994+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>other98</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 12:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3å°æ™‚</td>\n",
       "      <td>Tonight, Canada just proved that they have a h...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>10000</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>...</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:24</td>\n",
       "      <td>ğŸ™Œ</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 00:24:54.669936+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>scottiebeam</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 01:33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14å°æ™‚</td>\n",
       "      <td>Yall be fighting on here â€¦ i thought threads w...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>427</td>\n",
       "      <td>76</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:25</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 00:25:09.396585+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3101 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                 post_time topic time_info  \\\n",
       "0                ayofvr  2025-04-29T10:58:39.000Z   NaN       3å°æ™‚   \n",
       "1                   NaN  2025-04-29T12:33:44.000Z   NaN       1å°æ™‚   \n",
       "2        ribboworld2021  2025-04-29T04:39:46.000Z  å°ä¸€æ—¥å¸¸       9å°æ™‚   \n",
       "3                ayofvr  2025-04-29T11:25:22.000Z   NaN       3å°æ™‚   \n",
       "4              jose_ykc  2025-04-29T10:36:19.000Z  è¼”ä»å¤§å­¸       3å°æ™‚   \n",
       "...                 ...                       ...   ...       ...   \n",
       "3096  leighton.williams         2025å¹´04æœˆ29æ—¥ 07:30   NaN       8å°æ™‚   \n",
       "3097          cape__man         2025å¹´04æœˆ29æ—¥ 06:31   NaN       9å°æ™‚   \n",
       "3098      simimoonlight         2025å¹´04æœˆ29æ—¥ 06:26   NaN       9å°æ™‚   \n",
       "3099            other98         2025å¹´04æœˆ29æ—¥ 12:31   NaN       3å°æ™‚   \n",
       "3100        scottiebeam         2025å¹´04æœˆ29æ—¥ 01:33   NaN      14å°æ™‚   \n",
       "\n",
       "                                                content  has_photo  has_video  \\\n",
       "0                        Thank you God for another day.      False      False   \n",
       "1                                      ç™¾é”ç¿¡éº—ï¼Ÿ æ²’æœ‰ä¸‹é™çš„ç¶²è·¯ç—…æ…‹ï¼       True      False   \n",
       "2              è€ƒå®ŒæœŸä¸­è€ƒï¼Œæˆç¸¾éƒ½é‚„æ²’å‡ºä¾†ï¼Œå°ä¸€å¥³å…’å°±è‡ªä¿¡çš„å°æˆ‘èªªï¼šã€Œæˆ‘çœŸç¾¨æ…•å¦³ç”Ÿäº†ä¸€å€‹å¤©æ‰ï¼ã€      False      False   \n",
       "3     Just be strong. Confident. Hopeful. Intellectu...      False      False   \n",
       "4                             æ—¥æ–‡è¼”ç³»è€å¸«ä¸Šèª²å…§å®¹ä¹‹ä¸€AiScReam æ­Œè©å°è®€       True       True   \n",
       "...                                                 ...        ...        ...   \n",
       "3096                        First tasting in California      False      False   \n",
       "3097                 I hoped it would have been better.      False      False   \n",
       "3098  I canâ€™t wait to watch BeyoncÃ© on TikTok tonigh...      False      False   \n",
       "3099  Tonight, Canada just proved that they have a h...      False      False   \n",
       "3100  Yall be fighting on here â€¦ i thought threads w...      False      False   \n",
       "\n",
       "      like_count  reply_count  repost_count  ...        scrape_time  emojis  \\\n",
       "0            190            3            23  ...  2025å¹´04æœˆ30æ—¥ 06:27           \n",
       "1            196           16             0  ...  2025å¹´04æœˆ30æ—¥ 06:27           \n",
       "2             75            6             0  ...  2025å¹´04æœˆ30æ—¥ 06:28           \n",
       "3             83            5             3  ...  2025å¹´04æœˆ30æ—¥ 06:28           \n",
       "4           4334           55           513  ...  2025å¹´04æœˆ30æ—¥ 06:28           \n",
       "...          ...          ...           ...  ...                ...     ...   \n",
       "3096           7            0             0  ...  2025å¹´04æœˆ30æ—¥ 00:23           \n",
       "3097           2            0             0  ...  2025å¹´04æœˆ30æ—¥ 00:23           \n",
       "3098        2656           29           280  ...  2025å¹´04æœˆ30æ—¥ 00:24     ğŸ«¶ğŸ¿ğŸ¥¹   \n",
       "3099       10000          214           214  ...  2025å¹´04æœˆ30æ—¥ 00:24       ğŸ™Œ   \n",
       "3100         427           76            28  ...  2025å¹´04æœˆ30æ—¥ 00:25           \n",
       "\n",
       "      emoji_count lang               scrape_time_origin post_weekday  \\\n",
       "0               0   en 2025-04-30 06:27:40.176749+08:00    Wednesday   \n",
       "1               0   Ch 2025-04-30 06:27:54.964788+08:00    Wednesday   \n",
       "2               0   Ch 2025-04-30 06:28:09.873641+08:00    Wednesday   \n",
       "3               0   en 2025-04-30 06:28:24.726576+08:00    Wednesday   \n",
       "4               0   Ch 2025-04-30 06:28:39.393706+08:00    Wednesday   \n",
       "...           ...  ...                              ...          ...   \n",
       "3096            0   en 2025-04-30 00:23:40.328046+08:00    Wednesday   \n",
       "3097            0   en 2025-04-30 00:23:55.063517+08:00    Wednesday   \n",
       "3098            3   en 2025-04-30 00:24:39.870994+08:00    Wednesday   \n",
       "3099            1   en 2025-04-30 00:24:54.669936+08:00    Wednesday   \n",
       "3100            0   en 2025-04-30 00:25:09.396585+08:00    Wednesday   \n",
       "\n",
       "      post_hour viral has_question has_exclaim  \n",
       "0             6     0        False       False  \n",
       "1             6     1         True        True  \n",
       "2             6     0        False        True  \n",
       "3             6     0        False       False  \n",
       "4             6     1        False       False  \n",
       "...         ...   ...          ...         ...  \n",
       "3096          0     0        False       False  \n",
       "3097          0     0        False       False  \n",
       "3098          0     1        False       False  \n",
       "3099          0     1        False        True  \n",
       "3100          0     1        False       False  \n",
       "\n",
       "[3101 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === æ•¸å€¼æ¬„ä½æ¸…æ´—ï¼ˆè¬å­—ã€é€—è™Ÿæ ¼å¼è™•ç†ï¼‰===\n",
    "def parse_count(value):\n",
    "    # å°‡æ–‡å­—æ•¸å­—ï¼ˆå¦‚ \"1,234\"ã€\"2.5è¬\"ï¼‰çµ±ä¸€è½‰ç‚ºæ•´æ•¸ï¼ˆintï¼‰\n",
    "    if pd.isna(value): return 0\n",
    "    value = str(value).replace(\",\", \"\")\n",
    "    # \"è¬\" çš„éƒ¨åˆ†æœƒä¹˜ä¸Š 10,000 åšè½‰æ›\n",
    "    # ç„¡æ³•è™•ç†çš„æ ¼å¼å°±å›å‚³ 0\n",
    "    if \"è¬\" in value:\n",
    "        return int(float(value.replace(\"è¬\", \"\")) * 10000)\n",
    "    try:\n",
    "        return int(float(value))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "for col in [\"like_count\", \"view_count\", \"share_count\", \"repost_count\", \"reply_count\"]:\n",
    "    df[col] = df[col].apply(parse_count)\n",
    "\n",
    "# === å¸ƒæ—æ¬„ä½è™•ç† ===\n",
    "# å°‡åŸå§‹æ¬„ä½ï¼ˆY/Nï¼‰è½‰æ›ç‚º True/False\n",
    "# è™•ç†éç¨‹æœƒå»é™¤ç©ºç™½ã€è½‰æˆå¤§å¯«\n",
    "df[\"has_photo\"] = df[\"has_photo\"].apply(lambda x: str(x).strip().upper() == \"Y\")\n",
    "df[\"has_video\"] = df[\"has_video\"].apply(lambda x: str(x).strip().upper() == \"Y\")\n",
    "\n",
    "# === emoji èƒå–èˆ‡çµ±è¨ˆ ===\n",
    "# æª¢æŸ¥æ˜¯å¦ç‚ºæ–‡å­—å‹åˆ¥ï¼Œå¦‚æœæ˜¯æ–‡å­—ï¼Œå¾ä¸­èƒå–å‡ºæ‰€æœ‰ emoji å­—å…ƒä¸¦ä¸²æ¥æˆå­—ä¸²å›å‚³\n",
    "def extract_emojis(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return \"\".join([ch for ch in text if ch in emoji.EMOJI_DATA])\n",
    "\n",
    "df[\"emojis\"] = df[\"content\"].apply(extract_emojis)\n",
    "df[\"emoji_count\"] = df[\"emojis\"].apply(len)\n",
    "\n",
    "# # === èªè¨€åµæ¸¬ä¿®æ­£ç‰ˆ===\n",
    "# def detect_lang_custom(text):\n",
    "#     try:\n",
    "#         text = str(text)\n",
    "#         chinese_chars = re.findall(r'[\\u4e00-\\u9fff]', text)\n",
    "#         if len(chinese_chars) / max(len(text), 1) > 0.3:\n",
    "#             return \"Ch\"\n",
    "#         return detect(text)\n",
    "#     except:\n",
    "#         return \"unknown\"\n",
    "\n",
    "# ä½¿ç”¨å…ˆå‰å®šç¾©å¥½çš„èªè¨€åµæ¸¬å‡½æ•¸ detect_lang_with_preprocessing_lingua()ï¼Œè™•ç†æ¯ç¯‡æ–‡ç« çš„èªè¨€åˆ¤å®š\n",
    "df[\"lang\"] = df[\"content\"].apply(detect_lang_with_preprocessing_lingua)\n",
    "\n",
    "# === scrape_time è™•ç†ï¼ˆè½‰æ›æ™‚å€ + æŠ½å–æ˜ŸæœŸèˆ‡å°æ™‚ï¼‰===\n",
    "# å°‡æ™‚é–“æ¬„ä½è½‰ç‚ºå°åŒ—æ™‚å€ï¼Œé¡å¤–æŠ½å‡ºæ ¼å¼åŒ–å¾Œçš„æ™‚é–“å­—ä¸²ã€æ˜ŸæœŸå¹¾ã€å°æ™‚(0â€“23ï¼‰\n",
    "df[\"scrape_time_origin\"] = pd.to_datetime(df[\"scrape_time\"], utc=True).dt.tz_convert(\"Asia/Taipei\")\n",
    "df[\"scrape_time\"]  = df[\"scrape_time_origin\"].dt.strftime(\"%Yå¹´%mæœˆ%dæ—¥ %H:%M\")\n",
    "df[\"post_weekday\"] = df[\"scrape_time_origin\"].dt.day_name()\n",
    "df[\"post_hour\"] = df[\"scrape_time_origin\"].dt.hour\n",
    "\n",
    "# === æ˜¯å¦ç‚ºé«˜æµé‡æ–‡ç« ï¼ˆç ´è¬ï¼‰===\n",
    "# è¶…éç­‰æ–¼ 10,000 ç€è¦½ç‚º 1ï¼Œå…¶é¤˜ç‚º 0\n",
    "df[\"viral\"] = (df[\"view_count\"] >= 10000).astype(int)\n",
    "\n",
    "# === æ˜¯å¦ä½¿ç”¨å•è™Ÿã€é©šå˜†è™Ÿ ===\n",
    "df[\"has_question\"] = df[\"content\"].apply(lambda x: \"ï¼Ÿ\" in str(x) or \"?\" in str(x))\n",
    "df[\"has_exclaim\"] = df[\"content\"].apply(lambda x: \"ï¼\" in str(x) or \"!\" in str(x))\n",
    "\n",
    "# === å„²å­˜çµæœ ===\n",
    "df.to_csv(\"threads_cleaned_v1.csv\",encoding='utf_8_sig', index=False)\n",
    "print(\"âœ… è™•ç†å®Œæˆï¼Œå·²è¼¸å‡º threads_cleaned_v1.csv\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTVtegcwEfb1"
   },
   "source": [
    "## æ¸…æ´—æ•¸æ“šV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_a9vHZ0p9o2u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è™•ç†å®Œæˆï¼Œå·²è¼¸å‡º threads_cleaned_v2.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>content_length</th>\n",
       "      <th>lang</th>\n",
       "      <th>scrape_time</th>\n",
       "      <th>post_weekday</th>\n",
       "      <th>post_hour</th>\n",
       "      <th>post_period</th>\n",
       "      <th>viral</th>\n",
       "      <th>post_time</th>\n",
       "      <th>...</th>\n",
       "      <th>post_url</th>\n",
       "      <th>emojis</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>scrape_time_origin</th>\n",
       "      <th>has_question</th>\n",
       "      <th>has_exclaim</th>\n",
       "      <th>has_url</th>\n",
       "      <th>has_mention</th>\n",
       "      <th>has_hashtag</th>\n",
       "      <th>time_elapsed_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>Thank you God for another day.</td>\n",
       "      <td>30</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 06:27</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T10:58:39.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@ayofvr/post/DJBymf8uTrK</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:27:40.176749+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ç™¾é”ç¿¡éº—ï¼Ÿ æ²’æœ‰ä¸‹é™çš„ç¶²è·¯ç—…æ…‹ï¼</td>\n",
       "      <td>16</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 06:27</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-29T12:33:44.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@ban.mei.onnnnni/post/...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:27:54.964788+08:00</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ribboworld2021</td>\n",
       "      <td>è€ƒå®ŒæœŸä¸­è€ƒï¼Œæˆç¸¾éƒ½é‚„æ²’å‡ºä¾†ï¼Œå°ä¸€å¥³å…’å°±è‡ªä¿¡çš„å°æˆ‘èªªï¼šã€Œæˆ‘çœŸç¾¨æ…•å¦³ç”Ÿäº†ä¸€å€‹å¤©æ‰ï¼ã€</td>\n",
       "      <td>40</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T04:39:46.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@ribboworld2021/post/D...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:09.873641+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>Just be strong. Confident. Hopeful. Intellectu...</td>\n",
       "      <td>69</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T11:25:22.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@ayofvr/post/DJB1qP5OmzP</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:24.726576+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jose_ykc</td>\n",
       "      <td>æ—¥æ–‡è¼”ç³»è€å¸«ä¸Šèª²å…§å®¹ä¹‹ä¸€AiScReam æ­Œè©å°è®€</td>\n",
       "      <td>25</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-29T10:36:19.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@jose_ykc/post/DJBvpGI...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:39.393706+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>leighton.williams</td>\n",
       "      <td>First tasting in California</td>\n",
       "      <td>27</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:23</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 07:30</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@leighton.williams/pos...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:23:40.328046+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>cape__man</td>\n",
       "      <td>I hoped it would have been better.</td>\n",
       "      <td>34</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:23</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 06:31</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@cape__man/post/DJAdFd...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:23:55.063517+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>simimoonlight</td>\n",
       "      <td>I canâ€™t wait to watch BeyoncÃ© on TikTok tonigh...</td>\n",
       "      <td>51</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:24</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 06:26</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@simimoonlight/post/DJ...</td>\n",
       "      <td>ğŸ«¶ğŸ¿ğŸ¥¹</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-04-30 00:24:39.870994+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>other98</td>\n",
       "      <td>Tonight, Canada just proved that they have a h...</td>\n",
       "      <td>77</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:24</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 12:31</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@other98/post/DJBGV3NxiX_</td>\n",
       "      <td>ğŸ™Œ</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-30 00:24:54.669936+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>scottiebeam</td>\n",
       "      <td>Yall be fighting on here â€¦ i thought threads w...</td>\n",
       "      <td>57</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:25</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 01:33</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@scottiebeam/post/DI_6...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:25:09.396585+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3083 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                                            content  \\\n",
       "0                ayofvr                     Thank you God for another day.   \n",
       "1                   NaN                                   ç™¾é”ç¿¡éº—ï¼Ÿ æ²’æœ‰ä¸‹é™çš„ç¶²è·¯ç—…æ…‹ï¼   \n",
       "2        ribboworld2021           è€ƒå®ŒæœŸä¸­è€ƒï¼Œæˆç¸¾éƒ½é‚„æ²’å‡ºä¾†ï¼Œå°ä¸€å¥³å…’å°±è‡ªä¿¡çš„å°æˆ‘èªªï¼šã€Œæˆ‘çœŸç¾¨æ…•å¦³ç”Ÿäº†ä¸€å€‹å¤©æ‰ï¼ã€   \n",
       "3                ayofvr  Just be strong. Confident. Hopeful. Intellectu...   \n",
       "4              jose_ykc                          æ—¥æ–‡è¼”ç³»è€å¸«ä¸Šèª²å…§å®¹ä¹‹ä¸€AiScReam æ­Œè©å°è®€   \n",
       "...                 ...                                                ...   \n",
       "3096  leighton.williams                        First tasting in California   \n",
       "3097          cape__man                 I hoped it would have been better.   \n",
       "3098      simimoonlight  I canâ€™t wait to watch BeyoncÃ© on TikTok tonigh...   \n",
       "3099            other98  Tonight, Canada just proved that they have a h...   \n",
       "3100        scottiebeam  Yall be fighting on here â€¦ i thought threads w...   \n",
       "\n",
       "      content_length lang        scrape_time post_weekday  post_hour  \\\n",
       "0                 30   en  2025å¹´04æœˆ30æ—¥ 06:27    Wednesday          6   \n",
       "1                 16   Ch  2025å¹´04æœˆ30æ—¥ 06:27    Wednesday          6   \n",
       "2                 40   Ch  2025å¹´04æœˆ30æ—¥ 06:28    Wednesday          6   \n",
       "3                 69   en  2025å¹´04æœˆ30æ—¥ 06:28    Wednesday          6   \n",
       "4                 25   Ch  2025å¹´04æœˆ30æ—¥ 06:28    Wednesday          6   \n",
       "...              ...  ...                ...          ...        ...   \n",
       "3096              27   en  2025å¹´04æœˆ30æ—¥ 00:23    Wednesday          0   \n",
       "3097              34   en  2025å¹´04æœˆ30æ—¥ 00:23    Wednesday          0   \n",
       "3098              51   en  2025å¹´04æœˆ30æ—¥ 00:24    Wednesday          0   \n",
       "3099              77   en  2025å¹´04æœˆ30æ—¥ 00:24    Wednesday          0   \n",
       "3100              57   en  2025å¹´04æœˆ30æ—¥ 00:25    Wednesday          0   \n",
       "\n",
       "     post_period  viral                 post_time  ...  \\\n",
       "0        morning      0  2025-04-29T10:58:39.000Z  ...   \n",
       "1        morning      1  2025-04-29T12:33:44.000Z  ...   \n",
       "2        morning      0  2025-04-29T04:39:46.000Z  ...   \n",
       "3        morning      0  2025-04-29T11:25:22.000Z  ...   \n",
       "4        morning      1  2025-04-29T10:36:19.000Z  ...   \n",
       "...          ...    ...                       ...  ...   \n",
       "3096       night      0         2025å¹´04æœˆ29æ—¥ 07:30  ...   \n",
       "3097       night      0         2025å¹´04æœˆ29æ—¥ 06:31  ...   \n",
       "3098       night      1         2025å¹´04æœˆ29æ—¥ 06:26  ...   \n",
       "3099       night      1         2025å¹´04æœˆ29æ—¥ 12:31  ...   \n",
       "3100       night      1         2025å¹´04æœˆ29æ—¥ 01:33  ...   \n",
       "\n",
       "                                               post_url emojis  emoji_count  \\\n",
       "0      https://www.threads.net/@ayofvr/post/DJBymf8uTrK                   0   \n",
       "1     https://www.threads.net/@ban.mei.onnnnni/post/...                   0   \n",
       "2     https://www.threads.net/@ribboworld2021/post/D...                   0   \n",
       "3      https://www.threads.net/@ayofvr/post/DJB1qP5OmzP                   0   \n",
       "4     https://www.threads.net/@jose_ykc/post/DJBvpGI...                   0   \n",
       "...                                                 ...    ...          ...   \n",
       "3096  https://www.threads.net/@leighton.williams/pos...                   0   \n",
       "3097  https://www.threads.net/@cape__man/post/DJAdFd...                   0   \n",
       "3098  https://www.threads.net/@simimoonlight/post/DJ...    ğŸ«¶ğŸ¿ğŸ¥¹            3   \n",
       "3099  https://www.threads.net/@other98/post/DJBGV3NxiX_      ğŸ™Œ            1   \n",
       "3100  https://www.threads.net/@scottiebeam/post/DI_6...                   0   \n",
       "\n",
       "                   scrape_time_origin  has_question  has_exclaim  has_url  \\\n",
       "0    2025-04-30 06:27:40.176749+08:00         False        False    False   \n",
       "1    2025-04-30 06:27:54.964788+08:00          True         True    False   \n",
       "2    2025-04-30 06:28:09.873641+08:00         False         True    False   \n",
       "3    2025-04-30 06:28:24.726576+08:00         False        False    False   \n",
       "4    2025-04-30 06:28:39.393706+08:00         False        False    False   \n",
       "...                               ...           ...          ...      ...   \n",
       "3096 2025-04-30 00:23:40.328046+08:00         False        False    False   \n",
       "3097 2025-04-30 00:23:55.063517+08:00         False        False    False   \n",
       "3098 2025-04-30 00:24:39.870994+08:00         False        False    False   \n",
       "3099 2025-04-30 00:24:54.669936+08:00         False         True    False   \n",
       "3100 2025-04-30 00:25:09.396585+08:00         False        False    False   \n",
       "\n",
       "      has_mention  has_hashtag  time_elapsed_hours  \n",
       "0           False        False                 3.0  \n",
       "1           False        False                 1.0  \n",
       "2           False        False                 9.0  \n",
       "3           False        False                 3.0  \n",
       "4           False        False                 3.0  \n",
       "...           ...          ...                 ...  \n",
       "3096        False        False                 8.0  \n",
       "3097        False        False                 9.0  \n",
       "3098        False        False                 9.0  \n",
       "3099        False        False                 3.0  \n",
       "3100        False        False                14.0  \n",
       "\n",
       "[3083 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- æ–‡ç« é•·åº¦ ---\n",
    "df[\"content_length\"] = df[\"content\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "# --- æ˜¯å¦åŒ…å«ç¶²å€ ---\n",
    "df[\"has_url\"] = df[\"content\"].apply(lambda x: \"http\" in str(x) or \"www.\" in str(x))\n",
    "\n",
    "# --- æ˜¯å¦åŒ…å« @æ¨™è¨˜ä»–äºº ---\n",
    "df[\"has_mention\"] = df[\"content\"].apply(lambda x: \"@\" in str(x))\n",
    "\n",
    "# --- æ˜¯å¦ä½¿ç”¨ Hashtag ---\n",
    "df[\"has_hashtag\"] = df[\"content\"].apply(lambda x: \"#\" in str(x))\n",
    "\n",
    "# è²¼æ–‡ä¸»é¡Œå­—è©æå–ï¼ˆå¯å¾ŒçºŒåš TF-IDF æˆ–ä¸»é¡Œå»ºæ¨¡ï¼‰\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=100, stop_words='english')\n",
    "word_matrix = vectorizer.fit_transform(df['content'].astype(str))\n",
    "\n",
    "# å°‡å¸¸è¦‹è©èªæå–å‡ºä¾†\n",
    "keywords = vectorizer.get_feature_names_out()\n",
    "\n",
    "# æ˜¯å¦ç‚ºæ·±å¤œæˆ–ç™½å¤©è²¼æ–‡ï¼ˆæ™‚é–“æ®µåˆ†é¡ï¼‰\n",
    "def time_period(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return \"morning\"\n",
    "    elif 12 <= hour < 17:\n",
    "        return \"afternoon\"\n",
    "    elif 17 <= hour < 22:\n",
    "        return \"evening\"\n",
    "    else:\n",
    "        return \"night\"\n",
    "    \n",
    "# åˆ¤æ–·æ˜¯å¦åˆæ³•çš„time_infoæ ¼å¼\n",
    "def is_valid_time_info(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    text = str(text).strip()\n",
    "    return bool(re.match(r\"^\\d+\\s*(åˆ†é˜|å°æ™‚|å¤©)$\", text))\n",
    "\n",
    "df = df[df[\"time_info\"].apply(is_valid_time_info)].copy()\n",
    "\n",
    "# å°‡time_info çµ±ä¸€è½‰ç‚ºå°æ™‚\n",
    "def convert_time_info(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    text = str(text)\n",
    "    if \"åˆ†é˜\" in text:\n",
    "        match = re.search(r\"(\\d+)\", text)\n",
    "        return round(int(match.group(1)) / 60, 1)\n",
    "    elif \"å°æ™‚\" in text:\n",
    "        match = re.search(r\"(\\d+)\", text)\n",
    "        return round(int(match.group(1)), 1)\n",
    "    elif \"å¤©\" in text:\n",
    "        match = re.search(r\"(\\d+)\", text)\n",
    "        return round(int(match.group(1)) * 24, 1)\n",
    "    elif \"é€±\" in text or \"ç¦®æ‹œ\" in text:\n",
    "        match = re.search(r\"(\\d+)\", text)\n",
    "        return round(int(match.group(1)) * 7 * 24, 1)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "df[\"time_elapsed_hours\"] = df[\"time_info\"].apply(convert_time_info)\n",
    "df[\"post_period\"] = df[\"post_hour\"].apply(time_period)\n",
    "\n",
    "cols_to_show_first = ['author', 'content', 'content_length', 'lang', 'scrape_time', 'post_weekday', 'post_hour', 'post_period', 'viral']\n",
    "df = df[cols_to_show_first + [col for col in df.columns if col not in cols_to_show_first]]\n",
    "df.to_csv(\"threads_cleaned_v2.csv\",encoding='utf_8_sig',index=False)\n",
    "print(\"âœ… è™•ç†å®Œæˆï¼Œå·²è¼¸å‡º threads_cleaned_v2.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention æ¨¡çµ„è™•ç†æ–‡å­—è³‡æ–™ï¼ˆæ¸¬è©¦ä¸­ å› æœªå¯«å®Œå¯å…ˆè·³éï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>content_length</th>\n",
       "      <th>lang</th>\n",
       "      <th>scrape_time</th>\n",
       "      <th>post_weekday</th>\n",
       "      <th>post_hour</th>\n",
       "      <th>post_period</th>\n",
       "      <th>viral</th>\n",
       "      <th>post_time</th>\n",
       "      <th>...</th>\n",
       "      <th>emojis</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>scrape_time_origin</th>\n",
       "      <th>has_question</th>\n",
       "      <th>has_exclaim</th>\n",
       "      <th>has_url</th>\n",
       "      <th>has_mention</th>\n",
       "      <th>has_hashtag</th>\n",
       "      <th>time_elapsed_hours</th>\n",
       "      <th>semantic_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>Thank you God for another day.</td>\n",
       "      <td>30</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 06:27</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T10:58:39.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:27:40.176749+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[AUTHOR] ayofvr [EMOJI]  [CONTENT] Thank you G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ç™¾é”ç¿¡éº—ï¼Ÿ æ²’æœ‰ä¸‹é™çš„ç¶²è·¯ç—…æ…‹ï¼</td>\n",
       "      <td>16</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 06:27</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-29T12:33:44.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:27:54.964788+08:00</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[EMOJI]  [CONTENT] ç™¾é”ç¿¡éº—ï¼Ÿ æ²’æœ‰ä¸‹é™çš„ç¶²è·¯ç—…æ…‹ï¼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ribboworld2021</td>\n",
       "      <td>è€ƒå®ŒæœŸä¸­è€ƒï¼Œæˆç¸¾éƒ½é‚„æ²’å‡ºä¾†ï¼Œå°ä¸€å¥³å…’å°±è‡ªä¿¡çš„å°æˆ‘èªªï¼šã€Œæˆ‘çœŸç¾¨æ…•å¦³ç”Ÿäº†ä¸€å€‹å¤©æ‰ï¼ã€</td>\n",
       "      <td>40</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T04:39:46.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:09.873641+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[AUTHOR] ribboworld2021 [TOPIC] å°ä¸€æ—¥å¸¸ [EMOJI]  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>Just be strong. Confident. Hopeful. Intellectu...</td>\n",
       "      <td>69</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T11:25:22.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:24.726576+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[AUTHOR] ayofvr [EMOJI]  [CONTENT] Just be str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jose_ykc</td>\n",
       "      <td>æ—¥æ–‡è¼”ç³»è€å¸«ä¸Šèª²å…§å®¹ä¹‹ä¸€AiScReam æ­Œè©å°è®€</td>\n",
       "      <td>25</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-29T10:36:19.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:39.393706+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[AUTHOR] jose_ykc [TOPIC] è¼”ä»å¤§å­¸ [EMOJI]  [CONTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>leighton.williams</td>\n",
       "      <td>First tasting in California</td>\n",
       "      <td>27</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:23</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 07:30</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:23:40.328046+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8.0</td>\n",
       "      <td>[AUTHOR] leighton.williams [EMOJI]  [CONTENT] ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>cape__man</td>\n",
       "      <td>I hoped it would have been better.</td>\n",
       "      <td>34</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:23</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 06:31</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:23:55.063517+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[AUTHOR] cape__man [EMOJI]  [CONTENT] I hoped ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>simimoonlight</td>\n",
       "      <td>I canâ€™t wait to watch BeyoncÃ© on TikTok tonigh...</td>\n",
       "      <td>51</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:24</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 06:26</td>\n",
       "      <td>...</td>\n",
       "      <td>ğŸ«¶ğŸ¿ğŸ¥¹</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-04-30 00:24:39.870994+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[AUTHOR] simimoonlight [EMOJI] ğŸ«¶ ğŸ¿ ğŸ¥¹ [CONTENT]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>other98</td>\n",
       "      <td>Tonight, Canada just proved that they have a h...</td>\n",
       "      <td>77</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:24</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 12:31</td>\n",
       "      <td>...</td>\n",
       "      <td>ğŸ™Œ</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-30 00:24:54.669936+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[AUTHOR] other98 [EMOJI] ğŸ™Œ [CONTENT] Tonight, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>scottiebeam</td>\n",
       "      <td>Yall be fighting on here â€¦ i thought threads w...</td>\n",
       "      <td>57</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:25</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 01:33</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:25:09.396585+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>14.0</td>\n",
       "      <td>[AUTHOR] scottiebeam [EMOJI]  [CONTENT] Yall b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3083 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                                            content  \\\n",
       "0                ayofvr                     Thank you God for another day.   \n",
       "1                   NaN                                   ç™¾é”ç¿¡éº—ï¼Ÿ æ²’æœ‰ä¸‹é™çš„ç¶²è·¯ç—…æ…‹ï¼   \n",
       "2        ribboworld2021           è€ƒå®ŒæœŸä¸­è€ƒï¼Œæˆç¸¾éƒ½é‚„æ²’å‡ºä¾†ï¼Œå°ä¸€å¥³å…’å°±è‡ªä¿¡çš„å°æˆ‘èªªï¼šã€Œæˆ‘çœŸç¾¨æ…•å¦³ç”Ÿäº†ä¸€å€‹å¤©æ‰ï¼ã€   \n",
       "3                ayofvr  Just be strong. Confident. Hopeful. Intellectu...   \n",
       "4              jose_ykc                          æ—¥æ–‡è¼”ç³»è€å¸«ä¸Šèª²å…§å®¹ä¹‹ä¸€AiScReam æ­Œè©å°è®€   \n",
       "...                 ...                                                ...   \n",
       "3096  leighton.williams                        First tasting in California   \n",
       "3097          cape__man                 I hoped it would have been better.   \n",
       "3098      simimoonlight  I canâ€™t wait to watch BeyoncÃ© on TikTok tonigh...   \n",
       "3099            other98  Tonight, Canada just proved that they have a h...   \n",
       "3100        scottiebeam  Yall be fighting on here â€¦ i thought threads w...   \n",
       "\n",
       "      content_length lang        scrape_time post_weekday  post_hour  \\\n",
       "0                 30   en  2025å¹´04æœˆ30æ—¥ 06:27    Wednesday          6   \n",
       "1                 16   Ch  2025å¹´04æœˆ30æ—¥ 06:27    Wednesday          6   \n",
       "2                 40   Ch  2025å¹´04æœˆ30æ—¥ 06:28    Wednesday          6   \n",
       "3                 69   en  2025å¹´04æœˆ30æ—¥ 06:28    Wednesday          6   \n",
       "4                 25   Ch  2025å¹´04æœˆ30æ—¥ 06:28    Wednesday          6   \n",
       "...              ...  ...                ...          ...        ...   \n",
       "3096              27   en  2025å¹´04æœˆ30æ—¥ 00:23    Wednesday          0   \n",
       "3097              34   en  2025å¹´04æœˆ30æ—¥ 00:23    Wednesday          0   \n",
       "3098              51   en  2025å¹´04æœˆ30æ—¥ 00:24    Wednesday          0   \n",
       "3099              77   en  2025å¹´04æœˆ30æ—¥ 00:24    Wednesday          0   \n",
       "3100              57   en  2025å¹´04æœˆ30æ—¥ 00:25    Wednesday          0   \n",
       "\n",
       "     post_period  viral                 post_time  ... emojis emoji_count  \\\n",
       "0        morning      0  2025-04-29T10:58:39.000Z  ...                  0   \n",
       "1        morning      1  2025-04-29T12:33:44.000Z  ...                  0   \n",
       "2        morning      0  2025-04-29T04:39:46.000Z  ...                  0   \n",
       "3        morning      0  2025-04-29T11:25:22.000Z  ...                  0   \n",
       "4        morning      1  2025-04-29T10:36:19.000Z  ...                  0   \n",
       "...          ...    ...                       ...  ...    ...         ...   \n",
       "3096       night      0         2025å¹´04æœˆ29æ—¥ 07:30  ...                  0   \n",
       "3097       night      0         2025å¹´04æœˆ29æ—¥ 06:31  ...                  0   \n",
       "3098       night      1         2025å¹´04æœˆ29æ—¥ 06:26  ...    ğŸ«¶ğŸ¿ğŸ¥¹           3   \n",
       "3099       night      1         2025å¹´04æœˆ29æ—¥ 12:31  ...      ğŸ™Œ           1   \n",
       "3100       night      1         2025å¹´04æœˆ29æ—¥ 01:33  ...                  0   \n",
       "\n",
       "                   scrape_time_origin  has_question  has_exclaim  has_url  \\\n",
       "0    2025-04-30 06:27:40.176749+08:00         False        False    False   \n",
       "1    2025-04-30 06:27:54.964788+08:00          True         True    False   \n",
       "2    2025-04-30 06:28:09.873641+08:00         False         True    False   \n",
       "3    2025-04-30 06:28:24.726576+08:00         False        False    False   \n",
       "4    2025-04-30 06:28:39.393706+08:00         False        False    False   \n",
       "...                               ...           ...          ...      ...   \n",
       "3096 2025-04-30 00:23:40.328046+08:00         False        False    False   \n",
       "3097 2025-04-30 00:23:55.063517+08:00         False        False    False   \n",
       "3098 2025-04-30 00:24:39.870994+08:00         False        False    False   \n",
       "3099 2025-04-30 00:24:54.669936+08:00         False         True    False   \n",
       "3100 2025-04-30 00:25:09.396585+08:00         False        False    False   \n",
       "\n",
       "      has_mention  has_hashtag  time_elapsed_hours  \\\n",
       "0           False        False                 3.0   \n",
       "1           False        False                 1.0   \n",
       "2           False        False                 9.0   \n",
       "3           False        False                 3.0   \n",
       "4           False        False                 3.0   \n",
       "...           ...          ...                 ...   \n",
       "3096        False        False                 8.0   \n",
       "3097        False        False                 9.0   \n",
       "3098        False        False                 9.0   \n",
       "3099        False        False                 3.0   \n",
       "3100        False        False                14.0   \n",
       "\n",
       "                                          semantic_text  \n",
       "0     [AUTHOR] ayofvr [EMOJI]  [CONTENT] Thank you G...  \n",
       "1                   [EMOJI]  [CONTENT] ç™¾é”ç¿¡éº—ï¼Ÿ æ²’æœ‰ä¸‹é™çš„ç¶²è·¯ç—…æ…‹ï¼  \n",
       "2     [AUTHOR] ribboworld2021 [TOPIC] å°ä¸€æ—¥å¸¸ [EMOJI]  ...  \n",
       "3     [AUTHOR] ayofvr [EMOJI]  [CONTENT] Just be str...  \n",
       "4     [AUTHOR] jose_ykc [TOPIC] è¼”ä»å¤§å­¸ [EMOJI]  [CONTE...  \n",
       "...                                                 ...  \n",
       "3096  [AUTHOR] leighton.williams [EMOJI]  [CONTENT] ...  \n",
       "3097  [AUTHOR] cape__man [EMOJI]  [CONTENT] I hoped ...  \n",
       "3098  [AUTHOR] simimoonlight [EMOJI] ğŸ«¶ ğŸ¿ ğŸ¥¹ [CONTENT]...  \n",
       "3099  [AUTHOR] other98 [EMOJI] ğŸ™Œ [CONTENT] Tonight, ...  \n",
       "3100  [AUTHOR] scottiebeam [EMOJI]  [CONTENT] Yall b...  \n",
       "\n",
       "[3083 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT æœƒæŠŠ [EMOJI], [CONTENT] ç•¶ä½œåˆ†ç•Œçš„è©ä¾†ç†è§£\n",
    "def build_semantic_text(row):\n",
    "    parts = []\n",
    "\n",
    "    if pd.notna(row[\"author\"]):\n",
    "        parts.append(f\"[AUTHOR] {row['author']}\")\n",
    "\n",
    "    if pd.notna(row[\"topic\"]):\n",
    "        parts.append(f\"[TOPIC] {row['topic']}\")\n",
    "\n",
    "    if pd.notna(row[\"emojis\"]):\n",
    "        emoji_text = \" \".join(row[\"emojis\"])\n",
    "        parts.append(f\"[EMOJI] {emoji_text}\")\n",
    "\n",
    "    if pd.notna(row[\"content\"]):\n",
    "        parts.append(f\"[CONTENT] {row['content']}\")\n",
    "\n",
    "    return \" \".join(parts)\n",
    "\n",
    "\n",
    "# å»ºç«‹ semantic_textï¼Œä¸å½±éŸ¿ df æœ¬èº«\n",
    "semantic_text_series = df.apply(build_semantic_text, axis=1)\n",
    "\n",
    "# è‹¥ä½ æƒ³è¦æ–° DataFrameï¼š\n",
    "df_semantic = df.copy()\n",
    "df_semantic[\"semantic_text\"] = semantic_text_series\n",
    "df_semantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# è¼‰å…¥ BERT tokenizer å’Œæ¨¡å‹\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = BertModel.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# è‡ªè¨‚ Dataset é¡åˆ¥ï¼ˆä¸è®Šï¼‰\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, max_len=128):\n",
    "        self.encodings = tokenizer(\n",
    "            texts, \n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx]\n",
    "        }\n",
    "\n",
    "# å»ºç«‹ dataset å’Œ dataloaderï¼ˆé€™è£¡å»ºè­°ç”¨ semantic_text è€Œä¸æ˜¯ contentï¼‰\n",
    "texts = df_semantic[\"semantic_text\"].astype(str).tolist()\n",
    "text_dataset = TextDataset(texts)\n",
    "text_loader = DataLoader(text_dataset, batch_size=32)\n",
    "\n",
    "# æå–èªæ„å‘é‡ Z_text\n",
    "Z_text_list = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in text_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_rep = outputs.last_hidden_state[:, 0, :]  # å– [CLS] å‘é‡\n",
    "        \n",
    "        Z_text_list.append(cls_rep.cpu())\n",
    "\n",
    "Z_text_tensor = torch.cat(Z_text_list, dim=0)  # [N, 768]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.197812</td>\n",
       "      <td>0.491429</td>\n",
       "      <td>-0.485847</td>\n",
       "      <td>0.316401</td>\n",
       "      <td>1.326630</td>\n",
       "      <td>-1.078312</td>\n",
       "      <td>0.297176</td>\n",
       "      <td>-0.804608</td>\n",
       "      <td>-1.177580</td>\n",
       "      <td>0.156056</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.562810</td>\n",
       "      <td>-0.411340</td>\n",
       "      <td>0.433298</td>\n",
       "      <td>-0.422068</td>\n",
       "      <td>-0.669982</td>\n",
       "      <td>-0.726443</td>\n",
       "      <td>0.106795</td>\n",
       "      <td>1.173453</td>\n",
       "      <td>0.316311</td>\n",
       "      <td>0.108925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.215056</td>\n",
       "      <td>1.000118</td>\n",
       "      <td>-0.044262</td>\n",
       "      <td>-0.209858</td>\n",
       "      <td>0.740916</td>\n",
       "      <td>-1.110934</td>\n",
       "      <td>-0.698522</td>\n",
       "      <td>-0.050963</td>\n",
       "      <td>-0.216489</td>\n",
       "      <td>1.010718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426817</td>\n",
       "      <td>-0.219904</td>\n",
       "      <td>-0.014254</td>\n",
       "      <td>-0.705548</td>\n",
       "      <td>0.285813</td>\n",
       "      <td>-0.596890</td>\n",
       "      <td>-0.212060</td>\n",
       "      <td>0.695832</td>\n",
       "      <td>-0.082648</td>\n",
       "      <td>0.383929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.887392</td>\n",
       "      <td>0.952374</td>\n",
       "      <td>0.468288</td>\n",
       "      <td>0.363257</td>\n",
       "      <td>1.021098</td>\n",
       "      <td>-1.253425</td>\n",
       "      <td>-0.300586</td>\n",
       "      <td>-0.201750</td>\n",
       "      <td>-0.855971</td>\n",
       "      <td>0.126374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325378</td>\n",
       "      <td>-0.584822</td>\n",
       "      <td>-0.845940</td>\n",
       "      <td>-0.637423</td>\n",
       "      <td>-0.245427</td>\n",
       "      <td>0.500610</td>\n",
       "      <td>0.191019</td>\n",
       "      <td>0.498423</td>\n",
       "      <td>0.164274</td>\n",
       "      <td>-0.312768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.188987</td>\n",
       "      <td>0.500823</td>\n",
       "      <td>-1.228274</td>\n",
       "      <td>-0.300035</td>\n",
       "      <td>1.055788</td>\n",
       "      <td>-1.166170</td>\n",
       "      <td>0.346816</td>\n",
       "      <td>-0.645881</td>\n",
       "      <td>-0.857121</td>\n",
       "      <td>0.474149</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.987927</td>\n",
       "      <td>-0.638470</td>\n",
       "      <td>-0.161924</td>\n",
       "      <td>-0.339970</td>\n",
       "      <td>-0.626445</td>\n",
       "      <td>-0.612481</td>\n",
       "      <td>0.194379</td>\n",
       "      <td>1.441311</td>\n",
       "      <td>-0.297321</td>\n",
       "      <td>0.020321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.932422</td>\n",
       "      <td>0.218099</td>\n",
       "      <td>-1.037628</td>\n",
       "      <td>0.035938</td>\n",
       "      <td>1.054755</td>\n",
       "      <td>-0.978883</td>\n",
       "      <td>-0.409518</td>\n",
       "      <td>0.187629</td>\n",
       "      <td>-0.958486</td>\n",
       "      <td>0.335753</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.653077</td>\n",
       "      <td>-0.492126</td>\n",
       "      <td>0.483244</td>\n",
       "      <td>-0.070685</td>\n",
       "      <td>-0.069700</td>\n",
       "      <td>0.078444</td>\n",
       "      <td>0.408898</td>\n",
       "      <td>1.241559</td>\n",
       "      <td>-0.151853</td>\n",
       "      <td>-0.196577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3078</th>\n",
       "      <td>-1.150749</td>\n",
       "      <td>0.211006</td>\n",
       "      <td>-0.857209</td>\n",
       "      <td>0.486619</td>\n",
       "      <td>0.887699</td>\n",
       "      <td>-1.112387</td>\n",
       "      <td>0.242534</td>\n",
       "      <td>-0.489165</td>\n",
       "      <td>-1.174777</td>\n",
       "      <td>0.081529</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.239787</td>\n",
       "      <td>-0.378836</td>\n",
       "      <td>-0.029298</td>\n",
       "      <td>-0.404599</td>\n",
       "      <td>-0.395586</td>\n",
       "      <td>-0.504007</td>\n",
       "      <td>-0.001980</td>\n",
       "      <td>1.394140</td>\n",
       "      <td>-0.356526</td>\n",
       "      <td>-0.057205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3079</th>\n",
       "      <td>-0.742924</td>\n",
       "      <td>-0.134855</td>\n",
       "      <td>-0.337486</td>\n",
       "      <td>0.413057</td>\n",
       "      <td>1.205682</td>\n",
       "      <td>-1.366358</td>\n",
       "      <td>-0.096314</td>\n",
       "      <td>-0.656512</td>\n",
       "      <td>-1.172672</td>\n",
       "      <td>0.480091</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.972684</td>\n",
       "      <td>-0.425606</td>\n",
       "      <td>0.257879</td>\n",
       "      <td>-0.083000</td>\n",
       "      <td>-0.208448</td>\n",
       "      <td>-0.519165</td>\n",
       "      <td>0.292847</td>\n",
       "      <td>1.117920</td>\n",
       "      <td>0.169528</td>\n",
       "      <td>0.234989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3080</th>\n",
       "      <td>-1.257781</td>\n",
       "      <td>0.298899</td>\n",
       "      <td>-0.860130</td>\n",
       "      <td>0.276141</td>\n",
       "      <td>1.046705</td>\n",
       "      <td>-0.745504</td>\n",
       "      <td>-0.342909</td>\n",
       "      <td>-0.462435</td>\n",
       "      <td>-1.735537</td>\n",
       "      <td>0.148792</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.665857</td>\n",
       "      <td>-0.498027</td>\n",
       "      <td>-0.127219</td>\n",
       "      <td>-1.029894</td>\n",
       "      <td>-0.855351</td>\n",
       "      <td>-0.247067</td>\n",
       "      <td>0.405077</td>\n",
       "      <td>1.160195</td>\n",
       "      <td>0.243745</td>\n",
       "      <td>0.261609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3081</th>\n",
       "      <td>-0.801286</td>\n",
       "      <td>0.418368</td>\n",
       "      <td>-0.866499</td>\n",
       "      <td>-0.064372</td>\n",
       "      <td>0.974501</td>\n",
       "      <td>-1.084798</td>\n",
       "      <td>0.472064</td>\n",
       "      <td>-0.299250</td>\n",
       "      <td>-0.953439</td>\n",
       "      <td>0.027430</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.174034</td>\n",
       "      <td>-0.653110</td>\n",
       "      <td>0.092794</td>\n",
       "      <td>-0.495103</td>\n",
       "      <td>-0.692927</td>\n",
       "      <td>-0.327008</td>\n",
       "      <td>0.045146</td>\n",
       "      <td>1.080402</td>\n",
       "      <td>0.078887</td>\n",
       "      <td>-0.075638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>-0.707997</td>\n",
       "      <td>0.675630</td>\n",
       "      <td>-0.233938</td>\n",
       "      <td>0.159180</td>\n",
       "      <td>1.000529</td>\n",
       "      <td>-1.665780</td>\n",
       "      <td>0.296066</td>\n",
       "      <td>-0.708535</td>\n",
       "      <td>-1.331825</td>\n",
       "      <td>-0.255058</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.897987</td>\n",
       "      <td>-0.625595</td>\n",
       "      <td>0.092348</td>\n",
       "      <td>-0.311444</td>\n",
       "      <td>0.269258</td>\n",
       "      <td>-0.207657</td>\n",
       "      <td>0.134844</td>\n",
       "      <td>0.806955</td>\n",
       "      <td>-0.166361</td>\n",
       "      <td>0.017548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3083 rows Ã— 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -1.197812  0.491429 -0.485847  0.316401  1.326630 -1.078312  0.297176   \n",
       "1     0.215056  1.000118 -0.044262 -0.209858  0.740916 -1.110934 -0.698522   \n",
       "2     0.887392  0.952374  0.468288  0.363257  1.021098 -1.253425 -0.300586   \n",
       "3    -1.188987  0.500823 -1.228274 -0.300035  1.055788 -1.166170  0.346816   \n",
       "4    -0.932422  0.218099 -1.037628  0.035938  1.054755 -0.978883 -0.409518   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3078 -1.150749  0.211006 -0.857209  0.486619  0.887699 -1.112387  0.242534   \n",
       "3079 -0.742924 -0.134855 -0.337486  0.413057  1.205682 -1.366358 -0.096314   \n",
       "3080 -1.257781  0.298899 -0.860130  0.276141  1.046705 -0.745504 -0.342909   \n",
       "3081 -0.801286  0.418368 -0.866499 -0.064372  0.974501 -1.084798  0.472064   \n",
       "3082 -0.707997  0.675630 -0.233938  0.159180  1.000529 -1.665780  0.296066   \n",
       "\n",
       "           7         8         9    ...       758       759       760  \\\n",
       "0    -0.804608 -1.177580  0.156056  ... -0.562810 -0.411340  0.433298   \n",
       "1    -0.050963 -0.216489  1.010718  ...  0.426817 -0.219904 -0.014254   \n",
       "2    -0.201750 -0.855971  0.126374  ...  0.325378 -0.584822 -0.845940   \n",
       "3    -0.645881 -0.857121  0.474149  ... -0.987927 -0.638470 -0.161924   \n",
       "4     0.187629 -0.958486  0.335753  ... -0.653077 -0.492126  0.483244   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3078 -0.489165 -1.174777  0.081529  ... -0.239787 -0.378836 -0.029298   \n",
       "3079 -0.656512 -1.172672  0.480091  ... -0.972684 -0.425606  0.257879   \n",
       "3080 -0.462435 -1.735537  0.148792  ... -0.665857 -0.498027 -0.127219   \n",
       "3081 -0.299250 -0.953439  0.027430  ... -1.174034 -0.653110  0.092794   \n",
       "3082 -0.708535 -1.331825 -0.255058  ... -0.897987 -0.625595  0.092348   \n",
       "\n",
       "           761       762       763       764       765       766       767  \n",
       "0    -0.422068 -0.669982 -0.726443  0.106795  1.173453  0.316311  0.108925  \n",
       "1    -0.705548  0.285813 -0.596890 -0.212060  0.695832 -0.082648  0.383929  \n",
       "2    -0.637423 -0.245427  0.500610  0.191019  0.498423  0.164274 -0.312768  \n",
       "3    -0.339970 -0.626445 -0.612481  0.194379  1.441311 -0.297321  0.020321  \n",
       "4    -0.070685 -0.069700  0.078444  0.408898  1.241559 -0.151853 -0.196577  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3078 -0.404599 -0.395586 -0.504007 -0.001980  1.394140 -0.356526 -0.057205  \n",
       "3079 -0.083000 -0.208448 -0.519165  0.292847  1.117920  0.169528  0.234989  \n",
       "3080 -1.029894 -0.855351 -0.247067  0.405077  1.160195  0.243745  0.261609  \n",
       "3081 -0.495103 -0.692927 -0.327008  0.045146  1.080402  0.078887 -0.075638  \n",
       "3082 -0.311444  0.269258 -0.207657  0.134844  0.806955 -0.166361  0.017548  \n",
       "\n",
       "[3083 rows x 768 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Z_text_df = pd.DataFrame(Z_text_tensor.numpy())\n",
    "Z_text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP è™•ç†æ•¸å€¼è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•¸å€¼å‘é‡èˆ‡èªæ„å‘é‡å·²æˆåŠŸèåˆç‚º Z_full_tensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1978,  0.4914, -0.4858,  ..., -0.0843, -0.0798,  0.0808],\n",
       "        [ 0.2151,  1.0001, -0.0443,  ...,  0.3707, -0.0328, -0.3490],\n",
       "        [ 0.8874,  0.9524,  0.4683,  ...,  0.1201,  0.1975, -0.1585],\n",
       "        ...,\n",
       "        [-1.2578,  0.2989, -0.8601,  ...,  0.0864,  0.0967, -0.2644],\n",
       "        [-0.8013,  0.4184, -0.8665,  ...,  0.5446,  0.1440, -0.5102],\n",
       "        [-0.7080,  0.6756, -0.2339,  ...,  0.0632,  0.0072,  0.0233]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# å‡è¨­ä½ å·²è¼‰å…¥ DataFrame ç‚º dfï¼Œä¸¦æœ‰é€™äº›æ¬„ä½\n",
    "numeric_cols = [\n",
    "    \"view_count\", \"followers_count\", \"emoji_count\", \"content_length\", \"post_hour\", \"time_elapsed_hours\",\n",
    "    \"has_photo\", \"like_count\", \"reply_count\", \"repost_count\", \"share_count\",\n",
    "    \"has_hashtag\", \"has_url\", \"has_mention\", \"has_exclaim\", \"has_question\"\n",
    "]\n",
    "\n",
    "# å¸ƒæ—æ¬„ä½è½‰ intï¼ˆä¿éšªèµ·è¦‹ï¼‰\n",
    "for col in numeric_cols:\n",
    "    if df_semantic[col].dtype == bool:\n",
    "        df_semantic[col] = df_semantic[col].astype(int)\n",
    "\n",
    "# æ¨™æº–åŒ–æ•¸å€¼ç‰¹å¾µ\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(df_semantic[numeric_cols])\n",
    "\n",
    "# å®šç¾©ç°¡å–®çš„ MLP æ¨¡å‹\n",
    "class NumericMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# åˆå§‹åŒ–èˆ‡åŸ·è¡Œæ¨¡å‹\n",
    "# è‡ªå‹•åµæ¸¬ä½ æ˜¯å¦æœ‰ GPUï¼ˆç”¨ CUDAï¼‰ï¼Œå¦å‰‡å°±ç”¨ CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# æ ¹æ“šä½ æ•¸å€¼æ¬„ä½çš„æ•¸é‡è¨­å®š input_dim\n",
    "model_num = NumericMLP(input_dim=X_num_scaled.shape[1])\n",
    "# æŠŠæ¨¡å‹é€åˆ°å°æ‡‰è£ç½®ï¼ˆCPU / GPUï¼‰\n",
    "model_num.to(device)\n",
    "\n",
    "# æš«æ™‚é—œé–‰æ¢¯åº¦é‹ç®—ï¼Œå› æ²’æœ‰è¦è¨“ç·´æ¨¡å‹ï¼Œåƒ…è®“è³‡æ–™éç¥ç¶“ç¶²è·¯\n",
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_num_scaled, dtype=torch.float32).to(device)\n",
    "    Z_num_tensor = model_num(X_tensor).cpu()\n",
    "\n",
    "# åˆä½µ Z_text å’Œ Z_num\n",
    "Z_text_tensor = torch.tensor(Z_text_df.values, dtype=torch.float32)  # [N, 768]\n",
    "Z_full_tensor = torch.cat([Z_text_tensor, Z_num_tensor], dim=1)      # [N, 896]\n",
    "\n",
    "print(\"æ•¸å€¼å‘é‡èˆ‡èªæ„å‘é‡å·²æˆåŠŸèåˆç‚º Z_full_tensor\")\n",
    "Z_full_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9GcFmuNEsVn"
   },
   "source": [
    "## æ¸…æ´—æ•¸æ“šembbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5H23LR9aF0ny"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3083/3083 [00:00<00:00, 11189.63 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3083/3083 [01:23<00:00, 37.14 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å…¨éƒ¨è™•ç†å®Œæˆï¼Œå·²è¼¸å‡º threads_with_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# =============== BERT å‘é‡åµŒå…¥ ===============\n",
    "df = df.dropna(subset=['content']) #è¦å…ˆè™•ç†contentç©ºå€¼æ‰èƒ½embedding\n",
    "# --- è¼‰å…¥ tokenizer & model ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "# é¸æ“‡ç¡¬é«”è¨­å‚™ï¼ˆMPSã€CUDAã€CPUï¼‰ï¼Œè‡ªå‹•åˆ¤æ–·æ˜¯å¦å¯ç”¨ GPUï¼ˆM1/M2 æ™¶ç‰‡ä¸Šçš„ MPS æˆ– CUDAï¼‰ï¼Œå¦å‰‡ fallback åˆ° CPU\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# --- å»ºç«‹ HuggingFace Dataset ---\n",
    "hf_dataset = Dataset.from_pandas(df[[\"content\"]])\n",
    "\n",
    "# --- tokenize function ---\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['content'], truncation=True, padding='max_length', max_length=128)\n",
    "tokenized_dataset = hf_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# --- å–å¾— [CLS] å‘é‡ ---\n",
    "def extract_embeddings(batch):\n",
    "    inputs = {k: torch.tensor(v).to(model.device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
    "    return {\"embeddings\": embeddings}\n",
    "\n",
    "# --- æ‰¹æ¬¡è½‰æ›ç‚º embeddings ---\n",
    "batch_size = 64\n",
    "embeddings_dataset = tokenized_dataset.map(extract_embeddings, batched=True, batch_size=batch_size)\n",
    "\n",
    "# =============== åŒ¯å‡ºæœ€çµ‚çµæœ ===============\n",
    "# embeddings_dataset[\"embeddings\"] æ˜¯ list of 768-dim vectors\n",
    "embedding_df = pd.DataFrame(embeddings_dataset[\"embeddings\"])\n",
    "final_df = pd.concat([df.reset_index(drop=True), embedding_df], axis=1)\n",
    "\n",
    "# å„²å­˜\n",
    "# final_df.to_csv(\"C:/Users/User/Desktop/louis/threads_with_embeddings.csv\",encoding='utf_8_sig', index=False)\n",
    "final_df.to_csv(\"threads_with_embeddings.csv\",encoding='utf_8_sig', index=False)\n",
    "print(\"âœ… å…¨éƒ¨è™•ç†å®Œæˆï¼Œå·²è¼¸å‡º threads_with_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WxP6bcNEywY"
   },
   "source": [
    "## åˆ†è©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SovtoK069ox_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/59/tx1m33l955b0h85n6dlwm4440000gn/T/jieba.cache\n",
      "Loading model cost 0.499 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'è€ƒå®Œ æœŸä¸­è€ƒ ï¼Œ æˆç¸¾ éƒ½ é‚„æ²’å‡º ä¾† ï¼Œ å°ä¸€ å¥³å…’ å°± è‡ªä¿¡ å° æˆ‘ èªª ï¼š ã€Œ æˆ‘ çœŸç¾¨æ…• å¦³ ç”Ÿ ä¸€å€‹ å¤©æ‰ ï¼ ã€'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = set(['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'ä¹Ÿ', 'èˆ‡', 'æœ‰', 'ç‚º', 'ç­‰'])\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    words = jieba.cut(text)\n",
    "    words_filtered = [word for word in words if word not in stopwords]\n",
    "    return ' '.join(words_filtered)\n",
    "\n",
    "\n",
    "df['processed_content'] = df['content'].apply(tokenize_and_remove_stopwords)\n",
    "df['processed_content'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwCtvYh4E3vU"
   },
   "source": [
    "## æ©Ÿå™¨å­¸ç¿’å»ºæ¨¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eFdTaLb79ovp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 33538 stored elements and shape (3083, 12873)>\n",
      "  Coords\tValues\n",
      "  (0, 2799)\t0.4663056843152708\n",
      "  (0, 3188)\t0.2940455207159238\n",
      "  (0, 1306)\t0.44284097627197305\n",
      "  (0, 1210)\t0.31028579706458703\n",
      "  (0, 412)\t0.503121337625026\n",
      "  (0, 892)\t0.3880460720366238\n",
      "  (1, 9695)\t0.44774054192975266\n",
      "  (1, 10472)\t0.44774054192975266\n",
      "  (1, 8813)\t0.2568875443528821\n",
      "  (1, 3491)\t0.42597495668833046\n",
      "  (1, 10367)\t0.38876644295524665\n",
      "  (1, 9613)\t0.44774054192975266\n",
      "  (2, 10514)\t0.31144203947399207\n",
      "  (2, 8415)\t0.3288785193471119\n",
      "  (2, 7290)\t0.28537598711095685\n",
      "  (2, 12071)\t0.35868637857163266\n",
      "  (2, 6458)\t0.35868637857163266\n",
      "  (2, 6044)\t0.2990706601225911\n",
      "  (2, 10681)\t0.3288785193471119\n",
      "  (2, 9846)\t0.35868637857163266\n",
      "  (2, 3221)\t0.18120453315645962\n",
      "  (2, 5960)\t0.31144203947399207\n",
      "  (3, 1628)\t0.26164267502394156\n",
      "  (3, 538)\t0.23056465830230907\n",
      "  (3, 2699)\t0.3728698840922293\n",
      "  :\t:\n",
      "  (3080, 3034)\t0.4140307205152917\n",
      "  (3080, 3064)\t0.38762922360836444\n",
      "  (3080, 2872)\t0.36574251223930415\n",
      "  (3081, 1628)\t0.230329717220129\n",
      "  (3081, 692)\t0.2079266247071255\n",
      "  (3081, 2802)\t0.16248268778705707\n",
      "  (3081, 2801)\t0.21355771222235617\n",
      "  (3081, 1395)\t0.22662292335957387\n",
      "  (3081, 2812)\t0.23299934372416703\n",
      "  (3081, 1432)\t0.31634551298701585\n",
      "  (3081, 2872)\t0.25900164914242463\n",
      "  (3081, 2798)\t0.28767358106472024\n",
      "  (3081, 2289)\t0.34501744490931147\n",
      "  (3081, 1560)\t0.34501744490931147\n",
      "  (3081, 2975)\t0.34501744490931147\n",
      "  (3081, 1719)\t0.34501744490931147\n",
      "  (3082, 538)\t0.24991449565436066\n",
      "  (3082, 2827)\t0.2748480556731932\n",
      "  (3082, 2173)\t0.4041625013727274\n",
      "  (3082, 3058)\t0.2836006942769654\n",
      "  (3082, 2101)\t0.25158453952967386\n",
      "  (3082, 1169)\t0.4041625013727274\n",
      "  (3082, 1421)\t0.3221909718872812\n",
      "  (3082, 3166)\t0.3688592851939238\n",
      "  (3082, 2821)\t0.3895103428133763\n"
     ]
    }
   ],
   "source": [
    "# è¨ˆç®— TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_content'])\n",
    "\n",
    "# è¨ˆç®— TF\n",
    "tf_vectorizer = CountVectorizer()\n",
    "tf_matrix = tf_vectorizer.fit_transform(df['processed_content'])\n",
    "\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wvN8ioFE8Cz"
   },
   "source": [
    "# å¤šæ¨¡å‹åˆ†é¡å¯¦é©—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cROM4E4v9otD"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>content_length</th>\n",
       "      <th>lang</th>\n",
       "      <th>scrape_time</th>\n",
       "      <th>post_weekday</th>\n",
       "      <th>post_hour</th>\n",
       "      <th>post_period</th>\n",
       "      <th>viral</th>\n",
       "      <th>post_time</th>\n",
       "      <th>...</th>\n",
       "      <th>emojis</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>scrape_time_origin</th>\n",
       "      <th>has_question</th>\n",
       "      <th>has_exclaim</th>\n",
       "      <th>has_url</th>\n",
       "      <th>has_mention</th>\n",
       "      <th>has_hashtag</th>\n",
       "      <th>time_elapsed_hours</th>\n",
       "      <th>processed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>Thank you God for another day.</td>\n",
       "      <td>30</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 06:27</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T10:58:39.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:27:40.176749+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Thank   you   God   for   another   day .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ç™¾é”ç¿¡éº—ï¼Ÿ æ²’æœ‰ä¸‹é™çš„ç¶²è·¯ç—…æ…‹ï¼</td>\n",
       "      <td>16</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 06:27</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-29T12:33:44.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:27:54.964788+08:00</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ç™¾é” ç¿¡éº— ï¼Ÿ   æ²’æœ‰ ä¸‹é™ ç¶²è·¯ ç—…æ…‹ ï¼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ribboworld2021</td>\n",
       "      <td>è€ƒå®ŒæœŸä¸­è€ƒï¼Œæˆç¸¾éƒ½é‚„æ²’å‡ºä¾†ï¼Œå°ä¸€å¥³å…’å°±è‡ªä¿¡çš„å°æˆ‘èªªï¼šã€Œæˆ‘çœŸç¾¨æ…•å¦³ç”Ÿäº†ä¸€å€‹å¤©æ‰ï¼ã€</td>\n",
       "      <td>40</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T04:39:46.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:09.873641+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "      <td>è€ƒå®Œ æœŸä¸­è€ƒ ï¼Œ æˆç¸¾ éƒ½ é‚„æ²’å‡º ä¾† ï¼Œ å°ä¸€ å¥³å…’ å°± è‡ªä¿¡ å° æˆ‘ èªª ï¼š ã€Œ æˆ‘ çœŸ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>Just be strong. Confident. Hopeful. Intellectu...</td>\n",
       "      <td>69</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T11:25:22.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:24.726576+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Just   be   strong .   Confident .   Hopeful ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jose_ykc</td>\n",
       "      <td>æ—¥æ–‡è¼”ç³»è€å¸«ä¸Šèª²å…§å®¹ä¹‹ä¸€AiScReam æ­Œè©å°è®€</td>\n",
       "      <td>25</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-29T10:36:19.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:39.393706+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>æ—¥æ–‡ è¼”ç³» è€å¸« ä¸Š èª²å…§å®¹ ä¹‹ä¸€ AiScReam   æ­Œè© å°è®€</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>leighton.williams</td>\n",
       "      <td>First tasting in California</td>\n",
       "      <td>27</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:23</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 07:30</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:23:40.328046+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8.0</td>\n",
       "      <td>First   tasting   in   California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>cape__man</td>\n",
       "      <td>I hoped it would have been better.</td>\n",
       "      <td>34</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:23</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 06:31</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:23:55.063517+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "      <td>I   hoped   it   would   have   been   better .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>simimoonlight</td>\n",
       "      <td>I canâ€™t wait to watch BeyoncÃ© on TikTok tonigh...</td>\n",
       "      <td>51</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:24</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 06:26</td>\n",
       "      <td>...</td>\n",
       "      <td>ğŸ«¶ğŸ¿ğŸ¥¹</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-04-30 00:24:39.870994+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "      <td>I   can â€™ t   wait   to   watch   Beyonc Ã©   o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>other98</td>\n",
       "      <td>Tonight, Canada just proved that they have a h...</td>\n",
       "      <td>77</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:24</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 12:31</td>\n",
       "      <td>...</td>\n",
       "      <td>ğŸ™Œ</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-30 00:24:54.669936+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Tonight ,   Canada   just   proved   that   th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>scottiebeam</td>\n",
       "      <td>Yall be fighting on here â€¦ i thought threads w...</td>\n",
       "      <td>57</td>\n",
       "      <td>en</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:25</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 01:33</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:25:09.396585+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Yall   be   fighting   on   here   â€¦   i   tho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3083 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                                            content  \\\n",
       "0                ayofvr                     Thank you God for another day.   \n",
       "1                   NaN                                   ç™¾é”ç¿¡éº—ï¼Ÿ æ²’æœ‰ä¸‹é™çš„ç¶²è·¯ç—…æ…‹ï¼   \n",
       "2        ribboworld2021           è€ƒå®ŒæœŸä¸­è€ƒï¼Œæˆç¸¾éƒ½é‚„æ²’å‡ºä¾†ï¼Œå°ä¸€å¥³å…’å°±è‡ªä¿¡çš„å°æˆ‘èªªï¼šã€Œæˆ‘çœŸç¾¨æ…•å¦³ç”Ÿäº†ä¸€å€‹å¤©æ‰ï¼ã€   \n",
       "3                ayofvr  Just be strong. Confident. Hopeful. Intellectu...   \n",
       "4              jose_ykc                          æ—¥æ–‡è¼”ç³»è€å¸«ä¸Šèª²å…§å®¹ä¹‹ä¸€AiScReam æ­Œè©å°è®€   \n",
       "...                 ...                                                ...   \n",
       "3096  leighton.williams                        First tasting in California   \n",
       "3097          cape__man                 I hoped it would have been better.   \n",
       "3098      simimoonlight  I canâ€™t wait to watch BeyoncÃ© on TikTok tonigh...   \n",
       "3099            other98  Tonight, Canada just proved that they have a h...   \n",
       "3100        scottiebeam  Yall be fighting on here â€¦ i thought threads w...   \n",
       "\n",
       "      content_length lang        scrape_time post_weekday  post_hour  \\\n",
       "0                 30   en  2025å¹´04æœˆ30æ—¥ 06:27    Wednesday          6   \n",
       "1                 16   Ch  2025å¹´04æœˆ30æ—¥ 06:27    Wednesday          6   \n",
       "2                 40   Ch  2025å¹´04æœˆ30æ—¥ 06:28    Wednesday          6   \n",
       "3                 69   en  2025å¹´04æœˆ30æ—¥ 06:28    Wednesday          6   \n",
       "4                 25   Ch  2025å¹´04æœˆ30æ—¥ 06:28    Wednesday          6   \n",
       "...              ...  ...                ...          ...        ...   \n",
       "3096              27   en  2025å¹´04æœˆ30æ—¥ 00:23    Wednesday          0   \n",
       "3097              34   en  2025å¹´04æœˆ30æ—¥ 00:23    Wednesday          0   \n",
       "3098              51   en  2025å¹´04æœˆ30æ—¥ 00:24    Wednesday          0   \n",
       "3099              77   en  2025å¹´04æœˆ30æ—¥ 00:24    Wednesday          0   \n",
       "3100              57   en  2025å¹´04æœˆ30æ—¥ 00:25    Wednesday          0   \n",
       "\n",
       "     post_period  viral                 post_time  ... emojis emoji_count  \\\n",
       "0        morning      0  2025-04-29T10:58:39.000Z  ...                  0   \n",
       "1        morning      1  2025-04-29T12:33:44.000Z  ...                  0   \n",
       "2        morning      0  2025-04-29T04:39:46.000Z  ...                  0   \n",
       "3        morning      0  2025-04-29T11:25:22.000Z  ...                  0   \n",
       "4        morning      1  2025-04-29T10:36:19.000Z  ...                  0   \n",
       "...          ...    ...                       ...  ...    ...         ...   \n",
       "3096       night      0         2025å¹´04æœˆ29æ—¥ 07:30  ...                  0   \n",
       "3097       night      0         2025å¹´04æœˆ29æ—¥ 06:31  ...                  0   \n",
       "3098       night      1         2025å¹´04æœˆ29æ—¥ 06:26  ...    ğŸ«¶ğŸ¿ğŸ¥¹           3   \n",
       "3099       night      1         2025å¹´04æœˆ29æ—¥ 12:31  ...      ğŸ™Œ           1   \n",
       "3100       night      1         2025å¹´04æœˆ29æ—¥ 01:33  ...                  0   \n",
       "\n",
       "                   scrape_time_origin  has_question  has_exclaim  has_url  \\\n",
       "0    2025-04-30 06:27:40.176749+08:00         False        False    False   \n",
       "1    2025-04-30 06:27:54.964788+08:00          True         True    False   \n",
       "2    2025-04-30 06:28:09.873641+08:00         False         True    False   \n",
       "3    2025-04-30 06:28:24.726576+08:00         False        False    False   \n",
       "4    2025-04-30 06:28:39.393706+08:00         False        False    False   \n",
       "...                               ...           ...          ...      ...   \n",
       "3096 2025-04-30 00:23:40.328046+08:00         False        False    False   \n",
       "3097 2025-04-30 00:23:55.063517+08:00         False        False    False   \n",
       "3098 2025-04-30 00:24:39.870994+08:00         False        False    False   \n",
       "3099 2025-04-30 00:24:54.669936+08:00         False         True    False   \n",
       "3100 2025-04-30 00:25:09.396585+08:00         False        False    False   \n",
       "\n",
       "      has_mention  has_hashtag  time_elapsed_hours  \\\n",
       "0           False        False                 3.0   \n",
       "1           False        False                 1.0   \n",
       "2           False        False                 9.0   \n",
       "3           False        False                 3.0   \n",
       "4           False        False                 3.0   \n",
       "...           ...          ...                 ...   \n",
       "3096        False        False                 8.0   \n",
       "3097        False        False                 9.0   \n",
       "3098        False        False                 9.0   \n",
       "3099        False        False                 3.0   \n",
       "3100        False        False                14.0   \n",
       "\n",
       "                                      processed_content  \n",
       "0             Thank   you   God   for   another   day .  \n",
       "1                               ç™¾é” ç¿¡éº— ï¼Ÿ   æ²’æœ‰ ä¸‹é™ ç¶²è·¯ ç—…æ…‹ ï¼  \n",
       "2     è€ƒå®Œ æœŸä¸­è€ƒ ï¼Œ æˆç¸¾ éƒ½ é‚„æ²’å‡º ä¾† ï¼Œ å°ä¸€ å¥³å…’ å°± è‡ªä¿¡ å° æˆ‘ èªª ï¼š ã€Œ æˆ‘ çœŸ...  \n",
       "3     Just   be   strong .   Confident .   Hopeful ....  \n",
       "4                    æ—¥æ–‡ è¼”ç³» è€å¸« ä¸Š èª²å…§å®¹ ä¹‹ä¸€ AiScReam   æ­Œè© å°è®€  \n",
       "...                                                 ...  \n",
       "3096                  First   tasting   in   California  \n",
       "3097    I   hoped   it   would   have   been   better .  \n",
       "3098  I   can â€™ t   wait   to   watch   Beyonc Ã©   o...  \n",
       "3099  Tonight ,   Canada   just   proved   that   th...  \n",
       "3100  Yall   be   fighting   on   here   â€¦   i   tho...  \n",
       "\n",
       "[3083 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, random_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 8\n",
    "num_epochs = 5\n",
    "# ========== åƒæ•¸è¨­å®š ==========\n",
    "model_tokenizer_map = {\n",
    "    \"FusionMacBERT\": \"hfl/chinese-macbert-base\",\n",
    "    \"PureMacBERT\": \"hfl/chinese-macbert-base\",\n",
    "    \"NumericOnly\": None,\n",
    "    \"BiLSTMWithNumeric\": \"bert-base-chinese\",\n",
    "    \"MacBERTWithGRU\": \"hfl/chinese-macbert-base\",\n",
    "    \"MacBERTMLPFusion\": \"hfl/chinese-macbert-base\",\n",
    "    \"TextCNNMacBERT\": \"hfl/chinese-macbert-base\",\n",
    "    \"RoBERTa\": \"hfl/chinese-roberta-wwm-ext\",\n",
    "    \"BERTwwmExt\": \"hfl/chinese-bert-wwm-ext\",\n",
    "    \"ERNIE\": \"nghuyong/ernie-3.0-base-zh\",\n",
    "    \"ConvBERT\": \"YituTech/conv-bert-base\"\n",
    "}\n",
    "\n",
    "#tokenizer\n",
    "default_tokenizer_name = model_tokenizer_map[\"FusionMacBERT\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(default_tokenizer_name)\n",
    "\n",
    "#è¼‰å…¥è³‡æ–™\n",
    "# df = pd.read_csv(\"C:/Users/User/Desktop/louis/threads_cleaned_v2.csv\", encoding='utf_8_sig')\n",
    "#df = df.dropna(subset=['content', 'view_count']).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibbctyH9CUzu"
   },
   "source": [
    "## Label åˆ†ç¾¤ ï¼ˆ1000ä»¥ä¸‹ã€1000~10000ã€10000~100000ã€100000ä»¥ä¸Š)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5yL13rSfCUzu"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>content_length</th>\n",
       "      <th>lang</th>\n",
       "      <th>scrape_time</th>\n",
       "      <th>post_weekday</th>\n",
       "      <th>post_hour</th>\n",
       "      <th>post_period</th>\n",
       "      <th>viral</th>\n",
       "      <th>post_time</th>\n",
       "      <th>...</th>\n",
       "      <th>scrape_time_origin</th>\n",
       "      <th>has_question</th>\n",
       "      <th>has_exclaim</th>\n",
       "      <th>has_url</th>\n",
       "      <th>has_mention</th>\n",
       "      <th>has_hashtag</th>\n",
       "      <th>time_elapsed_hours</th>\n",
       "      <th>processed_content</th>\n",
       "      <th>view_class</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_jili.25</td>\n",
       "      <td>æˆ‘è¦è®“æ‰åŸæœ¬çš„æ‰èƒ½å»vipå•Šå•Šå•Šå•ŠğŸ˜­ğŸ˜­ 1</td>\n",
       "      <td>21</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 08:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>8</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T06:48:05.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-04-30 08:28:10.692106+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "      <td>æˆ‘è¦ è®“ æ‰ åŸæœ¬ æ‰èƒ½ å» vip å•Šå•Šå•Š å•Š ğŸ˜­ ğŸ˜­   1</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lv_auer</td>\n",
       "      <td>æ‹”å®Œæ™ºé½’ç¬¬ä¸‰å¤©äº†ï¼Œåˆ°åº•è¦ç—›å¹¾å¤©ï¼Œæˆ‘éƒ½å¿«ä¸èƒ½åƒæ±è¥¿äº†ğŸ˜‚ğŸ˜‚ #å“¦è—‰æ©Ÿæ¸›è‚¥</td>\n",
       "      <td>34</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 08:07</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>8</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T10:55:49.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-04-30 08:07:36.828332+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>æ‹” å®Œæ™ºé½’ ç¬¬ä¸‰å¤© ï¼Œ åˆ°åº• è¦ç—› å¹¾å¤© ï¼Œ æˆ‘ éƒ½ å¿« ä¸èƒ½ åƒ æ±è¥¿ ğŸ˜‚ ğŸ˜‚   # å“¦...</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cai.7052</td>\n",
       "      <td>å¦‚æœæœ‰ä¸‹è¼©å­çš„è©±ï¼Œæˆ‘ä¸€å®šè¦ç•¶å¥³ç”Ÿï¼Œç•¶ç”·ç”Ÿå¤ªç´¯å¤ªè‹¦äº†</td>\n",
       "      <td>25</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025å¹´05æœˆ03æ—¥ 19:30</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>19</td>\n",
       "      <td>evening</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-03T01:07:29.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-05-03 19:30:28.096090+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>å¦‚æœ ä¸‹è¼©å­ è©± ï¼Œ æˆ‘ ä¸€å®š è¦ç•¶ å¥³ç”Ÿ ï¼Œ ç•¶ ç”·ç”Ÿ å¤ªç´¯ å¤ªè‹¦</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brandin.podziemski2</td>\n",
       "      <td>æ‹å¾—æˆ‘çœŸå¸¥</td>\n",
       "      <td>5</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025å¹´05æœˆ03æ—¥ 19:55</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>19</td>\n",
       "      <td>evening</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-03T00:07:39.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-05-03 19:55:44.483391+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>æ‹å¾— æˆ‘ çœŸå¸¥</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emo_97_10_31</td>\n",
       "      <td>ğŸ«¤åœ¨è„†ä¸Šé¢çœŸçš„å¯ä»¥æ‰¾åˆ°çœŸå¿ƒçš„æœ‹å‹å—</td>\n",
       "      <td>17</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 07:50</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>7</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T11:39:04.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-04-30 07:50:25.230620+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "      <td>ğŸ«¤ è„† ä¸Šé¢ çœŸçš„ å¯ä»¥ æ‰¾åˆ° çœŸå¿ƒ æœ‹å‹ å—</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11342</th>\n",
       "      <td>goooosleep</td>\n",
       "      <td>ä¸ºä»€ä¹ˆè´è¶è¦é£åˆ°åå…­æ¥¼ï¼Ÿ</td>\n",
       "      <td>12</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025å¹´04æœˆ30æ—¥ 00:08</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 12:03</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-04-30 00:08:40.964074+08:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "      <td>ä¸ºä»€ä¹ˆ è´è¶ è¦ é£åˆ° åå…­ æ¥¼ ï¼Ÿ</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11343</th>\n",
       "      <td>kickflip_said</td>\n",
       "      <td>æˆ‘ç¾åœ¨çœ‹åˆ°å¤§å®¶è²·åˆ°ç¥¨é‚„æœ‰äº’å ±èªªVIPå€é‚„æœ‰ç¥¨çš„ä¸²æ–‡å°±æ˜¯ä¸€ç›´åœ¨ç ´é˜² è¾¦åœ¨æœŸæœ«è€ƒå‰ä¸€é€±æˆ‘çœŸçš„è¦å“­å‡ºä¾†</td>\n",
       "      <td>48</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 07:24</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>7</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025å¹´04æœˆ28æ—¥ 18:47</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-04-29 07:24:48.150110+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "      <td>æˆ‘ç¾ çœ‹åˆ° å¤§å®¶ è²·åˆ° ç¥¨ é‚„æœ‰ äº’å ± èªª VIP å€é‚„ æœ‰ç¥¨ ä¸²æ–‡ å°±æ˜¯ ä¸€ç›´ ç ´é˜²   ...</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11344</th>\n",
       "      <td>lucasbiubiubiubiu</td>\n",
       "      <td>äº¬ä¸œå¤–å–çœŸçš„æ˜¯ä¸€æ¬¡éƒ½æ²¡å‡†æ—¶è¿‡ï¼Œä¸æ˜¯æ—©åŠä¸ªå°æ—¶å°±æ˜¯æ™šåŠä¸ªå°æ—¶ğŸ˜¡</td>\n",
       "      <td>30</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025å¹´04æœˆ29æ—¥ 07:14</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>7</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025å¹´04æœˆ28æ—¥ 11:27</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-04-29 07:14:37.311967+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>11.0</td>\n",
       "      <td>äº¬ä¸œ å¤–å– çœŸçš„ ä¸€æ¬¡ éƒ½ æ²¡å‡† æ—¶è¿‡ ï¼Œ ä¸æ˜¯ æ—© åŠä¸ª å°æ—¶ å°±æ˜¯ æ™š åŠä¸ª å°æ—¶ ğŸ˜¡</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11345</th>\n",
       "      <td>clydecole620</td>\n",
       "      <td>Malik Beasley. Michael Ealy. Pop a wheelie. Kn...</td>\n",
       "      <td>56</td>\n",
       "      <td>tl</td>\n",
       "      <td>2025å¹´05æœˆ03æ—¥ 08:00</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>8</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-02T15:29:08.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-05-03 08:00:53.883370+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Malik   Beasley .   Michael   Ealy .   Pop   a...</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11346</th>\n",
       "      <td>hflyizgt</td>\n",
       "      <td>så±¬æ€§çˆ†ç™¼ shinonome akito</td>\n",
       "      <td>21</td>\n",
       "      <td>st</td>\n",
       "      <td>2025å¹´05æœˆ02æ—¥ 01:38</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-01T09:06:59.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-05-02 01:38:12.735659+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>s å±¬æ€§ çˆ†ç™¼   shinonome   akito</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11347 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    author                                            content  \\\n",
       "0                 _jili.25                              æˆ‘è¦è®“æ‰åŸæœ¬çš„æ‰èƒ½å»vipå•Šå•Šå•Šå•ŠğŸ˜­ğŸ˜­ 1   \n",
       "1                  lv_auer                 æ‹”å®Œæ™ºé½’ç¬¬ä¸‰å¤©äº†ï¼Œåˆ°åº•è¦ç—›å¹¾å¤©ï¼Œæˆ‘éƒ½å¿«ä¸èƒ½åƒæ±è¥¿äº†ğŸ˜‚ğŸ˜‚ #å“¦è—‰æ©Ÿæ¸›è‚¥   \n",
       "2                 cai.7052                          å¦‚æœæœ‰ä¸‹è¼©å­çš„è©±ï¼Œæˆ‘ä¸€å®šè¦ç•¶å¥³ç”Ÿï¼Œç•¶ç”·ç”Ÿå¤ªç´¯å¤ªè‹¦äº†   \n",
       "3      brandin.podziemski2                                              æ‹å¾—æˆ‘çœŸå¸¥   \n",
       "4             emo_97_10_31                                  ğŸ«¤åœ¨è„†ä¸Šé¢çœŸçš„å¯ä»¥æ‰¾åˆ°çœŸå¿ƒçš„æœ‹å‹å—   \n",
       "...                    ...                                                ...   \n",
       "11342           goooosleep                                       ä¸ºä»€ä¹ˆè´è¶è¦é£åˆ°åå…­æ¥¼ï¼Ÿ   \n",
       "11343        kickflip_said   æˆ‘ç¾åœ¨çœ‹åˆ°å¤§å®¶è²·åˆ°ç¥¨é‚„æœ‰äº’å ±èªªVIPå€é‚„æœ‰ç¥¨çš„ä¸²æ–‡å°±æ˜¯ä¸€ç›´åœ¨ç ´é˜² è¾¦åœ¨æœŸæœ«è€ƒå‰ä¸€é€±æˆ‘çœŸçš„è¦å“­å‡ºä¾†   \n",
       "11344    lucasbiubiubiubiu                     äº¬ä¸œå¤–å–çœŸçš„æ˜¯ä¸€æ¬¡éƒ½æ²¡å‡†æ—¶è¿‡ï¼Œä¸æ˜¯æ—©åŠä¸ªå°æ—¶å°±æ˜¯æ™šåŠä¸ªå°æ—¶ğŸ˜¡   \n",
       "11345         clydecole620  Malik Beasley. Michael Ealy. Pop a wheelie. Kn...   \n",
       "11346             hflyizgt                              så±¬æ€§çˆ†ç™¼ shinonome akito   \n",
       "\n",
       "       content_length lang        scrape_time post_weekday  post_hour  \\\n",
       "0                  21   Ch  2025å¹´04æœˆ30æ—¥ 08:28    Wednesday          8   \n",
       "1                  34   Ch  2025å¹´04æœˆ30æ—¥ 08:07    Wednesday          8   \n",
       "2                  25   Ch  2025å¹´05æœˆ03æ—¥ 19:30     Saturday         19   \n",
       "3                   5   Ch  2025å¹´05æœˆ03æ—¥ 19:55     Saturday         19   \n",
       "4                  17   Ch  2025å¹´04æœˆ30æ—¥ 07:50    Wednesday          7   \n",
       "...               ...  ...                ...          ...        ...   \n",
       "11342              12   Ch  2025å¹´04æœˆ30æ—¥ 00:08    Wednesday          0   \n",
       "11343              48   Ch  2025å¹´04æœˆ29æ—¥ 07:24      Tuesday          7   \n",
       "11344              30   Ch  2025å¹´04æœˆ29æ—¥ 07:14      Tuesday          7   \n",
       "11345              56   tl  2025å¹´05æœˆ03æ—¥ 08:00     Saturday          8   \n",
       "11346              21   st  2025å¹´05æœˆ02æ—¥ 01:38       Friday          1   \n",
       "\n",
       "      post_period  viral                 post_time  ...  \\\n",
       "0         morning      0  2025-04-29T06:48:05.000Z  ...   \n",
       "1         morning      0  2025-04-29T10:55:49.000Z  ...   \n",
       "2         evening      0  2025-05-03T01:07:29.000Z  ...   \n",
       "3         evening      0  2025-05-03T00:07:39.000Z  ...   \n",
       "4         morning      0  2025-04-29T11:39:04.000Z  ...   \n",
       "...           ...    ...                       ...  ...   \n",
       "11342       night      0         2025å¹´04æœˆ29æ—¥ 12:03  ...   \n",
       "11343     morning      0         2025å¹´04æœˆ28æ—¥ 18:47  ...   \n",
       "11344     morning      0         2025å¹´04æœˆ28æ—¥ 11:27  ...   \n",
       "11345     morning      0  2025-05-02T15:29:08.000Z  ...   \n",
       "11346       night      0  2025-05-01T09:06:59.000Z  ...   \n",
       "\n",
       "                    scrape_time_origin has_question  has_exclaim  has_url  \\\n",
       "0     2025-04-30 08:28:10.692106+08:00        False        False    False   \n",
       "1     2025-04-30 08:07:36.828332+08:00        False        False    False   \n",
       "2     2025-05-03 19:30:28.096090+08:00        False        False    False   \n",
       "3     2025-05-03 19:55:44.483391+08:00        False        False    False   \n",
       "4     2025-04-30 07:50:25.230620+08:00        False        False    False   \n",
       "...                                ...          ...          ...      ...   \n",
       "11342 2025-04-30 00:08:40.964074+08:00         True        False    False   \n",
       "11343 2025-04-29 07:24:48.150110+08:00        False        False    False   \n",
       "11344 2025-04-29 07:14:37.311967+08:00        False        False    False   \n",
       "11345 2025-05-03 08:00:53.883370+08:00        False        False    False   \n",
       "11346 2025-05-02 01:38:12.735659+08:00        False        False    False   \n",
       "\n",
       "       has_mention  has_hashtag  time_elapsed_hours  \\\n",
       "0            False        False                 9.0   \n",
       "1            False         True                 5.0   \n",
       "2            False        False                 2.0   \n",
       "3            False        False                 3.0   \n",
       "4            False        False                 4.0   \n",
       "...            ...          ...                 ...   \n",
       "11342        False        False                 4.0   \n",
       "11343        False        False                 4.0   \n",
       "11344        False        False                11.0   \n",
       "11345        False        False                 0.5   \n",
       "11346        False        False                 0.5   \n",
       "\n",
       "                                       processed_content  view_class  label  \n",
       "0                       æˆ‘è¦ è®“ æ‰ åŸæœ¬ æ‰èƒ½ å» vip å•Šå•Šå•Š å•Š ğŸ˜­ ğŸ˜­   1         low      1  \n",
       "1      æ‹” å®Œæ™ºé½’ ç¬¬ä¸‰å¤© ï¼Œ åˆ°åº• è¦ç—› å¹¾å¤© ï¼Œ æˆ‘ éƒ½ å¿« ä¸èƒ½ åƒ æ±è¥¿ ğŸ˜‚ ğŸ˜‚   # å“¦...         low      1  \n",
       "2                     å¦‚æœ ä¸‹è¼©å­ è©± ï¼Œ æˆ‘ ä¸€å®š è¦ç•¶ å¥³ç”Ÿ ï¼Œ ç•¶ ç”·ç”Ÿ å¤ªç´¯ å¤ªè‹¦         low      1  \n",
       "3                                                æ‹å¾— æˆ‘ çœŸå¸¥         low      1  \n",
       "4                                ğŸ«¤ è„† ä¸Šé¢ çœŸçš„ å¯ä»¥ æ‰¾åˆ° çœŸå¿ƒ æœ‹å‹ å—         low      1  \n",
       "...                                                  ...         ...    ...  \n",
       "11342                                 ä¸ºä»€ä¹ˆ è´è¶ è¦ é£åˆ° åå…­ æ¥¼ ï¼Ÿ         low      1  \n",
       "11343  æˆ‘ç¾ çœ‹åˆ° å¤§å®¶ è²·åˆ° ç¥¨ é‚„æœ‰ äº’å ± èªª VIP å€é‚„ æœ‰ç¥¨ ä¸²æ–‡ å°±æ˜¯ ä¸€ç›´ ç ´é˜²   ...         low      1  \n",
       "11344      äº¬ä¸œ å¤–å– çœŸçš„ ä¸€æ¬¡ éƒ½ æ²¡å‡† æ—¶è¿‡ ï¼Œ ä¸æ˜¯ æ—© åŠä¸ª å°æ—¶ å°±æ˜¯ æ™š åŠä¸ª å°æ—¶ ğŸ˜¡         low      1  \n",
       "11345  Malik   Beasley .   Michael   Ealy .   Pop   a...         low      1  \n",
       "11346                        s å±¬æ€§ çˆ†ç™¼   shinonome   akito         low      1  \n",
       "\n",
       "[11347 rows x 33 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å°‡ view_count åˆ†æˆå››é¡ï¼š\n",
    "# 0: å°æ–¼ 1000\n",
    "# 1: 1000 ~ 9999\n",
    "# 2: 10000 ~ 99999\n",
    "# 3: 100000 ä»¥ä¸Š\n",
    "\n",
    "def map_view_class(x):\n",
    "    if x < 1000:\n",
    "        return 'low'\n",
    "    elif x < 10000:\n",
    "        return 'medium'\n",
    "    elif x < 100000:\n",
    "        return 'high'\n",
    "    else:\n",
    "        return 'very_high'\n",
    "\n",
    "df['view_class'] = df['view_count'].apply(map_view_class)\n",
    "\n",
    "# ç·¨ç¢¼æˆæ•¸å­— label\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['view_class'])\n",
    "\n",
    "df_high = df[df['view_class'] == 'very_high']\n",
    "df_medium = df[df['view_class'] == 'high']\n",
    "df_low = df[df['view_class'] == 'medium']\n",
    "df_very_low = df[df['view_class'] == 'low']\n",
    "\n",
    "# é‡å°è¼ƒå°‘çš„é¡åˆ¥é€²è¡Œæ“´å¢ï¼ˆå‡è¨­ high å’Œ very_low æ¯”è¼ƒå°‘ï¼‰\n",
    "df_high_oversampled = pd.concat([df_high] * 3, ignore_index=True)\n",
    "df_very_low_oversampled = pd.concat([df_very_low] * 3, ignore_index=True)\n",
    "\n",
    "# åˆä½µä¸¦æ‰“äº‚\n",
    "df = pd.concat([df_medium, df_low, df_high_oversampled, df_very_low_oversampled], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeqEslof2OBB"
   },
   "source": [
    "# Normalization æ•¸å€¼ç‰¹å¾µæ¨™æº–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "N3pJMu-g9on2"
   },
   "outputs": [],
   "source": [
    "# Normalization æ•¸å€¼ç‰¹å¾µæ¨™æº–åŒ–\n",
    "base_num_cols = ['like_count', 'share_count', 'repost_count', 'reply_count', 'emoji_count', 'has_photo', 'has_video', 'has_question', 'has_exclaim', 'has_mention', 'has_url', 'has_hashtag', 'content_length']\n",
    "# æ‰¾å‡º one-hot ç·¨ç¢¼çš„æ¬„ä½ï¼ˆèªè¨€é¡å‹ã€ç™¼æ–‡æ™‚æ®µã€æ˜ŸæœŸå¹¾ç­‰é¡åˆ¥æ¬„ä½ï¼‰\n",
    "# ä½¿ç”¨ StandardScaler å°‡æ•¸å€¼æ¬„ä½è½‰æ›ç‚ºã€Œæ¨™æº–å¸¸æ…‹åˆ†å¸ƒã€ï¼ˆmean=0, std=1ï¼‰ï¼Œæœ‰åŠ©æ–¼æ¨¡å‹å­¸ç¿’ç©©å®šã€‚\n",
    "onehot_cols = [col for col in df.columns if col.startswith('lang_') or col.startswith('post_period_') or col.startswith('post_weekday_')]\n",
    "num_cols = base_num_cols + onehot_cols\n",
    "scaler = StandardScaler()\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "# Tokenizer æ–‡æœ¬ç·¨ç¢¼ ï¼šä½¿ç”¨äº‹å…ˆå®šç¾©å¥½çš„ tokenizerï¼ˆä¾‹å¦‚ MacBERTã€RoBERTaï¼‰å°è²¼æ–‡é€²è¡Œæ–·è©ã€ç·¨ç¢¼\n",
    "# å°‡ç·¨ç¢¼å¾Œçš„çµæœå„²å­˜åˆ° df ä¸­ï¼Œé€™å…©å€‹æ¬„ä½æœƒä½œç‚º BERT æ¨¡å‹çš„è¼¸å…¥\n",
    "encodings = tokenizer(df['content'].tolist(), truncation=True, padding='max_length', max_length=128)\n",
    "df['input_ids'] = encodings['input_ids']  # æ–·è©å¾Œå°æ‡‰çš„è©å½™ ID\n",
    "df['attention_mask'] = encodings['attention_mask']  # å°æ‡‰ä½ç½®æ˜¯å¦æ˜¯ paddingï¼ˆ0ï¼‰æˆ–å¯¦éš›å…§å®¹ï¼ˆ1ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "j32CLLcICUzu"
   },
   "outputs": [],
   "source": [
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df['input_ids'].tolist()\n",
    "        self.attention_mask = df['attention_mask'].tolist()\n",
    "        self.numerics = df[num_cols].values\n",
    "        self.targets = df['target'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'numerics': torch.tensor(self.numerics[idx], dtype=torch.float),\n",
    "            'target': torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsA6q_qf9ojJ"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "4OBPv79a10W0"
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df['input_ids'].tolist()\n",
    "        self.attention_mask = df['attention_mask'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.numerics = df[num_cols].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'numerics': torch.tensor(self.numerics[idx], dtype=torch.float),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_iVSfG82Gpt"
   },
   "source": [
    "#æ¨¡å‹æ¶æ§‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "um-YKfVt10Uf"
   },
   "outputs": [],
   "source": [
    "#æ¨¡å‹æ¶æ§‹\n",
    "# 1. FusionMacBERTï¼šBERT + æ•¸å€¼ç‰¹å¾µ concat\n",
    "class FusionMacBERTModel(nn.Module):\n",
    "    def __init__(self, model_name, num_numeric_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size + 64, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        cls_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        num_out = torch.relu(self.numeric_fc(numerics))\n",
    "        combined = torch.cat((cls_output, num_out), dim=1)\n",
    "        return self.classifier(self.dropout(combined))\n",
    "\n",
    "# 2. PureMacBERTï¼šåªæœ‰æ–‡å­—\n",
    "class PureMacBERTModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics=None):\n",
    "        cls_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        return self.classifier(self.dropout(cls_output))\n",
    "\n",
    "# 3. NumericOnlyï¼šåªæœ‰æ•¸å€¼ç‰¹å¾µ\n",
    "class NumericOnlyModel(nn.Module):\n",
    "    def __init__(self, num_numeric_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_numeric_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, numerics=None):\n",
    "        return self.classifier(numerics)\n",
    "\n",
    "# 4. BiLSTMWithNumericï¼šLSTM è™•ç†è©åµŒå…¥ + æ•¸å€¼ç‰¹å¾µ\n",
    "class BiLSTMWithNumeric(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_numeric_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2 + 64, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        x = self.embedding(input_ids)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        pooled = lstm_out[:, -1, :]\n",
    "        num_out = torch.relu(self.numeric_fc(numerics))\n",
    "        combined = torch.cat((pooled, num_out), dim=1)\n",
    "        return self.classifier(self.dropout(combined))\n",
    "\n",
    "# 5. MacBERTWithGRUï¼šBERT + GRU + æ•¸å€¼ç‰¹å¾µ\n",
    "class MacBERTWithGRU(nn.Module):\n",
    "    def __init__(self, model_name, num_numeric_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.gru = nn.GRU(self.bert.config.hidden_size, 128, batch_first=True, bidirectional=True)\n",
    "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
    "        self.classifier = nn.Linear(128*2 + 64, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        gru_out, _ = self.gru(bert_out)\n",
    "        pooled = gru_out[:, -1, :]\n",
    "        num_out = torch.relu(self.numeric_fc(numerics))\n",
    "        combined = torch.cat((pooled, num_out), dim=1)\n",
    "        return self.classifier(self.dropout(combined))\n",
    "\n",
    "# 6. MacBERTMLPFusionï¼šBERT + æ•¸å€¼ç‰¹å¾µ -> MLP\n",
    "class MacBERTMLPFusion(nn.Module):\n",
    "    def __init__(self, model_name, num_numeric_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size + num_numeric_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        cls_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        combined = torch.cat((cls_output, numerics), dim=1)\n",
    "        return self.fc(combined)\n",
    "\n",
    "# 7. TextCNNMacBERTï¼šBERT è¼¸å‡ºå·ç©å¾Œ + æ•¸å€¼ç‰¹å¾µ\n",
    "class TextCNNMacBERT(nn.Module):\n",
    "    def __init__(self, model_name, num_numeric_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, 64, (k, self.bert.config.hidden_size)) for k in [2, 3, 4]])\n",
    "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
    "        self.classifier = nn.Linear(64 * len([2, 3, 4]) + 64, num_classes)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = torch.relu(conv(x)).squeeze(3)\n",
    "        x = torch.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        x = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state.unsqueeze(1)\n",
    "        cnn_out = torch.cat([self.conv_and_pool(x, conv) for conv in self.convs], 1)\n",
    "        num_out = torch.relu(self.numeric_fc(numerics))\n",
    "        combined = torch.cat((cnn_out, num_out), dim=1)\n",
    "        return self.classifier(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "De3g_-Qs2Vux"
   },
   "outputs": [],
   "source": [
    "#è¨“ç·´èˆ‡è©•ä¼°\n",
    "def train_and_eval(model, name, preview_count=10):\n",
    "    model = model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    loss_fn = FocalLoss()\n",
    "    # loss_fn = nn.MSELoss()\n",
    "\n",
    "    # è¨“ç·´éšæ®µ\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            output = model(input_ids, attention_mask, numerics)\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # è©•ä¼°éšæ®µ\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            output = model(input_ids, attention_mask, numerics)\n",
    "            preds = output.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    mae = mean_absolute_error(all_targets, all_preds)\n",
    "    print(f\"[{name} è©•ä¼°çµæœ] MSE: {mse:.2f} | MAE: {mae:.2f}\")\n",
    "    print(f\"\\n{name} è©•ä¼°çµæœï¼š\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "    '''\n",
    "    preview_shown = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            output = model(input_ids, attention_mask, numerics)\n",
    "            preds = output.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            #å°å‡ºå‰å¹¾ç­†çš„é æ¸¬ã€çœŸå¯¦å€¼\n",
    "            if preview_shown < preview_count:\n",
    "                batch_size = input_ids.shape[0]\n",
    "                for i in range(batch_size):\n",
    "                    if preview_shown >= preview_count:\n",
    "                        break\n",
    "                    input_id = input_ids[i].cpu().numpy()\n",
    "                    text = tokenizer.decode(input_id, skip_special_tokens=True)\n",
    "                    print(f\"\\n[{name} é æ¸¬] ç¬¬ {preview_shown+1} ç­†\")\n",
    "                    print(f\"Text: {text}\")\n",
    "                    print(f\"Predicted: {label_encoder.inverse_transform([preds[i]])[0]}\")\n",
    "                    print(f\"Actual:    {label_encoder.inverse_transform([labels[i].cpu().item()])[0]}\")\n",
    "                    preview_shown += 1\n",
    "\n",
    "    print(f\"\\n{name} è©•ä¼°çµæœï¼š\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wqejxS910SJ"
   },
   "outputs": [],
   "source": [
    "# è³‡æ–™åˆ†å‰²ï¼šè³‡æ–™é›†åˆ‡åˆ†èˆ‡å–æ¨£\n",
    "dataset = CustomDataset(df)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, len(dataset)-train_size])\n",
    "train_labels = [train_dataset[i]['labels'].item() for i in range(len(train_dataset))]\n",
    "class_counts = pd.Series(train_labels).value_counts().to_dict()\n",
    "weights = [1.0 / class_counts[label] for label in train_labels]\n",
    "sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, sampler=sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# åŸ·è¡Œå¤šæ¨¡å‹è¨“ç·´\n",
    "model_variants = {\n",
    "    # \"FusionMacRegressor\": FusionMacBERTRegressor(\"hfl/chinese-macbert-base\", len(num_cols), 3),\n",
    "    \"FusionMacBERT\": FusionMacBERTModel(\"hfl/chinese-macbert-base\", len(num_cols), 4),\n",
    "    \"PureMacBERT\": PureMacBERTModel(\"hfl/chinese-macbert-base\", 4),\n",
    "    \"NumericOnly\": NumericOnlyModel(len(num_cols), 4),\n",
    "    \"BiLSTMWithNumeric\": BiLSTMWithNumeric(tokenizer.vocab_size, 128, 128, len(num_cols), 4),\n",
    "    \"MacBERTWithGRU\": MacBERTWithGRU(\"hfl/chinese-macbert-base\", len(num_cols), 4),\n",
    "    \"MacBERTMLPFusion\": MacBERTMLPFusion(\"hfl/chinese-macbert-base\", len(num_cols), 4),\n",
    "    \"TextCNNMacBERT\": TextCNNMacBERT(\"hfl/chinese-macbert-base\", len(num_cols), 4),\n",
    "    \"RoBERTa\": FusionMacBERTModel(\"hfl/chinese-roberta-wwm-ext\", len(num_cols), 4),\n",
    "    \"BERTwwmExt\": FusionMacBERTModel(\"hfl/chinese-bert-wwm-ext\", len(num_cols), 4),\n",
    "    \"ERNIE\": FusionMacBERTModel(\"nghuyong/ernie-3.0-base-zh\", len(num_cols), 4),\n",
    "    \"ConvBERT\": FusionMacBERTModel(\"YituTech/conv-bert-base\", len(num_cols), 4)\n",
    "}\n",
    "\n",
    "# é€å€‹æ¨¡å‹è¨“ç·´èˆ‡è¼¸å‡ºçµæœ\n",
    "for name, model in model_variants.items():\n",
    "    tokenizer_name = model_tokenizer_map.get(name, default_tokenizer_name)\n",
    "    if tokenizer_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        encodings = tokenizer(df['content'].tolist(), truncation=True, padding='max_length', max_length=128)\n",
    "        df['input_ids'] = encodings['input_ids']\n",
    "        df['attention_mask'] = encodings['attention_mask']\n",
    "    train_and_eval(model, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WP2MxFtLUhR"
   },
   "source": [
    "1. FusionMacBERT âœ…æ–‡å­— + âœ…æ•¸å€¼\n",
    "BERTï¼š ä½¿ç”¨ MacBERT\n",
    "æ¶æ§‹ï¼š æŠŠ [CLS] å‘é‡èˆ‡æ•¸å€¼ç‰¹å¾µç¶“é MLP èåˆ\n",
    "ç”¨é€”ï¼š åšç‚º baseline èåˆæ¨¡å‹\n",
    "å„ªé»ï¼š åŒæ™‚è€ƒæ…®å…§å®¹èªç¾©èˆ‡è²¼æ–‡çµ±è¨ˆè³‡æ–™ï¼ˆå¦‚æŒ‰è®šæ•¸ã€æ˜¯å¦æœ‰ hashtagï¼‰\n",
    "\n",
    "2. PureMacBERT âœ…æ–‡å­— + âŒæ•¸å€¼\n",
    "BERTï¼š ä½¿ç”¨ MacBERT\n",
    "æ¶æ§‹ï¼š å–®ç´”ä½¿ç”¨ [CLS]ï¼Œå¾Œæ¥ linear å±¤åˆ†é¡\n",
    "ç”¨é€”ï¼š ç´”èªè¨€æ¨¡å‹ baseline\n",
    "å°ç…§ï¼š å¯ç”¨ä¾†æ¯”è¼ƒæ˜¯å¦æœ‰æ•¸å€¼è¼”åŠ©æå‡æ•ˆæœ\n",
    "\n",
    "3. NumericOnly âŒæ–‡å­— + âœ…æ•¸å€¼\n",
    "æ¨¡å‹é¡å‹ï¼š åªæœ‰æ•¸å€¼è¼¸å…¥ï¼Œç¶“é MLP åšåˆ†é¡\n",
    "ç”¨é€”ï¼š æ¸¬è©¦ã€Œåªé è²¼æ–‡çµ±è¨ˆè³‡æ–™ã€èƒ½å¦é”åˆ°åˆç†åˆ†é¡\n",
    "å°ç…§ï¼š å¯èˆ‡æ–‡å­—æ¨¡å‹æˆ–èåˆæ¨¡å‹å°æ¯”æ•ˆæœ\n",
    "\n",
    "4. BiLSTMWithNumeric âœ…æ–‡å­—ï¼ˆEmbedding+LSTMï¼‰+ âœ…æ•¸å€¼\n",
    "åµŒå…¥æ–¹å¼ï¼š ä½¿ç”¨ nn.Embedding + BiLSTM è™•ç†æ–‡å­—ï¼ˆä¸æ˜¯ BERTï¼‰\n",
    "èåˆæ–¹å¼ï¼š å°‡ LSTM æœ€å¾Œæ™‚é–“æ­¥ + æ•¸å€¼ç‰¹å¾µæ‹¼æ¥\n",
    "ç‰¹åˆ¥é»ï¼š æ¸¬è©¦ã€Œé Transformer æ¨¡å‹ã€æ˜¯å¦ä»å…·ç«¶çˆ­åŠ›\n",
    "\n",
    "5. MacBERTWithGRU âœ…æ–‡å­—ï¼ˆMacBERTï¼‰+ âœ…æ•¸å€¼\n",
    "æ–‡å­—è™•ç†ï¼š MacBERT ä¹‹å¾Œå†ä¸² GRU\n",
    "èåˆæ–¹å¼ï¼š GRU è¼¸å‡ºæœ€å¾Œä¸€æ­¥æ‹¼æ¥æ•¸å€¼ç‰¹å¾µ\n",
    "æ„åœ–ï¼š æƒ³çœ‹ BERT+RNN çš„è¡¨ç¾ vs. å‚³çµ± BERT\n",
    "\n",
    "6. MacBERTMLPFusion âœ…æ–‡å­— + âœ…æ•¸å€¼\n",
    "è™•ç†æ–¹å¼ï¼š æ–‡å­—èˆ‡æ•¸å€¼ç›´æ¥æ‹¼æ¥å¾Œé€²å…¥ MLP\n",
    "ä¸åŒæ–¼ FusionMacBERTï¼š\n",
    "æ²’æœ‰é¡å¤–è™•ç†æ•¸å€¼ç‰¹å¾µï¼ˆå¦‚æ²’æœ‰ç¶“é nn.Linear)\n",
    "æ›´å–®ç´”çš„èåˆè¨­è¨ˆï¼ˆå±¬æ–¼ Early Fusionï¼‰\n",
    "\n",
    "7. TextCNNMacBERT âœ…æ–‡å­— + âœ…æ•¸å€¼\n",
    "æ¨¡å‹çµ„åˆï¼š\n",
    "ä½¿ç”¨ BERT ç·¨ç¢¼å¾Œä¸Ÿé€² CNN filter (TextCNN)\n",
    "å†èˆ‡æ•¸å€¼ç‰¹å¾µèåˆ\n",
    "ç”¨é€”ï¼š æ¸¬è©¦ BERT çµåˆ CNN ç‰¹å¾µæå–æ˜¯å¦æå‡æ•ˆæœ\n",
    "æœ‰è¶£é»ï¼š æœ‰äº›çŸ­æ–‡æ¨¡å‹ï¼ˆå¦‚å¾®åšã€Threadsï¼‰å° CNN ç‰¹å¾µæŠ“å–æ•æ„Ÿ\n",
    "\n",
    "8. RoBERTa âœ…æ–‡å­— + âœ…æ•¸å€¼\n",
    "BERT æ›¿ä»£å“ï¼š æ”¹ç”¨ RoBERTaï¼ˆä¸­æ–‡ç‰ˆæœ¬ï¼‰\n",
    "èåˆæ–¹å¼ï¼š åŒ FusionMacBERT\n",
    "å¯¦é©—ç›®çš„ï¼š æ¸¬è©¦ä¸åŒèªè¨€æ¨¡å‹å°çµæœçš„å½±éŸ¿ï¼ˆèªè¨€æ¨¡å‹ ablationï¼‰\n",
    "\n",
    "\n",
    "9. BERTwwmExt âœ…æ–‡å­— + âœ…æ•¸å€¼\n",
    "BERTï¼š ä½¿ç”¨ Chinese BERT whole-word-masking æ“´å±•ç‰ˆ\n",
    "æ¯”è¼ƒç›®çš„ï¼š åŒä¸Šï¼Œç”¨æ–¼æ¸¬è©¦ä¸åŒèªè¨€æ¨¡å‹ç‰¹æ€§çš„å½±éŸ¿\n",
    "\n",
    "10. ERNIE âœ…æ–‡å­— + âœ…æ•¸å€¼\n",
    "BERTï¼š æ”¹ç”¨ç™¾åº¦çš„ ERNIEï¼ˆå¼•å…¥çŸ¥è­˜å¢å¼·ï¼‰\n",
    "é©ç”¨å ´æ™¯ï¼š ç•¶æ–‡æœ¬èˆ‡å¸¸è­˜æœ‰é—œï¼ˆå¦‚è©±é¡Œã€ç”¨èªï¼‰\n",
    "ç›®çš„ï¼š è©•ä¼°çŸ¥è­˜å‹èªè¨€æ¨¡å‹åœ¨ç¤¾ç¾¤æ–‡æœ¬åˆ†é¡çš„æ•ˆæœ\n",
    "\n",
    "11. ConvBERT âœ…æ–‡å­— + âœ…æ•¸å€¼\n",
    "æ¨¡å‹ç‰¹è‰²ï¼š ä½¿ç”¨ Convolution + Self-Attention æ··åˆæ¶æ§‹çš„ BERT\n",
    "å¯¦é©—æ„ç¾©ï¼š è©¦é©—éå‚³çµ± Self-Attention æ¨¡å‹æ˜¯å¦æœ‰å„ªå‹¢\n",
    "\n",
    "\n",
    "| æ¨¡å‹åç¨±              | èªªæ˜               | æ˜¯å¦èåˆ | æ–‡æœ¬è™•ç†æ³•         | ç‰¹æ®Šè™•ç†       |\n",
    "| ----------------- | ---------------- | ---- | ------------- | ---------- |\n",
    "| FusionMacBERT     | BERT + æ•¸å€¼ç‰¹å¾µ      | âœ…    | MacBERT       | è‡ªè£½èåˆå±¤      |\n",
    "| PureMacBERT       | ç´”æ–‡æœ¬æ¨¡å‹            | âŒ    | MacBERT       | baseline   |\n",
    "| NumericOnly       | ç´”çµ±è¨ˆæ•¸å€¼            | âŒ    | ç„¡             | MLP only   |\n",
    "| BiLSTMWithNumeric | LSTM + æ•¸å€¼        | âœ…    | nn.Embedding  | ä¸ä½¿ç”¨ BERT   |\n",
    "| MacBERTWithGRU    | BERT + GRU + æ•¸å€¼  | âœ…    | MacBERT + GRU | æ™‚åºç‰¹å¾µå¼·åŒ–     |\n",
    "| MacBERTMLPFusion  | BERT + æ•¸å€¼        | âœ…    | MacBERT       | æ‹¼æ¥å¾Œé€² MLP   |\n",
    "| TextCNNMacBERT    | BERT + CNN + æ•¸å€¼  | âœ…    | MacBERT + CNN | æ¨¡ä»¿ TextCNN |\n",
    "| RoBERTa           | æ› BERT backbone  | âœ…    | RoBERTa       | æ¨¡å‹æ¯”è¼ƒ       |\n",
    "| BERTwwmExt        | æ› BERT backbone  | âœ…    | BERT-wwm      | æ¨¡å‹æ¯”è¼ƒ       |\n",
    "| ERNIE             | å¼•å…¥çŸ¥è­˜çš„ BERT       | âœ…    | ERNIE         | æ¨¡å‹æ¯”è¼ƒ       |\n",
    "| ConvBERT          | æ··åˆå·ç© + æ³¨æ„åŠ›çš„ BERT | âœ…    | ConvBERT      | æ¨¡å‹æ¯”è¼ƒ       |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapley value analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_numeric_shap(model, df, num_cols, sample_count=200, background_count=100):\n",
    "    model.eval().cpu()\n",
    "    X = df[num_cols].values.astype(np.float32)\n",
    "\n",
    "    def model_forward(x_numpy):\n",
    "        x_tensor = torch.tensor(x_numpy, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(numerics=x_tensor)\n",
    "            return outputs.numpy()\n",
    "\n",
    "    explainer = shap.Explainer(model_forward, shap.sample(X, background_count))\n",
    "    shap_values = explainer(X[:sample_count])\n",
    "\n",
    "    print(\"ğŸ” ç‰¹å¾µå¹³å‡è²¢ç»å€¼ (bar chart):\")\n",
    "    shap.plots.bar(shap_values)\n",
    "\n",
    "    print(\"ğŸ ç‰¹å¾µå½±éŸ¿åˆ†å¸ƒ (beeswarm plot):\")\n",
    "    shap.plots.beeswarm(shap_values)\n",
    "\n",
    "    return shap_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericBranchWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.numeric_fc = model.numeric_fc\n",
    "        self.classifier = model.classifier\n",
    "        self.hidden_size = model.bert.config.hidden_size\n",
    "        self.dummy_bert = torch.zeros((1, self.hidden_size))  # å‡è£ CLS å‘é‡æ˜¯ 0\n",
    "\n",
    "    def forward(self, numerics):\n",
    "        num_out = torch.relu(self.numeric_fc(numerics))\n",
    "        dummy_bert = self.dummy_bert.expand(numerics.size(0), -1)\n",
    "        combined = torch.cat((dummy_bert, num_out), dim=1)\n",
    "        return self.classifier(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… æŒ‡å®šæ”¯æ´ SHAP çš„æ¨¡å‹åç¨±æ¸…å–®\n",
    "shap_supported_models = [\"NumericOnly\", \"FusionMacBERT\", \"MacBERTMLPFusion\"]\n",
    "\n",
    "for name, model in model_variants.items():\n",
    "    if name in shap_supported_models:\n",
    "        print(f\"\\nğŸ“Š åˆ†ææ¨¡å‹ï¼š{name}\")\n",
    "\n",
    "        # å¦‚æœæ˜¯éœ€è¦åŒ…è£çš„ MacBERT èåˆé¡å‹\n",
    "        if name in [\"FusionMacBERT\", \"MacBERTMLPFusion\"]:\n",
    "            wrapper_model = NumericBranchWrapper(model)\n",
    "            shap_vals = analyze_numeric_shap(wrapper_model, df, num_cols)\n",
    "        else:\n",
    "            # å¦å‰‡å°±æ˜¯ NumericOnly å¯ä»¥ç›´æ¥ç”¨\n",
    "            shap_vals = analyze_numeric_shap(model, df, num_cols)\n",
    "\n",
    "        # âœ…ï¼ˆé¸é…ï¼‰å„²å­˜åœ–è¡¨ï¼šå¦‚éœ€å­˜åœ–æª”å¯è§£é™¤è¨»è§£\n",
    "        \"\"\"\n",
    "        plt.figure()\n",
    "        shap.plots.bar(shap_vals, show=False)\n",
    "        plt.savefig(f\"shap_bar_{name}.png\", bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        shap.plots.beeswarm(shap_vals, show=False)\n",
    "        plt.savefig(f\"shap_beeswarm_{name}.png\", bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sd8HLHR6VL1"
   },
   "source": [
    "# è¿´æ­¸é æ¸¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "291iyZhHxpj9"
   },
   "outputs": [],
   "source": [
    "# åŸæœ¬é€™æ¨£åˆ†é¡ï¼ˆè¦æ‹¿æ‰ï¼‰\n",
    "# df['view_class'] = ...\n",
    "# df['label'] = ...\n",
    "\n",
    "# ç›´æ¥ç”¨åŸå§‹ view_count ä½œç‚º regression target\n",
    "df = df.dropna(subset=[\"content\", \"view_count\"])\n",
    "df['target'] = df['view_count'].apply(parse_count)  # å¦‚æœ view_count ä¸æ˜¯æ•¸å­—è¦å…ˆè½‰æ›\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "dedliSNv7UGU"
   },
   "outputs": [],
   "source": [
    "def train_and_eval_regression(model, name):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # è¨“ç·´\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            targets = batch['labels'].float().to(device)  # é‡è¦ï¼šlabels å¿…é ˆæ˜¯ float\n",
    "            preds = model(input_ids, attention_mask, numerics)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # è©•ä¼°\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            targets = batch['labels'].float().to(device)\n",
    "            preds = model(input_ids, attention_mask, numerics)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    mae = mean_absolute_error(all_targets, all_preds)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "    print(f\"\\n{name}  MSE: {mse:.2f} | MAE: {mae:.2f}| R2: {r2:.2f}\")\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "    return all_targets, all_preds\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5c7CQi1v6wwI"
   },
   "outputs": [],
   "source": [
    "class FusionMacBERTRegressor(nn.Module):\n",
    "    def __init__(self, model_name, num_numeric_features, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
    "        self.regressor = nn.Linear(self.bert.config.hidden_size + 64, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        cls_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        num_out = torch.relu(self.numeric_fc(numerics))\n",
    "        combined = torch.cat((cls_output, num_out), dim=1)\n",
    "        return self.regressor(self.dropout(combined)).squeeze(1)  # (batch,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kTncOradCUz8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "rm -rf ~/.cache/huggingface/transformers/hfl__chinese-macbert-base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ptY9GnoACUz8"
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "'''\n",
    "class CustomDatasetRegression(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df['input_ids'].tolist()\n",
    "        self.attention_mask = df['attention_mask'].tolist()\n",
    "        self.labels = df['label'].astype(float).values   # â† ç‚ºå›æ­¸ä»»å‹™éœ€è½‰æˆ float\n",
    "        self.numerics = df[num_cols].astype(float).values  # â† ç¢ºä¿ç‚º float array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'numerics': torch.tensor(self.numerics[idx], dtype=torch.float),  # â† ä¿®æ­£ç‚º float\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.float)       # â† ä¿®æ­£ç‚º float\n",
    "        }\n",
    "'''\n",
    "class CustomDatasetRegression(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df['input_ids'].tolist()\n",
    "        self.attention_mask = df['attention_mask'].tolist()\n",
    "        self.labels = df['target'].astype(float).values\n",
    "        self.numerics = df[num_cols].astype(float).values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'numerics': torch.tensor(self.numerics[idx], dtype=torch.float),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Rlezwnry2K0h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FusionMacBERTRegressor  MSE: 4410895810.43 | MAE: 10676.62 | RÂ²: -0.03\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = CustomDatasetRegression(df)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "\n",
    "class FusionMacBERTRegressor(nn.Module):\n",
    "    def __init__(self, model_name, num_numeric_features, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
    "        self.regressor = nn.Linear(self.bert.config.hidden_size + 64, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        cls_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        num_out = torch.relu(self.numeric_fc(numerics))\n",
    "        combined = torch.cat((cls_output, num_out), dim=1)\n",
    "        return self.regressor(self.dropout(combined)).squeeze(1)\n",
    "\n",
    "\n",
    "def train_and_eval_regression(model, name):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            targets = batch['labels'].float().to(device)\n",
    "\n",
    "            preds = model(input_ids, attention_mask, numerics)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            targets = batch['labels'].float().to(device)\n",
    "\n",
    "            preds = model(input_ids, attention_mask, numerics)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    mae = mean_absolute_error(all_targets, all_preds)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f\"\\n{name}  MSE: {mse:.2f} | MAE: {mae:.2f} | RÂ²: {r2:.2f}\")\n",
    "    return all_targets, all_preds\n",
    "\n",
    "\n",
    "model = FusionMacBERTRegressor(\"hfl/chinese-macbert-base\", len(num_cols))\n",
    "all_targets, all_preds = train_and_eval_regression(model, \"FusionMacBERTRegressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmUJQfroFBqt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEyKqKjE9oZy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OeqnHopFJuT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0DyH1z7FJm3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnoabF2fFJgR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaucnuMEFCV-"
   },
   "source": [
    "#**ä¸‹é¢éƒ½æ˜¯èˆŠçš„æ±è¥¿è€Œå·²~~~~**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvuTn85bUTA5"
   },
   "outputs": [],
   "source": [
    "stopwords = set(['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'ä¹Ÿ', 'èˆ‡', 'æœ‰', 'ç‚º', 'ç­‰'])\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    words = jieba.cut(text)\n",
    "    words_filtered = [word for word in words if word not in stopwords]\n",
    "    return ' '.join(words_filtered)\n",
    "\n",
    "df['processed_content'] = df['Content'].apply(tokenize_and_remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nzhynZN7IYy"
   },
   "source": [
    "new_article = \"IC è¨­è¨ˆå¤§å» è¯ç™¼ç§‘ (2454-TW) å‰¯è‘£äº‹é•·æš¨åŸ·è¡Œé•·è”¡åŠ›è¡Œä»Š (26) æ—¥ç²é ’æ½˜æ–‡æ·µçï¼Œæœƒå¾Œå—è¨ªè¡¨ç¤ºï¼Œè¯ç™¼ç§‘ 3 å¥ˆç±³æœƒåœ¨å°ç©é›» (2330-TW)(TSM-US) åšï¼Œä¸”ç”±æ–¼å…ˆé€²è£½ç¨‹æŠ€è¡“ç›¸ç•¶è¤‡é›œï¼Œä¸è«–è¦æ¡ç”¨æˆ–æ›´æ›éƒ½éå¸¸å›°é›£ï¼Œé›™æ–¹æœƒæŒçºŒç·Šå¯†åˆä½œã€‚å¤–ç•Œä»Šæ—¥æå•ä¸è«–æ˜¯è¼é” (NVDA-US)ã€è˜‹æœ (AAPL-US) ç­‰éƒ½è¡¨ç¤ºå°‹æ±‚å¤šå…ƒçš„æ™¶åœ“ä»£å·¥æ–¹æ¡ˆï¼Œè”¡åŠ›è¡Œå›æ‡‰ï¼Œè¯ç™¼ç§‘åœ¨å…ˆé€²è£½ç¨‹æŒçºŒèˆ‡å°ç©é›»ç·Šå¯†åˆä½œï¼Œè‹±ç‰¹çˆ¾ (INTC-US) å‰‡è² è²¬ 16 å¥ˆç±³è”¡åŠ›è¡Œä¹Ÿå¼·èª¿ï¼Œè¯ç™¼ç§‘ä¸æœƒåªåœåœ¨æ¡ç”¨ 4 å¥ˆç±³ï¼Œä¹Ÿæœƒæ¡ç”¨ 3 å¥ˆç±³è£½ç¨‹ï¼Œæ­¤å¤–ï¼Œç”±æ–¼é›»æ™¶é«”å¾®ç¸®é€Ÿåº¦è¶¨ç·©ï¼Œå„˜ç®¡æŠ€è¡“ä¸Šå¯è¡Œï¼Œä½†ä¸ä¸€å®šç¬¦åˆç¶“æ¿Ÿæ•ˆç›Šï¼Œå› æ­¤æŠ€è¡“ä¹Ÿé€æ­¥å¾å¹³é¢è®Šæˆ 2Dã€2.5Dï¼Œç”šè‡³ 3D ç­‰ï¼Œå…ˆé€²å°è£çš„é‡è¦æ€§æ¯”ä»¥å‰å¢åŠ ã€‚è‡³æ–¼è·Ÿè¼é”åˆä½œï¼Œè”¡åŠ›è¡Œé‡ç”³ï¼Œé›™æ–¹åˆä½œä»ä»¥æ±½è»Šç‚ºä¸»ï¼Œè¼é”å¸ƒå±€è»Šç”¨æ¯”è¯ç™¼ç§‘æ—©ï¼Œä¸»è¦è‘—å¢¨åœ¨æ™ºæ…§åº§è‰™èˆ‡ ADAS ç³»çµ±ï¼Œé›™æ–¹æœ‰å¾ˆå¥½çš„é…åˆï¼Œå…¶ä¸­ï¼Œè¼é”ä¸»æ”»é«˜éšã€è¯ç™¼ç§‘å‰‡ç„æº–ä¸­éšï¼Œé›™æ–¹æ­£å¯†åˆ‡åˆä½œé–‹æœƒã€‚\"\n",
    "processed_new_article = tokenize_and_remove_stopwords(new_article)\n",
    "\n",
    "# Random Forest æ¨¡å‹è¨“ç·´èˆ‡é æ¸¬\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ä½¿ç”¨ç›¸åŒçš„æ•¸æ“šåˆ†å‰²æ–¹å¼\n",
    "X_train_rf, X_val_rf, y_train_rf, y_val_rf = train_test_split(tfidf_matrix, y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train_rf, X_val_rf, y_train_rf, y_val_rf = train_test_split(X_train_rf, y_train_rf, test_size=0.1, random_state=42)\n",
    "\n",
    "# å‰µå»ºéš¨æ©Ÿæ£®æ—æ¨¡å‹\n",
    "rand_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# è¨“ç·´éš¨æ©Ÿæ£®æ—æ¨¡å‹\n",
    "rand_forest_model.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "# é æ¸¬\n",
    "y_val_pred_rf = rand_forest_model.predict(X_val_rf)\n",
    "y_test_pred_rf = rand_forest_model.predict(X_test)\n",
    "\n",
    "# åˆ†é¡å ±å‘Š\n",
    "print(\"é©—è­‰é›† Validation Classification Report:\")\n",
    "print(classification_report(y_val_rf, y_val_pred_rf))\n",
    "\n",
    "print(\"\\næ¸¬è©¦é›† Test Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_rf))\n",
    "\n",
    "# å°ç…§è¡¨\n",
    "result_df_val_rf = pd.DataFrame({'Actual': y_val_rf, 'Predicted': y_val_pred_rf})\n",
    "result_df_test_rf = pd.DataFrame({'Actual': y_test, 'Predicted': y_test_pred_rf})\n",
    "\n",
    "print(\"é©—è­‰é›† Validation Result Comparison:\")\n",
    "print(result_df_val_rf)\n",
    "\n",
    "print(\"\\næ¸¬è©¦é›† Test Result Comparison:\")\n",
    "print(result_df_test_rf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTn9buro9xlD"
   },
   "outputs": [],
   "source": [
    "new_article = \"IC è¨­è¨ˆå¤§å» è¯ç™¼ç§‘ (2454-TW) å‰¯è‘£äº‹é•·æš¨åŸ·è¡Œé•·è”¡åŠ›è¡Œä»Š (26) æ—¥ç²é ’æ½˜æ–‡æ·µçï¼Œæœƒå¾Œå—è¨ªè¡¨ç¤ºï¼Œè¯ç™¼ç§‘ 3 å¥ˆç±³æœƒåœ¨å°ç©é›» (2330-TW)(TSM-US) åšï¼Œä¸”ç”±æ–¼å…ˆé€²è£½ç¨‹æŠ€è¡“ç›¸ç•¶è¤‡é›œï¼Œä¸è«–è¦æ¡ç”¨æˆ–æ›´æ›éƒ½éå¸¸å›°é›£ï¼Œé›™æ–¹æœƒæŒçºŒç·Šå¯†åˆä½œã€‚å¤–ç•Œä»Šæ—¥æå•ä¸è«–æ˜¯è¼é” (NVDA-US)ã€è˜‹æœ (AAPL-US) ç­‰éƒ½è¡¨ç¤ºå°‹æ±‚å¤šå…ƒçš„æ™¶åœ“ä»£å·¥æ–¹æ¡ˆï¼Œè”¡åŠ›è¡Œå›æ‡‰ï¼Œè¯ç™¼ç§‘åœ¨å…ˆé€²è£½ç¨‹æŒçºŒèˆ‡å°ç©é›»ç·Šå¯†åˆä½œï¼Œè‹±ç‰¹çˆ¾ (INTC-US) å‰‡è² è²¬ 16 å¥ˆç±³è”¡åŠ›è¡Œä¹Ÿå¼·èª¿ï¼Œè¯ç™¼ç§‘ä¸æœƒåªåœåœ¨æ¡ç”¨ 4 å¥ˆç±³ï¼Œä¹Ÿæœƒæ¡ç”¨ 3 å¥ˆç±³è£½ç¨‹ï¼Œæ­¤å¤–ï¼Œç”±æ–¼é›»æ™¶é«”å¾®ç¸®é€Ÿåº¦è¶¨ç·©ï¼Œå„˜ç®¡æŠ€è¡“ä¸Šå¯è¡Œï¼Œä½†ä¸ä¸€å®šç¬¦åˆç¶“æ¿Ÿæ•ˆç›Šï¼Œå› æ­¤æŠ€è¡“ä¹Ÿé€æ­¥å¾å¹³é¢è®Šæˆ 2Dã€2.5Dï¼Œç”šè‡³ 3D ç­‰ï¼Œå…ˆé€²å°è£çš„é‡è¦æ€§æ¯”ä»¥å‰å¢åŠ ã€‚è‡³æ–¼è·Ÿè¼é”åˆä½œï¼Œè”¡åŠ›è¡Œé‡ç”³ï¼Œé›™æ–¹åˆä½œä»ä»¥æ±½è»Šç‚ºä¸»ï¼Œè¼é”å¸ƒå±€è»Šç”¨æ¯”è¯ç™¼ç§‘æ—©ï¼Œä¸»è¦è‘—å¢¨åœ¨æ™ºæ…§åº§è‰™èˆ‡ ADAS ç³»çµ±ï¼Œé›™æ–¹æœ‰å¾ˆå¥½çš„é…åˆï¼Œå…¶ä¸­ï¼Œè¼é”ä¸»æ”»é«˜éšã€è¯ç™¼ç§‘å‰‡ç„æº–ä¸­éšï¼Œé›™æ–¹æ­£å¯†åˆ‡åˆä½œé–‹æœƒã€‚\"\n",
    "\n",
    "processed_new_article = tokenize_and_remove_stopwords(new_article)\n",
    "\n",
    "print(processed_new_article)\n",
    "\n",
    "# å°‡æ–°æ–‡ç« è½‰æ›ç‚º TF-IDF è¡¨ç¤ºå½¢å¼\n",
    "new_article_tfidf = tfidf_vectorizer.transform([processed_new_article])\n",
    "\n",
    "# ä½¿ç”¨æŠ•ç¥¨åˆ†é¡å™¨é€²è¡Œé æ¸¬\n",
    "predicted_label_ensemble = voting_classifier.predict(new_article_tfidf)\n",
    "\n",
    "print(f\"æ–°æ–‡ç« é æ¸¬çµæœ: {predicted_label_ensemble}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjsKVnf0-iiU"
   },
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ inverse_transform å°‡é æ¸¬çš„æ•¸å­—ç·¨ç¢¼è½‰æ›å›åŸå§‹æ¨™ç±¤\n",
    "predicted_label_original = label_encoder.inverse_transform(predicted_label_ensemble)\n",
    "\n",
    "print(f\"æ–°æ–‡ç« é æ¸¬çµæœï¼ˆåŸå§‹æ¨™ç±¤ï¼‰: {predicted_label_original}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMJI_upxwRgP"
   },
   "source": [
    "## å¯¦éš›é æ¸¬ï¼ˆç ”ç©¶ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gbdm9r7_qFq"
   },
   "outputs": [],
   "source": [
    "# ç²å–æ‰€æœ‰æ¨™ç±¤å°æ‡‰çš„ç·¨ç¢¼\n",
    "all_labels = label_encoder.classes_\n",
    "\n",
    "print(\"æ‰€æœ‰æ¨™ç±¤å°æ‡‰çš„ç·¨ç¢¼:\")\n",
    "for label_code, label in enumerate(all_labels):\n",
    "    print(f\"ç·¨ç¢¼ {label_code}: æ¨™ç±¤ {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTk9I4ZtEPBP"
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "# å®šç¾©åœç”¨è©\n",
    "stopwords = set(['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'ä¹Ÿ', 'èˆ‡', 'æœ‰', 'ç‚º', 'ç­‰'])\n",
    "\n",
    "# å®šç¾©åˆ†è©ä¸¦å»é™¤åœç”¨è©çš„å‡½æ•¸\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    words = jieba.cut(text)  # ä½¿ç”¨ jieba è¿›è¡Œåˆ†è¯\n",
    "    words_filtered = [word for word in words if word not in stopwords]  # å»é™¤åœç”¨è¯\n",
    "    return ' '.join(words_filtered)\n",
    "\n",
    "# å°‡è™•ç†å¾Œçš„å…§å®¹åŠ å…¥ DataFrame ä¸­\n",
    "df['processed_content'] = df['Content'].apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "# æ–°æ–‡ç« \n",
    "new_article = \"å°è‚¡å®ˆç©©å­£ç·šï¼Œé€±ç·šä¸‰é€£ç´…ã€‚ï¼ˆè³‡æ–™ç…§ï¼‰ ã€”è²¡ç¶“é »é“ï¼ç¶œåˆå ±å°ã€•ç¾åœ‹CPIç•¥é«˜æ–¼å¸‚å ´é æœŸï¼Œç¾è‚¡æ¼²å‹¢æš«æ­‡ï¼Œæœ¬é€±ä»¥ä¾†ï¼Œå°è‚¡ç¶“éå…©æ—¥å¤§æ¼²å¾Œï¼Œä»Šï¼ˆ13ï¼‰æ—¥æŒ‡æ•¸éœ‡ç›ªèµ°ä½ï¼Œçµ‚å ´ä¸‹è·Œ43.34é»ï¼Œä»¥16782.57é»ä½œæ”¶ï¼Œå®ˆä½å­£ç·šé—œå¡ï¼Œæˆäº¤é‡ç‚º2986.08å„„å…ƒï¼Œé€±ç·šä¸Šæ¼²262é»ï¼Œå‘ˆç¾ä¸‰é€£ç´…ï¼Œç·¯å‰µå¤±å®ˆç™¾å…ƒå¤§é—œï¼ŒAIæ—ç¾¤æ™®ééƒ½æ˜¯æ”¶é»‘ï¼Œé›»å­é¡è‚¡ä»¥çŸ½å…‰å­ã€ç¶²é€šç­‰æ¬¡æ—ç¾¤æ¯”è¼ƒæœ‰è¡¨ç¾ï¼Œå‚³ç”¢è¼ªå‹•åˆ°ç‡Ÿå»ºã€é€ ç´™ã€ç™¾è²¨ç­‰æ¥æ£’æ¼”å‡ºã€‚ å‰10å¤§æˆäº¤é¡å€‹è‚¡æ¼²å¤šè·Œå°‘ï¼Œé™¤äº†AIæ—ç¾¤æ”¶é»‘ï¼Œå…¶ä»–éƒ½æ˜¯ç´…ç›¤å±…å¤šï¼Œå»£é”è·Œ12å…ƒï¼Œæ”¶226å…ƒï¼Œæˆäº¤é¡182.32å„„å…ƒï¼Œæ’åç¬¬1ï¼›å°ç©é›»çµ‚å ´æ¼²3å…ƒï¼Œæ”¶553å…ƒï¼Œæˆäº¤é¡171.29å„„å…ƒï¼Œæ’åç¬¬2ï¼›çŸ½çµ±çµ‚å ´æ¼²2.75å…ƒï¼Œæ”¶47.7å…ƒï¼Œæˆäº¤é¡146.25å„„å…ƒï¼Œæ’åç¬¬3ï¼›å®šç©æŠ•æ§æ¼²3.3å…ƒï¼Œæ”¶103å…ƒï¼Œæˆäº¤é¡95.92å„„å…ƒï¼Œæ’åç¬¬4ï¼›ç·¯å‰µè·Œ3.4å…ƒï¼Œæ”¶99.1å…ƒï¼Œæˆäº¤é¡93.89å„„å…ƒï¼Œæ’åç¬¬5ã€‚ è«‹ç¹¼çºŒå¾€ä¸‹é–±è®€...  æŠ€å˜‰è·Œ13.5å…ƒï¼Œæ”¶271å…ƒï¼Œæˆäº¤é¡89.05å„„å…ƒï¼Œæ’åç¬¬6ï¼›å‰µæ„æ”¶1695å…ƒå¹³ç›¤ï¼Œæˆäº¤é¡86.78å„„å…ƒï¼Œæ’åç¬¬7ï¼›è¯ç™¼ç§‘ä¸Šæ¼²27å…ƒï¼Œæ”¶842å…ƒï¼Œæˆäº¤é¡81.63å„„å…ƒï¼Œæ’åç¬¬8ï¼›è£•éš†æ¼²1.1å…ƒï¼Œæ”¶85.1å…ƒï¼Œæˆäº¤é¡66.51å„„å…ƒï¼Œæ’åç¬¬9ï¼›ææ–™-KYæ¼² 5å…ƒï¼Œæ”¶1185å…ƒï¼Œæˆäº¤é¡63.34å„„å…ƒï¼Œæ’åç¬¬10ã€‚ ä¸€æ‰‹æŒæ¡ç¶“æ¿Ÿè„ˆå‹•é»æˆ‘è¨‚é–±è‡ªç”±è²¡ç¶“Youtubeé »é“ ä¸ç”¨æŠ½ ä¸ç”¨æ¶ ç¾åœ¨ç”¨APPçœ‹æ–°è ä¿è­‰å¤©å¤©ä¸­çé»æˆ‘ä¸‹è¼‰APPæŒ‰æˆ‘çœ‹æ´»å‹•è¾¦æ³• ç›¸é—œæ–°è\"\n",
    "\n",
    "# è™•ç†æ–°æ–‡ç« \n",
    "processed_new_article = tokenize_and_remove_stopwords(new_article)\n",
    "\n",
    "# è¼¸å‡ºè™•ç†å¾Œçš„æ–‡ç« \n",
    "print(processed_new_article)\n",
    "\n",
    "# å°‡æ–°æ–‡ç« è½‰æ›ç‚º TF-IDF è¡¨ç¤ºå½¢å¼\n",
    "new_article_tfidf = tfidf_vectorizer.transform([processed_new_article])\n",
    "\n",
    "# ä½¿ç”¨æŠ•ç¥¨åˆ†é¡å™¨é€²è¡Œé æ¸¬\n",
    "predicted_label_ensemble = voting_classifier.predict(new_article_tfidf)\n",
    "\n",
    "# è¼¸å‡ºé æ¸¬çµæœ\n",
    "print(f\"æ–°æ–‡ç« é æ¸¬çµæœ: {predicted_label_ensemble}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzB1yP1GFFki"
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "# å®šç¾©åœç”¨è©\n",
    "stopwords = set(['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'ä¹Ÿ', 'èˆ‡', 'æœ‰', 'ç‚º', 'ç­‰'])\n",
    "\n",
    "# å®šç¾©åˆ†è©ä¸¦å»é™¤åœç”¨è©çš„å‡½æ•¸\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    words = jieba.cut(text)  # ä½¿ç”¨ jieba è¿›è¡Œåˆ†è¯\n",
    "    words_filtered = [word for word in words if word not in stopwords]  # å»é™¤åœç”¨è¯\n",
    "    return ' '.join(words_filtered)\n",
    "\n",
    "# å°‡è™•ç†å¾Œçš„å…§å®¹åŠ å…¥ DataFrame ä¸­\n",
    "df['processed_content'] = df['Content'].apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "# æ–°æ–‡ç« \n",
    "new_article = \"å°æ¼²\"\n",
    "\n",
    "# è™•ç†æ–°æ–‡ç« \n",
    "processed_new_article = tokenize_and_remove_stopwords(new_article)\n",
    "\n",
    "# è¼¸å‡ºè™•ç†å¾Œçš„æ–‡ç« \n",
    "print(processed_new_article)\n",
    "\n",
    "# å°‡æ–°æ–‡ç« è½‰æ›ç‚º TF-IDF è¡¨ç¤ºå½¢å¼\n",
    "new_article_tfidf = tfidf_vectorizer.transform([processed_new_article])\n",
    "\n",
    "# ä½¿ç”¨æŠ•ç¥¨åˆ†é¡å™¨é€²è¡Œé æ¸¬\n",
    "predicted_label_ensemble = voting_classifier.predict(new_article_tfidf)\n",
    "\n",
    "# è¼¸å‡ºé æ¸¬çµæœ\n",
    "print(f\"æ–°æ–‡ç« é æ¸¬çµæœ: {predicted_label_ensemble}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
