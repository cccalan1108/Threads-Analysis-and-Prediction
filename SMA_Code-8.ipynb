{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6sEhoTFDEH3"
   },
   "source": [
    "## 安裝套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2V9shIFDDG3J",
    "outputId": "72d8024f-64c8-41fc-8ca0-00a5bffe1574"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (0.42.1)\n",
      "Requirement already satisfied: emoji in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (2.14.1)\n",
      "Requirement already satisfied: langdetect in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: datasets in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (2.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (0.15.1)\n",
      "Requirement already satisfied: packaging in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: responses<0.19 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.0)\n",
      "Requirement already satisfied: filelock in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2024.12.14)\n",
      "Requirement already satisfied: six in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: lingua-language-detector in /Users/zhuotingzhen/anaconda3/lib/python3.11/site-packages (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba\n",
    "!pip install emoji langdetect\n",
    "!pip install datasets\n",
    "!pip install lingua-language-detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhtPvgrLCUzp",
    "outputId": "1ecc2382-688c-437a-bc60-6b7a4ea45862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.42.1)\n",
      "Requirement already satisfied: emoji in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.14.1)\n",
      "Requirement already satisfied: langdetect in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.0.9)\n",
      "Requirement already satisfied: pytz in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2024.1)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: lingua-language-detector in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.1.5)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.31.1)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install jieba emoji langdetect pytz torch lingua-language-detector datasets openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HGttCcMDKE0"
   },
   "source": [
    "## 引入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JQ6rzOpBTI5K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "import langdetect\n",
    "from langdetect import detect\n",
    "from lingua import LanguageDetectorBuilder, Language, IsoCode639_1\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zM7PgPNTPXMd",
    "outputId": "ca72f2aa-3135-4a1f-857e-ea5f956efe1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 設定好路徑 (後面都是使用相對路徑)\n",
    "base_path = '/content/drive/My Drive/SMA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "xOIyJJ3KSNT9",
    "outputId": "e38e0262-6047-4c2a-c225-454cac6940d1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>post_time</th>\n",
       "      <th>topic</th>\n",
       "      <th>time_info</th>\n",
       "      <th>content</th>\n",
       "      <th>has_photo</th>\n",
       "      <th>has_video</th>\n",
       "      <th>like_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>repost_count</th>\n",
       "      <th>share_count</th>\n",
       "      <th>view_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>post_url</th>\n",
       "      <th>scrape_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>2025-04-29T10:58:39.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3小時</td>\n",
       "      <td>Thank you God for another day.</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>190</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3,197</td>\n",
       "      <td>141073</td>\n",
       "      <td>https://www.threads.net/@ayofvr/post/DJBymf8uTrK</td>\n",
       "      <td>2025-04-29T22:27:40.176749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-04-29T12:33:44.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1小時</td>\n",
       "      <td>百達翡麗？ 沒有下限的網路病態！</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>196</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>3 萬</td>\n",
       "      <td>77683</td>\n",
       "      <td>https://www.threads.net/@ban.mei.onnnnni/post/...</td>\n",
       "      <td>2025-04-29T22:27:54.964788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ribboworld2021</td>\n",
       "      <td>2025-04-29T04:39:46.000Z</td>\n",
       "      <td>小一日常</td>\n",
       "      <td>9小時</td>\n",
       "      <td>考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>75</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4,559</td>\n",
       "      <td>99</td>\n",
       "      <td>https://www.threads.net/@ribboworld2021/post/D...</td>\n",
       "      <td>2025-04-29T22:28:09.873641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>2025-04-29T11:25:22.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3小時</td>\n",
       "      <td>Just be strong. Confident. Hopeful. Intellectu...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>83</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1,967</td>\n",
       "      <td>141093</td>\n",
       "      <td>https://www.threads.net/@ayofvr/post/DJB1qP5OmzP</td>\n",
       "      <td>2025-04-29T22:28:24.726576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jose_ykc</td>\n",
       "      <td>2025-04-29T10:36:19.000Z</td>\n",
       "      <td>輔仁大學</td>\n",
       "      <td>3小時</td>\n",
       "      <td>日文輔系老師上課內容之一AiScReam 歌詞導讀</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>4,334</td>\n",
       "      <td>55</td>\n",
       "      <td>513</td>\n",
       "      <td>2,460</td>\n",
       "      <td>10 萬</td>\n",
       "      <td>65</td>\n",
       "      <td>https://www.threads.net/@jose_ykc/post/DJBvpGI...</td>\n",
       "      <td>2025-04-29T22:28:39.393706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>leighton.williams</td>\n",
       "      <td>2025年04月29日 07:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8小時</td>\n",
       "      <td>First tasting in California</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>726</td>\n",
       "      <td>30</td>\n",
       "      <td>https://www.threads.net/@leighton.williams/pos...</td>\n",
       "      <td>2025-04-29T16:23:40.328046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>cape__man</td>\n",
       "      <td>2025年04月29日 06:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9小時</td>\n",
       "      <td>I hoped it would have been better.</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1,291</td>\n",
       "      <td>114</td>\n",
       "      <td>https://www.threads.net/@cape__man/post/DJAdFd...</td>\n",
       "      <td>2025-04-29T16:23:55.063517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>simimoonlight</td>\n",
       "      <td>2025年04月29日 06:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9小時</td>\n",
       "      <td>I can’t wait to watch Beyoncé on TikTok tonigh...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>2,656</td>\n",
       "      <td>29</td>\n",
       "      <td>280</td>\n",
       "      <td>47</td>\n",
       "      <td>5.6 萬</td>\n",
       "      <td>65732</td>\n",
       "      <td>https://www.threads.net/@simimoonlight/post/DJ...</td>\n",
       "      <td>2025-04-29T16:24:39.870994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>other98</td>\n",
       "      <td>2025年04月29日 12:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3小時</td>\n",
       "      <td>Tonight, Canada just proved that they have a h...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0 萬</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>22</td>\n",
       "      <td>14 萬</td>\n",
       "      <td>217182</td>\n",
       "      <td>https://www.threads.net/@other98/post/DJBGV3NxiX_</td>\n",
       "      <td>2025-04-29T16:24:54.669936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>scottiebeam</td>\n",
       "      <td>2025年04月29日 01:33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14小時</td>\n",
       "      <td>Yall be fighting on here … i thought threads w...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>427</td>\n",
       "      <td>76</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1 萬</td>\n",
       "      <td>59249</td>\n",
       "      <td>https://www.threads.net/@scottiebeam/post/DI_6...</td>\n",
       "      <td>2025-04-29T16:25:09.396585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3101 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                 post_time topic time_info  \\\n",
       "0                ayofvr  2025-04-29T10:58:39.000Z   NaN       3小時   \n",
       "1                   NaN  2025-04-29T12:33:44.000Z   NaN       1小時   \n",
       "2        ribboworld2021  2025-04-29T04:39:46.000Z  小一日常       9小時   \n",
       "3                ayofvr  2025-04-29T11:25:22.000Z   NaN       3小時   \n",
       "4              jose_ykc  2025-04-29T10:36:19.000Z  輔仁大學       3小時   \n",
       "...                 ...                       ...   ...       ...   \n",
       "3096  leighton.williams         2025年04月29日 07:30   NaN       8小時   \n",
       "3097          cape__man         2025年04月29日 06:31   NaN       9小時   \n",
       "3098      simimoonlight         2025年04月29日 06:26   NaN       9小時   \n",
       "3099            other98         2025年04月29日 12:31   NaN       3小時   \n",
       "3100        scottiebeam         2025年04月29日 01:33   NaN      14小時   \n",
       "\n",
       "                                                content has_photo has_video  \\\n",
       "0                        Thank you God for another day.         N         N   \n",
       "1                                      百達翡麗？ 沒有下限的網路病態！         Y         N   \n",
       "2              考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」         N         N   \n",
       "3     Just be strong. Confident. Hopeful. Intellectu...         N         N   \n",
       "4                             日文輔系老師上課內容之一AiScReam 歌詞導讀         Y         Y   \n",
       "...                                                 ...       ...       ...   \n",
       "3096                        First tasting in California         N         N   \n",
       "3097                 I hoped it would have been better.         N         N   \n",
       "3098  I can’t wait to watch Beyoncé on TikTok tonigh...         N         N   \n",
       "3099  Tonight, Canada just proved that they have a h...         N         N   \n",
       "3100  Yall be fighting on here … i thought threads w...         N         N   \n",
       "\n",
       "     like_count reply_count repost_count share_count view_count  \\\n",
       "0           190           3           23         NaN      3,197   \n",
       "1           196          16          NaN           6        3 萬   \n",
       "2            75           6          NaN         NaN      4,559   \n",
       "3            83           5            3           1      1,967   \n",
       "4         4,334          55          513       2,460       10 萬   \n",
       "...         ...         ...          ...         ...        ...   \n",
       "3096          7         NaN          NaN           0        726   \n",
       "3097          2         NaN          NaN           0      1,291   \n",
       "3098      2,656          29          280          47      5.6 萬   \n",
       "3099      1.0 萬         214          214          22       14 萬   \n",
       "3100        427          76           28           1        1 萬   \n",
       "\n",
       "      followers_count                                           post_url  \\\n",
       "0              141073   https://www.threads.net/@ayofvr/post/DJBymf8uTrK   \n",
       "1               77683  https://www.threads.net/@ban.mei.onnnnni/post/...   \n",
       "2                  99  https://www.threads.net/@ribboworld2021/post/D...   \n",
       "3              141093   https://www.threads.net/@ayofvr/post/DJB1qP5OmzP   \n",
       "4                  65  https://www.threads.net/@jose_ykc/post/DJBvpGI...   \n",
       "...               ...                                                ...   \n",
       "3096               30  https://www.threads.net/@leighton.williams/pos...   \n",
       "3097              114  https://www.threads.net/@cape__man/post/DJAdFd...   \n",
       "3098            65732  https://www.threads.net/@simimoonlight/post/DJ...   \n",
       "3099           217182  https://www.threads.net/@other98/post/DJBGV3NxiX_   \n",
       "3100            59249  https://www.threads.net/@scottiebeam/post/DI_6...   \n",
       "\n",
       "                     scrape_time  \n",
       "0     2025-04-29T22:27:40.176749  \n",
       "1     2025-04-29T22:27:54.964788  \n",
       "2     2025-04-29T22:28:09.873641  \n",
       "3     2025-04-29T22:28:24.726576  \n",
       "4     2025-04-29T22:28:39.393706  \n",
       "...                          ...  \n",
       "3096  2025-04-29T16:23:40.328046  \n",
       "3097  2025-04-29T16:23:55.063517  \n",
       "3098  2025-04-29T16:24:39.870994  \n",
       "3099  2025-04-29T16:24:54.669936  \n",
       "3100  2025-04-29T16:25:09.396585  \n",
       "\n",
       "[3101 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 讀取資料（請確認你的 Excel 路徑）\n",
    "# df = pd.read_excel(base_path+\"/threads.xlsx\")\n",
    "df = pd.read_excel(\"threads.xlsx\", engine='openpyxl')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kIM9eoPtiDSC"
   },
   "outputs": [],
   "source": [
    "# === 語言偵測修正版===\n",
    "lingua_detector = LanguageDetectorBuilder.from_all_languages().with_preloaded_language_models().build()\n",
    "lingua_available = True\n",
    "def detect_lang_with_preprocessing_lingua(text):\n",
    "    original_text = text\n",
    "\n",
    "    # 若是 NaN 或空字串就回傳 \"unknown\"\n",
    "    if pd.isna(text):\n",
    "        return \"unknown\"\n",
    "    text = str(text).strip()\n",
    "    if not text:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    # 移除 URL、@標記、 #hashtag、emoji、多餘空白\n",
    "    try:\n",
    "      text_cleaned = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "      text_cleaned = re.sub(r'@\\w+', '', text_cleaned)\n",
    "      text_cleaned = re.sub(r'#\\w+', '', text_cleaned)\n",
    "      text_cleaned = emoji.replace_emoji(text_cleaned, replace='')\n",
    "      text_cleaned = re.sub(r'\\s+', ' ', text_cleaned).strip()\n",
    "    except Exception as e:\n",
    "      return \"error_state_preprocessing\"\n",
    "\n",
    "    # 若這些清理完後變成空字串\n",
    "    if not text_cleaned:\n",
    "      return \"empty_after_clean\"\n",
    "\n",
    "    # 若文字中超過 30% 是中文，就直接判定為 \"Ch\"（中文）\n",
    "    try:\n",
    "      chinese_chars = re.findall(r'[\\u4e00-\\u9fff]', text_cleaned)\n",
    "      text_len = len(text_cleaned)\n",
    "      ratio = len(chinese_chars) / max(text_len, 1)\n",
    "      chinese_threshold = 0.3\n",
    "      if ratio > chinese_threshold:\n",
    "        return \"Ch\"\n",
    "\n",
    "      # 呼叫 lingua 偵測語言\n",
    "      detected_language = lingua_detector.detect_language_of(text_cleaned)\n",
    "\n",
    "      # 若 lingua 判定是中文（'ZH'），則回傳 \"Ch\"，其餘語言以小寫的 ISO 639-1 回傳（如 en, ja, fr）\n",
    "      # 若無法偵測出語言，回傳 \"unknown\"\n",
    "      if detected_language is not None:\n",
    "        iso_code = detected_language.iso_code_639_1.name\n",
    "        if iso_code == 'ZH':\n",
    "          return \"Ch\"\n",
    "        else:\n",
    "          return iso_code.lower()\n",
    "      else:\n",
    "        return \"unknown\"\n",
    "\n",
    "    except Exception as e:\n",
    "      return \"unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_nqIl5HD2ia"
   },
   "source": [
    "## 清洗數據V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LsfgBIPFrsc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 處理完成，已輸出 threads_cleaned_v1.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>post_time</th>\n",
       "      <th>topic</th>\n",
       "      <th>time_info</th>\n",
       "      <th>content</th>\n",
       "      <th>has_photo</th>\n",
       "      <th>has_video</th>\n",
       "      <th>like_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>repost_count</th>\n",
       "      <th>...</th>\n",
       "      <th>scrape_time</th>\n",
       "      <th>emojis</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>scrape_time_origin</th>\n",
       "      <th>post_weekday</th>\n",
       "      <th>post_hour</th>\n",
       "      <th>viral</th>\n",
       "      <th>has_question</th>\n",
       "      <th>has_exclaim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>2025-04-29T10:58:39.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3小時</td>\n",
       "      <td>Thank you God for another day.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>190</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>2025年04月30日 06:27</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 06:27:40.176749+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-04-29T12:33:44.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1小時</td>\n",
       "      <td>百達翡麗？ 沒有下限的網路病態！</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>196</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2025年04月30日 06:27</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025-04-30 06:27:54.964788+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ribboworld2021</td>\n",
       "      <td>2025-04-29T04:39:46.000Z</td>\n",
       "      <td>小一日常</td>\n",
       "      <td>9小時</td>\n",
       "      <td>考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>75</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025-04-30 06:28:09.873641+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>2025-04-29T11:25:22.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3小時</td>\n",
       "      <td>Just be strong. Confident. Hopeful. Intellectu...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>83</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 06:28:24.726576+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jose_ykc</td>\n",
       "      <td>2025-04-29T10:36:19.000Z</td>\n",
       "      <td>輔仁大學</td>\n",
       "      <td>3小時</td>\n",
       "      <td>日文輔系老師上課內容之一AiScReam 歌詞導讀</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4334</td>\n",
       "      <td>55</td>\n",
       "      <td>513</td>\n",
       "      <td>...</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025-04-30 06:28:39.393706+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>leighton.williams</td>\n",
       "      <td>2025年04月29日 07:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8小時</td>\n",
       "      <td>First tasting in California</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2025年04月30日 00:23</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 00:23:40.328046+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>cape__man</td>\n",
       "      <td>2025年04月29日 06:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9小時</td>\n",
       "      <td>I hoped it would have been better.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2025年04月30日 00:23</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 00:23:55.063517+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>simimoonlight</td>\n",
       "      <td>2025年04月29日 06:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9小時</td>\n",
       "      <td>I can’t wait to watch Beyoncé on TikTok tonigh...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2656</td>\n",
       "      <td>29</td>\n",
       "      <td>280</td>\n",
       "      <td>...</td>\n",
       "      <td>2025年04月30日 00:24</td>\n",
       "      <td>🫶🏿🥹</td>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 00:24:39.870994+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>other98</td>\n",
       "      <td>2025年04月29日 12:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3小時</td>\n",
       "      <td>Tonight, Canada just proved that they have a h...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>10000</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>...</td>\n",
       "      <td>2025年04月30日 00:24</td>\n",
       "      <td>🙌</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 00:24:54.669936+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>scottiebeam</td>\n",
       "      <td>2025年04月29日 01:33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14小時</td>\n",
       "      <td>Yall be fighting on here … i thought threads w...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>427</td>\n",
       "      <td>76</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>2025年04月30日 00:25</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>2025-04-30 00:25:09.396585+08:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3101 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                 post_time topic time_info  \\\n",
       "0                ayofvr  2025-04-29T10:58:39.000Z   NaN       3小時   \n",
       "1                   NaN  2025-04-29T12:33:44.000Z   NaN       1小時   \n",
       "2        ribboworld2021  2025-04-29T04:39:46.000Z  小一日常       9小時   \n",
       "3                ayofvr  2025-04-29T11:25:22.000Z   NaN       3小時   \n",
       "4              jose_ykc  2025-04-29T10:36:19.000Z  輔仁大學       3小時   \n",
       "...                 ...                       ...   ...       ...   \n",
       "3096  leighton.williams         2025年04月29日 07:30   NaN       8小時   \n",
       "3097          cape__man         2025年04月29日 06:31   NaN       9小時   \n",
       "3098      simimoonlight         2025年04月29日 06:26   NaN       9小時   \n",
       "3099            other98         2025年04月29日 12:31   NaN       3小時   \n",
       "3100        scottiebeam         2025年04月29日 01:33   NaN      14小時   \n",
       "\n",
       "                                                content  has_photo  has_video  \\\n",
       "0                        Thank you God for another day.      False      False   \n",
       "1                                      百達翡麗？ 沒有下限的網路病態！       True      False   \n",
       "2              考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」      False      False   \n",
       "3     Just be strong. Confident. Hopeful. Intellectu...      False      False   \n",
       "4                             日文輔系老師上課內容之一AiScReam 歌詞導讀       True       True   \n",
       "...                                                 ...        ...        ...   \n",
       "3096                        First tasting in California      False      False   \n",
       "3097                 I hoped it would have been better.      False      False   \n",
       "3098  I can’t wait to watch Beyoncé on TikTok tonigh...      False      False   \n",
       "3099  Tonight, Canada just proved that they have a h...      False      False   \n",
       "3100  Yall be fighting on here … i thought threads w...      False      False   \n",
       "\n",
       "      like_count  reply_count  repost_count  ...        scrape_time  emojis  \\\n",
       "0            190            3            23  ...  2025年04月30日 06:27           \n",
       "1            196           16             0  ...  2025年04月30日 06:27           \n",
       "2             75            6             0  ...  2025年04月30日 06:28           \n",
       "3             83            5             3  ...  2025年04月30日 06:28           \n",
       "4           4334           55           513  ...  2025年04月30日 06:28           \n",
       "...          ...          ...           ...  ...                ...     ...   \n",
       "3096           7            0             0  ...  2025年04月30日 00:23           \n",
       "3097           2            0             0  ...  2025年04月30日 00:23           \n",
       "3098        2656           29           280  ...  2025年04月30日 00:24     🫶🏿🥹   \n",
       "3099       10000          214           214  ...  2025年04月30日 00:24       🙌   \n",
       "3100         427           76            28  ...  2025年04月30日 00:25           \n",
       "\n",
       "      emoji_count lang               scrape_time_origin post_weekday  \\\n",
       "0               0   en 2025-04-30 06:27:40.176749+08:00    Wednesday   \n",
       "1               0   Ch 2025-04-30 06:27:54.964788+08:00    Wednesday   \n",
       "2               0   Ch 2025-04-30 06:28:09.873641+08:00    Wednesday   \n",
       "3               0   en 2025-04-30 06:28:24.726576+08:00    Wednesday   \n",
       "4               0   Ch 2025-04-30 06:28:39.393706+08:00    Wednesday   \n",
       "...           ...  ...                              ...          ...   \n",
       "3096            0   en 2025-04-30 00:23:40.328046+08:00    Wednesday   \n",
       "3097            0   en 2025-04-30 00:23:55.063517+08:00    Wednesday   \n",
       "3098            3   en 2025-04-30 00:24:39.870994+08:00    Wednesday   \n",
       "3099            1   en 2025-04-30 00:24:54.669936+08:00    Wednesday   \n",
       "3100            0   en 2025-04-30 00:25:09.396585+08:00    Wednesday   \n",
       "\n",
       "      post_hour viral has_question has_exclaim  \n",
       "0             6     0        False       False  \n",
       "1             6     1         True        True  \n",
       "2             6     0        False        True  \n",
       "3             6     0        False       False  \n",
       "4             6     1        False       False  \n",
       "...         ...   ...          ...         ...  \n",
       "3096          0     0        False       False  \n",
       "3097          0     0        False       False  \n",
       "3098          0     1        False       False  \n",
       "3099          0     1        False        True  \n",
       "3100          0     1        False       False  \n",
       "\n",
       "[3101 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === 數值欄位清洗（萬字、逗號格式處理）===\n",
    "def parse_count(value):\n",
    "    # 將文字數字（如 \"1,234\"、\"2.5萬\"）統一轉為整數（int）\n",
    "    if pd.isna(value): return 0\n",
    "    value = str(value).replace(\",\", \"\")\n",
    "    # \"萬\" 的部分會乘上 10,000 做轉換\n",
    "    # 無法處理的格式就回傳 0\n",
    "    if \"萬\" in value:\n",
    "        return int(float(value.replace(\"萬\", \"\")) * 10000)\n",
    "    try:\n",
    "        return int(float(value))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "for col in [\"like_count\", \"view_count\", \"share_count\", \"repost_count\", \"reply_count\"]:\n",
    "    df[col] = df[col].apply(parse_count)\n",
    "\n",
    "# === 布林欄位處理 ===\n",
    "# 將原始欄位（Y/N）轉換為 True/False\n",
    "# 處理過程會去除空白、轉成大寫\n",
    "df[\"has_photo\"] = df[\"has_photo\"].apply(lambda x: str(x).strip().upper() == \"Y\")\n",
    "df[\"has_video\"] = df[\"has_video\"].apply(lambda x: str(x).strip().upper() == \"Y\")\n",
    "\n",
    "# === emoji 萃取與統計 ===\n",
    "# 檢查是否為文字型別，如果是文字，從中萃取出所有 emoji 字元並串接成字串回傳\n",
    "def extract_emojis(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return \"\".join([ch for ch in text if ch in emoji.EMOJI_DATA])\n",
    "\n",
    "df[\"emojis\"] = df[\"content\"].apply(extract_emojis)\n",
    "df[\"emoji_count\"] = df[\"emojis\"].apply(len)\n",
    "\n",
    "# # === 語言偵測修正版===\n",
    "# def detect_lang_custom(text):\n",
    "#     try:\n",
    "#         text = str(text)\n",
    "#         chinese_chars = re.findall(r'[\\u4e00-\\u9fff]', text)\n",
    "#         if len(chinese_chars) / max(len(text), 1) > 0.3:\n",
    "#             return \"Ch\"\n",
    "#         return detect(text)\n",
    "#     except:\n",
    "#         return \"unknown\"\n",
    "\n",
    "# 使用先前定義好的語言偵測函數 detect_lang_with_preprocessing_lingua()，處理每篇文章的語言判定\n",
    "df[\"lang\"] = df[\"content\"].apply(detect_lang_with_preprocessing_lingua)\n",
    "\n",
    "# === scrape_time 處理（轉換時區 + 抽取星期與小時）===\n",
    "# 將時間欄位轉為台北時區，額外抽出格式化後的時間字串、星期幾、小時(0–23）\n",
    "df[\"scrape_time_origin\"] = pd.to_datetime(df[\"scrape_time\"], utc=True).dt.tz_convert(\"Asia/Taipei\")\n",
    "df[\"scrape_time\"]  = df[\"scrape_time_origin\"].dt.strftime(\"%Y年%m月%d日 %H:%M\")\n",
    "df[\"post_weekday\"] = df[\"scrape_time_origin\"].dt.day_name()\n",
    "df[\"post_hour\"] = df[\"scrape_time_origin\"].dt.hour\n",
    "\n",
    "# === 是否為高流量文章（破萬）===\n",
    "# 超過等於 10,000 瀏覽為 1，其餘為 0\n",
    "df[\"viral\"] = (df[\"view_count\"] >= 10000).astype(int)\n",
    "\n",
    "# === 是否使用問號、驚嘆號 ===\n",
    "df[\"has_question\"] = df[\"content\"].apply(lambda x: \"？\" in str(x) or \"?\" in str(x))\n",
    "df[\"has_exclaim\"] = df[\"content\"].apply(lambda x: \"！\" in str(x) or \"!\" in str(x))\n",
    "\n",
    "# === 儲存結果 ===\n",
    "df.to_csv(\"threads_cleaned_v1.csv\",encoding='utf_8_sig', index=False)\n",
    "print(\"✅ 處理完成，已輸出 threads_cleaned_v1.csv\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTVtegcwEfb1"
   },
   "source": [
    "## 清洗數據V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_a9vHZ0p9o2u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 處理完成，已輸出 threads_cleaned_v2.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>content_length</th>\n",
       "      <th>lang</th>\n",
       "      <th>scrape_time</th>\n",
       "      <th>post_weekday</th>\n",
       "      <th>post_hour</th>\n",
       "      <th>post_period</th>\n",
       "      <th>viral</th>\n",
       "      <th>post_time</th>\n",
       "      <th>...</th>\n",
       "      <th>post_url</th>\n",
       "      <th>emojis</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>scrape_time_origin</th>\n",
       "      <th>has_question</th>\n",
       "      <th>has_exclaim</th>\n",
       "      <th>has_url</th>\n",
       "      <th>has_mention</th>\n",
       "      <th>has_hashtag</th>\n",
       "      <th>time_elapsed_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>Thank you God for another day.</td>\n",
       "      <td>30</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 06:27</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T10:58:39.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@ayofvr/post/DJBymf8uTrK</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:27:40.176749+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>百達翡麗？ 沒有下限的網路病態！</td>\n",
       "      <td>16</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 06:27</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-29T12:33:44.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@ban.mei.onnnnni/post/...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:27:54.964788+08:00</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ribboworld2021</td>\n",
       "      <td>考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」</td>\n",
       "      <td>40</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T04:39:46.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@ribboworld2021/post/D...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:09.873641+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>Just be strong. Confident. Hopeful. Intellectu...</td>\n",
       "      <td>69</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T11:25:22.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@ayofvr/post/DJB1qP5OmzP</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:24.726576+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jose_ykc</td>\n",
       "      <td>日文輔系老師上課內容之一AiScReam 歌詞導讀</td>\n",
       "      <td>25</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-29T10:36:19.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@jose_ykc/post/DJBvpGI...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:39.393706+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>leighton.williams</td>\n",
       "      <td>First tasting in California</td>\n",
       "      <td>27</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:23</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025年04月29日 07:30</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@leighton.williams/pos...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:23:40.328046+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>cape__man</td>\n",
       "      <td>I hoped it would have been better.</td>\n",
       "      <td>34</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:23</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025年04月29日 06:31</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@cape__man/post/DJAdFd...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:23:55.063517+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>simimoonlight</td>\n",
       "      <td>I can’t wait to watch Beyoncé on TikTok tonigh...</td>\n",
       "      <td>51</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:24</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025年04月29日 06:26</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@simimoonlight/post/DJ...</td>\n",
       "      <td>🫶🏿🥹</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-04-30 00:24:39.870994+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>other98</td>\n",
       "      <td>Tonight, Canada just proved that they have a h...</td>\n",
       "      <td>77</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:24</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025年04月29日 12:31</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@other98/post/DJBGV3NxiX_</td>\n",
       "      <td>🙌</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-30 00:24:54.669936+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>scottiebeam</td>\n",
       "      <td>Yall be fighting on here … i thought threads w...</td>\n",
       "      <td>57</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:25</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025年04月29日 01:33</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.threads.net/@scottiebeam/post/DI_6...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:25:09.396585+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3083 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                                            content  \\\n",
       "0                ayofvr                     Thank you God for another day.   \n",
       "1                   NaN                                   百達翡麗？ 沒有下限的網路病態！   \n",
       "2        ribboworld2021           考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」   \n",
       "3                ayofvr  Just be strong. Confident. Hopeful. Intellectu...   \n",
       "4              jose_ykc                          日文輔系老師上課內容之一AiScReam 歌詞導讀   \n",
       "...                 ...                                                ...   \n",
       "3096  leighton.williams                        First tasting in California   \n",
       "3097          cape__man                 I hoped it would have been better.   \n",
       "3098      simimoonlight  I can’t wait to watch Beyoncé on TikTok tonigh...   \n",
       "3099            other98  Tonight, Canada just proved that they have a h...   \n",
       "3100        scottiebeam  Yall be fighting on here … i thought threads w...   \n",
       "\n",
       "      content_length lang        scrape_time post_weekday  post_hour  \\\n",
       "0                 30   en  2025年04月30日 06:27    Wednesday          6   \n",
       "1                 16   Ch  2025年04月30日 06:27    Wednesday          6   \n",
       "2                 40   Ch  2025年04月30日 06:28    Wednesday          6   \n",
       "3                 69   en  2025年04月30日 06:28    Wednesday          6   \n",
       "4                 25   Ch  2025年04月30日 06:28    Wednesday          6   \n",
       "...              ...  ...                ...          ...        ...   \n",
       "3096              27   en  2025年04月30日 00:23    Wednesday          0   \n",
       "3097              34   en  2025年04月30日 00:23    Wednesday          0   \n",
       "3098              51   en  2025年04月30日 00:24    Wednesday          0   \n",
       "3099              77   en  2025年04月30日 00:24    Wednesday          0   \n",
       "3100              57   en  2025年04月30日 00:25    Wednesday          0   \n",
       "\n",
       "     post_period  viral                 post_time  ...  \\\n",
       "0        morning      0  2025-04-29T10:58:39.000Z  ...   \n",
       "1        morning      1  2025-04-29T12:33:44.000Z  ...   \n",
       "2        morning      0  2025-04-29T04:39:46.000Z  ...   \n",
       "3        morning      0  2025-04-29T11:25:22.000Z  ...   \n",
       "4        morning      1  2025-04-29T10:36:19.000Z  ...   \n",
       "...          ...    ...                       ...  ...   \n",
       "3096       night      0         2025年04月29日 07:30  ...   \n",
       "3097       night      0         2025年04月29日 06:31  ...   \n",
       "3098       night      1         2025年04月29日 06:26  ...   \n",
       "3099       night      1         2025年04月29日 12:31  ...   \n",
       "3100       night      1         2025年04月29日 01:33  ...   \n",
       "\n",
       "                                               post_url emojis  emoji_count  \\\n",
       "0      https://www.threads.net/@ayofvr/post/DJBymf8uTrK                   0   \n",
       "1     https://www.threads.net/@ban.mei.onnnnni/post/...                   0   \n",
       "2     https://www.threads.net/@ribboworld2021/post/D...                   0   \n",
       "3      https://www.threads.net/@ayofvr/post/DJB1qP5OmzP                   0   \n",
       "4     https://www.threads.net/@jose_ykc/post/DJBvpGI...                   0   \n",
       "...                                                 ...    ...          ...   \n",
       "3096  https://www.threads.net/@leighton.williams/pos...                   0   \n",
       "3097  https://www.threads.net/@cape__man/post/DJAdFd...                   0   \n",
       "3098  https://www.threads.net/@simimoonlight/post/DJ...    🫶🏿🥹            3   \n",
       "3099  https://www.threads.net/@other98/post/DJBGV3NxiX_      🙌            1   \n",
       "3100  https://www.threads.net/@scottiebeam/post/DI_6...                   0   \n",
       "\n",
       "                   scrape_time_origin  has_question  has_exclaim  has_url  \\\n",
       "0    2025-04-30 06:27:40.176749+08:00         False        False    False   \n",
       "1    2025-04-30 06:27:54.964788+08:00          True         True    False   \n",
       "2    2025-04-30 06:28:09.873641+08:00         False         True    False   \n",
       "3    2025-04-30 06:28:24.726576+08:00         False        False    False   \n",
       "4    2025-04-30 06:28:39.393706+08:00         False        False    False   \n",
       "...                               ...           ...          ...      ...   \n",
       "3096 2025-04-30 00:23:40.328046+08:00         False        False    False   \n",
       "3097 2025-04-30 00:23:55.063517+08:00         False        False    False   \n",
       "3098 2025-04-30 00:24:39.870994+08:00         False        False    False   \n",
       "3099 2025-04-30 00:24:54.669936+08:00         False         True    False   \n",
       "3100 2025-04-30 00:25:09.396585+08:00         False        False    False   \n",
       "\n",
       "      has_mention  has_hashtag  time_elapsed_hours  \n",
       "0           False        False                 3.0  \n",
       "1           False        False                 1.0  \n",
       "2           False        False                 9.0  \n",
       "3           False        False                 3.0  \n",
       "4           False        False                 3.0  \n",
       "...           ...          ...                 ...  \n",
       "3096        False        False                 8.0  \n",
       "3097        False        False                 9.0  \n",
       "3098        False        False                 9.0  \n",
       "3099        False        False                 3.0  \n",
       "3100        False        False                14.0  \n",
       "\n",
       "[3083 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 文章長度 ---\n",
    "df[\"content_length\"] = df[\"content\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "# --- 是否包含網址 ---\n",
    "df[\"has_url\"] = df[\"content\"].apply(lambda x: \"http\" in str(x) or \"www.\" in str(x))\n",
    "\n",
    "# --- 是否包含 @標記他人 ---\n",
    "df[\"has_mention\"] = df[\"content\"].apply(lambda x: \"@\" in str(x))\n",
    "\n",
    "# --- 是否使用 Hashtag ---\n",
    "df[\"has_hashtag\"] = df[\"content\"].apply(lambda x: \"#\" in str(x))\n",
    "\n",
    "# 貼文主題字詞提取（可後續做 TF-IDF 或主題建模）\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=100, stop_words='english')\n",
    "word_matrix = vectorizer.fit_transform(df['content'].astype(str))\n",
    "\n",
    "# 將常見詞語提取出來\n",
    "keywords = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 是否為深夜或白天貼文（時間段分類）\n",
    "def time_period(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return \"morning\"\n",
    "    elif 12 <= hour < 17:\n",
    "        return \"afternoon\"\n",
    "    elif 17 <= hour < 22:\n",
    "        return \"evening\"\n",
    "    else:\n",
    "        return \"night\"\n",
    "    \n",
    "# 判斷是否合法的time_info格式\n",
    "def is_valid_time_info(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    text = str(text).strip()\n",
    "    return bool(re.match(r\"^\\d+\\s*(分鐘|小時|天)$\", text))\n",
    "\n",
    "df = df[df[\"time_info\"].apply(is_valid_time_info)].copy()\n",
    "\n",
    "# 將time_info 統一轉為小時\n",
    "def convert_time_info(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    text = str(text)\n",
    "    if \"分鐘\" in text:\n",
    "        match = re.search(r\"(\\d+)\", text)\n",
    "        return round(int(match.group(1)) / 60, 1)\n",
    "    elif \"小時\" in text:\n",
    "        match = re.search(r\"(\\d+)\", text)\n",
    "        return round(int(match.group(1)), 1)\n",
    "    elif \"天\" in text:\n",
    "        match = re.search(r\"(\\d+)\", text)\n",
    "        return round(int(match.group(1)) * 24, 1)\n",
    "    elif \"週\" in text or \"禮拜\" in text:\n",
    "        match = re.search(r\"(\\d+)\", text)\n",
    "        return round(int(match.group(1)) * 7 * 24, 1)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "df[\"time_elapsed_hours\"] = df[\"time_info\"].apply(convert_time_info)\n",
    "df[\"post_period\"] = df[\"post_hour\"].apply(time_period)\n",
    "\n",
    "cols_to_show_first = ['author', 'content', 'content_length', 'lang', 'scrape_time', 'post_weekday', 'post_hour', 'post_period', 'viral']\n",
    "df = df[cols_to_show_first + [col for col in df.columns if col not in cols_to_show_first]]\n",
    "df.to_csv(\"threads_cleaned_v2.csv\",encoding='utf_8_sig',index=False)\n",
    "print(\"✅ 處理完成，已輸出 threads_cleaned_v2.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention 模組處理文字資料（測試中 因未寫完可先跳過）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>content_length</th>\n",
       "      <th>lang</th>\n",
       "      <th>scrape_time</th>\n",
       "      <th>post_weekday</th>\n",
       "      <th>post_hour</th>\n",
       "      <th>post_period</th>\n",
       "      <th>viral</th>\n",
       "      <th>post_time</th>\n",
       "      <th>...</th>\n",
       "      <th>emojis</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>scrape_time_origin</th>\n",
       "      <th>has_question</th>\n",
       "      <th>has_exclaim</th>\n",
       "      <th>has_url</th>\n",
       "      <th>has_mention</th>\n",
       "      <th>has_hashtag</th>\n",
       "      <th>time_elapsed_hours</th>\n",
       "      <th>semantic_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>Thank you God for another day.</td>\n",
       "      <td>30</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 06:27</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T10:58:39.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:27:40.176749+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[AUTHOR] ayofvr [EMOJI]  [CONTENT] Thank you G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>百達翡麗？ 沒有下限的網路病態！</td>\n",
       "      <td>16</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 06:27</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-29T12:33:44.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:27:54.964788+08:00</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[EMOJI]  [CONTENT] 百達翡麗？ 沒有下限的網路病態！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ribboworld2021</td>\n",
       "      <td>考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」</td>\n",
       "      <td>40</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T04:39:46.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:09.873641+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[AUTHOR] ribboworld2021 [TOPIC] 小一日常 [EMOJI]  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>Just be strong. Confident. Hopeful. Intellectu...</td>\n",
       "      <td>69</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T11:25:22.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:24.726576+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[AUTHOR] ayofvr [EMOJI]  [CONTENT] Just be str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jose_ykc</td>\n",
       "      <td>日文輔系老師上課內容之一AiScReam 歌詞導讀</td>\n",
       "      <td>25</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-29T10:36:19.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:39.393706+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[AUTHOR] jose_ykc [TOPIC] 輔仁大學 [EMOJI]  [CONTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>leighton.williams</td>\n",
       "      <td>First tasting in California</td>\n",
       "      <td>27</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:23</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025年04月29日 07:30</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:23:40.328046+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8.0</td>\n",
       "      <td>[AUTHOR] leighton.williams [EMOJI]  [CONTENT] ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>cape__man</td>\n",
       "      <td>I hoped it would have been better.</td>\n",
       "      <td>34</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:23</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025年04月29日 06:31</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:23:55.063517+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[AUTHOR] cape__man [EMOJI]  [CONTENT] I hoped ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>simimoonlight</td>\n",
       "      <td>I can’t wait to watch Beyoncé on TikTok tonigh...</td>\n",
       "      <td>51</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:24</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025年04月29日 06:26</td>\n",
       "      <td>...</td>\n",
       "      <td>🫶🏿🥹</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-04-30 00:24:39.870994+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[AUTHOR] simimoonlight [EMOJI] 🫶 🏿 🥹 [CONTENT]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>other98</td>\n",
       "      <td>Tonight, Canada just proved that they have a h...</td>\n",
       "      <td>77</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:24</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025年04月29日 12:31</td>\n",
       "      <td>...</td>\n",
       "      <td>🙌</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-30 00:24:54.669936+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[AUTHOR] other98 [EMOJI] 🙌 [CONTENT] Tonight, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>scottiebeam</td>\n",
       "      <td>Yall be fighting on here … i thought threads w...</td>\n",
       "      <td>57</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:25</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025年04月29日 01:33</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:25:09.396585+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>14.0</td>\n",
       "      <td>[AUTHOR] scottiebeam [EMOJI]  [CONTENT] Yall b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3083 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                                            content  \\\n",
       "0                ayofvr                     Thank you God for another day.   \n",
       "1                   NaN                                   百達翡麗？ 沒有下限的網路病態！   \n",
       "2        ribboworld2021           考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」   \n",
       "3                ayofvr  Just be strong. Confident. Hopeful. Intellectu...   \n",
       "4              jose_ykc                          日文輔系老師上課內容之一AiScReam 歌詞導讀   \n",
       "...                 ...                                                ...   \n",
       "3096  leighton.williams                        First tasting in California   \n",
       "3097          cape__man                 I hoped it would have been better.   \n",
       "3098      simimoonlight  I can’t wait to watch Beyoncé on TikTok tonigh...   \n",
       "3099            other98  Tonight, Canada just proved that they have a h...   \n",
       "3100        scottiebeam  Yall be fighting on here … i thought threads w...   \n",
       "\n",
       "      content_length lang        scrape_time post_weekday  post_hour  \\\n",
       "0                 30   en  2025年04月30日 06:27    Wednesday          6   \n",
       "1                 16   Ch  2025年04月30日 06:27    Wednesday          6   \n",
       "2                 40   Ch  2025年04月30日 06:28    Wednesday          6   \n",
       "3                 69   en  2025年04月30日 06:28    Wednesday          6   \n",
       "4                 25   Ch  2025年04月30日 06:28    Wednesday          6   \n",
       "...              ...  ...                ...          ...        ...   \n",
       "3096              27   en  2025年04月30日 00:23    Wednesday          0   \n",
       "3097              34   en  2025年04月30日 00:23    Wednesday          0   \n",
       "3098              51   en  2025年04月30日 00:24    Wednesday          0   \n",
       "3099              77   en  2025年04月30日 00:24    Wednesday          0   \n",
       "3100              57   en  2025年04月30日 00:25    Wednesday          0   \n",
       "\n",
       "     post_period  viral                 post_time  ... emojis emoji_count  \\\n",
       "0        morning      0  2025-04-29T10:58:39.000Z  ...                  0   \n",
       "1        morning      1  2025-04-29T12:33:44.000Z  ...                  0   \n",
       "2        morning      0  2025-04-29T04:39:46.000Z  ...                  0   \n",
       "3        morning      0  2025-04-29T11:25:22.000Z  ...                  0   \n",
       "4        morning      1  2025-04-29T10:36:19.000Z  ...                  0   \n",
       "...          ...    ...                       ...  ...    ...         ...   \n",
       "3096       night      0         2025年04月29日 07:30  ...                  0   \n",
       "3097       night      0         2025年04月29日 06:31  ...                  0   \n",
       "3098       night      1         2025年04月29日 06:26  ...    🫶🏿🥹           3   \n",
       "3099       night      1         2025年04月29日 12:31  ...      🙌           1   \n",
       "3100       night      1         2025年04月29日 01:33  ...                  0   \n",
       "\n",
       "                   scrape_time_origin  has_question  has_exclaim  has_url  \\\n",
       "0    2025-04-30 06:27:40.176749+08:00         False        False    False   \n",
       "1    2025-04-30 06:27:54.964788+08:00          True         True    False   \n",
       "2    2025-04-30 06:28:09.873641+08:00         False         True    False   \n",
       "3    2025-04-30 06:28:24.726576+08:00         False        False    False   \n",
       "4    2025-04-30 06:28:39.393706+08:00         False        False    False   \n",
       "...                               ...           ...          ...      ...   \n",
       "3096 2025-04-30 00:23:40.328046+08:00         False        False    False   \n",
       "3097 2025-04-30 00:23:55.063517+08:00         False        False    False   \n",
       "3098 2025-04-30 00:24:39.870994+08:00         False        False    False   \n",
       "3099 2025-04-30 00:24:54.669936+08:00         False         True    False   \n",
       "3100 2025-04-30 00:25:09.396585+08:00         False        False    False   \n",
       "\n",
       "      has_mention  has_hashtag  time_elapsed_hours  \\\n",
       "0           False        False                 3.0   \n",
       "1           False        False                 1.0   \n",
       "2           False        False                 9.0   \n",
       "3           False        False                 3.0   \n",
       "4           False        False                 3.0   \n",
       "...           ...          ...                 ...   \n",
       "3096        False        False                 8.0   \n",
       "3097        False        False                 9.0   \n",
       "3098        False        False                 9.0   \n",
       "3099        False        False                 3.0   \n",
       "3100        False        False                14.0   \n",
       "\n",
       "                                          semantic_text  \n",
       "0     [AUTHOR] ayofvr [EMOJI]  [CONTENT] Thank you G...  \n",
       "1                   [EMOJI]  [CONTENT] 百達翡麗？ 沒有下限的網路病態！  \n",
       "2     [AUTHOR] ribboworld2021 [TOPIC] 小一日常 [EMOJI]  ...  \n",
       "3     [AUTHOR] ayofvr [EMOJI]  [CONTENT] Just be str...  \n",
       "4     [AUTHOR] jose_ykc [TOPIC] 輔仁大學 [EMOJI]  [CONTE...  \n",
       "...                                                 ...  \n",
       "3096  [AUTHOR] leighton.williams [EMOJI]  [CONTENT] ...  \n",
       "3097  [AUTHOR] cape__man [EMOJI]  [CONTENT] I hoped ...  \n",
       "3098  [AUTHOR] simimoonlight [EMOJI] 🫶 🏿 🥹 [CONTENT]...  \n",
       "3099  [AUTHOR] other98 [EMOJI] 🙌 [CONTENT] Tonight, ...  \n",
       "3100  [AUTHOR] scottiebeam [EMOJI]  [CONTENT] Yall b...  \n",
       "\n",
       "[3083 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT 會把 [EMOJI], [CONTENT] 當作分界的詞來理解\n",
    "def build_semantic_text(row):\n",
    "    parts = []\n",
    "\n",
    "    if pd.notna(row[\"author\"]):\n",
    "        parts.append(f\"[AUTHOR] {row['author']}\")\n",
    "\n",
    "    if pd.notna(row[\"topic\"]):\n",
    "        parts.append(f\"[TOPIC] {row['topic']}\")\n",
    "\n",
    "    if pd.notna(row[\"emojis\"]):\n",
    "        emoji_text = \" \".join(row[\"emojis\"])\n",
    "        parts.append(f\"[EMOJI] {emoji_text}\")\n",
    "\n",
    "    if pd.notna(row[\"content\"]):\n",
    "        parts.append(f\"[CONTENT] {row['content']}\")\n",
    "\n",
    "    return \" \".join(parts)\n",
    "\n",
    "\n",
    "# 建立 semantic_text，不影響 df 本身\n",
    "semantic_text_series = df.apply(build_semantic_text, axis=1)\n",
    "\n",
    "# 若你想要新 DataFrame：\n",
    "df_semantic = df.copy()\n",
    "df_semantic[\"semantic_text\"] = semantic_text_series\n",
    "df_semantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 載入 BERT tokenizer 和模型\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = BertModel.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# 自訂 Dataset 類別（不變）\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, max_len=128):\n",
    "        self.encodings = tokenizer(\n",
    "            texts, \n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx]\n",
    "        }\n",
    "\n",
    "# 建立 dataset 和 dataloader（這裡建議用 semantic_text 而不是 content）\n",
    "texts = df_semantic[\"semantic_text\"].astype(str).tolist()\n",
    "text_dataset = TextDataset(texts)\n",
    "text_loader = DataLoader(text_dataset, batch_size=32)\n",
    "\n",
    "# 提取語意向量 Z_text\n",
    "Z_text_list = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in text_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_rep = outputs.last_hidden_state[:, 0, :]  # 取 [CLS] 向量\n",
    "        \n",
    "        Z_text_list.append(cls_rep.cpu())\n",
    "\n",
    "Z_text_tensor = torch.cat(Z_text_list, dim=0)  # [N, 768]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.197812</td>\n",
       "      <td>0.491429</td>\n",
       "      <td>-0.485847</td>\n",
       "      <td>0.316401</td>\n",
       "      <td>1.326630</td>\n",
       "      <td>-1.078312</td>\n",
       "      <td>0.297176</td>\n",
       "      <td>-0.804608</td>\n",
       "      <td>-1.177580</td>\n",
       "      <td>0.156056</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.562810</td>\n",
       "      <td>-0.411340</td>\n",
       "      <td>0.433298</td>\n",
       "      <td>-0.422068</td>\n",
       "      <td>-0.669982</td>\n",
       "      <td>-0.726443</td>\n",
       "      <td>0.106795</td>\n",
       "      <td>1.173453</td>\n",
       "      <td>0.316311</td>\n",
       "      <td>0.108925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.215056</td>\n",
       "      <td>1.000118</td>\n",
       "      <td>-0.044262</td>\n",
       "      <td>-0.209858</td>\n",
       "      <td>0.740916</td>\n",
       "      <td>-1.110934</td>\n",
       "      <td>-0.698522</td>\n",
       "      <td>-0.050963</td>\n",
       "      <td>-0.216489</td>\n",
       "      <td>1.010718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426817</td>\n",
       "      <td>-0.219904</td>\n",
       "      <td>-0.014254</td>\n",
       "      <td>-0.705548</td>\n",
       "      <td>0.285813</td>\n",
       "      <td>-0.596890</td>\n",
       "      <td>-0.212060</td>\n",
       "      <td>0.695832</td>\n",
       "      <td>-0.082648</td>\n",
       "      <td>0.383929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.887392</td>\n",
       "      <td>0.952374</td>\n",
       "      <td>0.468288</td>\n",
       "      <td>0.363257</td>\n",
       "      <td>1.021098</td>\n",
       "      <td>-1.253425</td>\n",
       "      <td>-0.300586</td>\n",
       "      <td>-0.201750</td>\n",
       "      <td>-0.855971</td>\n",
       "      <td>0.126374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325378</td>\n",
       "      <td>-0.584822</td>\n",
       "      <td>-0.845940</td>\n",
       "      <td>-0.637423</td>\n",
       "      <td>-0.245427</td>\n",
       "      <td>0.500610</td>\n",
       "      <td>0.191019</td>\n",
       "      <td>0.498423</td>\n",
       "      <td>0.164274</td>\n",
       "      <td>-0.312768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.188987</td>\n",
       "      <td>0.500823</td>\n",
       "      <td>-1.228274</td>\n",
       "      <td>-0.300035</td>\n",
       "      <td>1.055788</td>\n",
       "      <td>-1.166170</td>\n",
       "      <td>0.346816</td>\n",
       "      <td>-0.645881</td>\n",
       "      <td>-0.857121</td>\n",
       "      <td>0.474149</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.987927</td>\n",
       "      <td>-0.638470</td>\n",
       "      <td>-0.161924</td>\n",
       "      <td>-0.339970</td>\n",
       "      <td>-0.626445</td>\n",
       "      <td>-0.612481</td>\n",
       "      <td>0.194379</td>\n",
       "      <td>1.441311</td>\n",
       "      <td>-0.297321</td>\n",
       "      <td>0.020321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.932422</td>\n",
       "      <td>0.218099</td>\n",
       "      <td>-1.037628</td>\n",
       "      <td>0.035938</td>\n",
       "      <td>1.054755</td>\n",
       "      <td>-0.978883</td>\n",
       "      <td>-0.409518</td>\n",
       "      <td>0.187629</td>\n",
       "      <td>-0.958486</td>\n",
       "      <td>0.335753</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.653077</td>\n",
       "      <td>-0.492126</td>\n",
       "      <td>0.483244</td>\n",
       "      <td>-0.070685</td>\n",
       "      <td>-0.069700</td>\n",
       "      <td>0.078444</td>\n",
       "      <td>0.408898</td>\n",
       "      <td>1.241559</td>\n",
       "      <td>-0.151853</td>\n",
       "      <td>-0.196577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3078</th>\n",
       "      <td>-1.150749</td>\n",
       "      <td>0.211006</td>\n",
       "      <td>-0.857209</td>\n",
       "      <td>0.486619</td>\n",
       "      <td>0.887699</td>\n",
       "      <td>-1.112387</td>\n",
       "      <td>0.242534</td>\n",
       "      <td>-0.489165</td>\n",
       "      <td>-1.174777</td>\n",
       "      <td>0.081529</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.239787</td>\n",
       "      <td>-0.378836</td>\n",
       "      <td>-0.029298</td>\n",
       "      <td>-0.404599</td>\n",
       "      <td>-0.395586</td>\n",
       "      <td>-0.504007</td>\n",
       "      <td>-0.001980</td>\n",
       "      <td>1.394140</td>\n",
       "      <td>-0.356526</td>\n",
       "      <td>-0.057205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3079</th>\n",
       "      <td>-0.742924</td>\n",
       "      <td>-0.134855</td>\n",
       "      <td>-0.337486</td>\n",
       "      <td>0.413057</td>\n",
       "      <td>1.205682</td>\n",
       "      <td>-1.366358</td>\n",
       "      <td>-0.096314</td>\n",
       "      <td>-0.656512</td>\n",
       "      <td>-1.172672</td>\n",
       "      <td>0.480091</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.972684</td>\n",
       "      <td>-0.425606</td>\n",
       "      <td>0.257879</td>\n",
       "      <td>-0.083000</td>\n",
       "      <td>-0.208448</td>\n",
       "      <td>-0.519165</td>\n",
       "      <td>0.292847</td>\n",
       "      <td>1.117920</td>\n",
       "      <td>0.169528</td>\n",
       "      <td>0.234989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3080</th>\n",
       "      <td>-1.257781</td>\n",
       "      <td>0.298899</td>\n",
       "      <td>-0.860130</td>\n",
       "      <td>0.276141</td>\n",
       "      <td>1.046705</td>\n",
       "      <td>-0.745504</td>\n",
       "      <td>-0.342909</td>\n",
       "      <td>-0.462435</td>\n",
       "      <td>-1.735537</td>\n",
       "      <td>0.148792</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.665857</td>\n",
       "      <td>-0.498027</td>\n",
       "      <td>-0.127219</td>\n",
       "      <td>-1.029894</td>\n",
       "      <td>-0.855351</td>\n",
       "      <td>-0.247067</td>\n",
       "      <td>0.405077</td>\n",
       "      <td>1.160195</td>\n",
       "      <td>0.243745</td>\n",
       "      <td>0.261609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3081</th>\n",
       "      <td>-0.801286</td>\n",
       "      <td>0.418368</td>\n",
       "      <td>-0.866499</td>\n",
       "      <td>-0.064372</td>\n",
       "      <td>0.974501</td>\n",
       "      <td>-1.084798</td>\n",
       "      <td>0.472064</td>\n",
       "      <td>-0.299250</td>\n",
       "      <td>-0.953439</td>\n",
       "      <td>0.027430</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.174034</td>\n",
       "      <td>-0.653110</td>\n",
       "      <td>0.092794</td>\n",
       "      <td>-0.495103</td>\n",
       "      <td>-0.692927</td>\n",
       "      <td>-0.327008</td>\n",
       "      <td>0.045146</td>\n",
       "      <td>1.080402</td>\n",
       "      <td>0.078887</td>\n",
       "      <td>-0.075638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>-0.707997</td>\n",
       "      <td>0.675630</td>\n",
       "      <td>-0.233938</td>\n",
       "      <td>0.159180</td>\n",
       "      <td>1.000529</td>\n",
       "      <td>-1.665780</td>\n",
       "      <td>0.296066</td>\n",
       "      <td>-0.708535</td>\n",
       "      <td>-1.331825</td>\n",
       "      <td>-0.255058</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.897987</td>\n",
       "      <td>-0.625595</td>\n",
       "      <td>0.092348</td>\n",
       "      <td>-0.311444</td>\n",
       "      <td>0.269258</td>\n",
       "      <td>-0.207657</td>\n",
       "      <td>0.134844</td>\n",
       "      <td>0.806955</td>\n",
       "      <td>-0.166361</td>\n",
       "      <td>0.017548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3083 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -1.197812  0.491429 -0.485847  0.316401  1.326630 -1.078312  0.297176   \n",
       "1     0.215056  1.000118 -0.044262 -0.209858  0.740916 -1.110934 -0.698522   \n",
       "2     0.887392  0.952374  0.468288  0.363257  1.021098 -1.253425 -0.300586   \n",
       "3    -1.188987  0.500823 -1.228274 -0.300035  1.055788 -1.166170  0.346816   \n",
       "4    -0.932422  0.218099 -1.037628  0.035938  1.054755 -0.978883 -0.409518   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3078 -1.150749  0.211006 -0.857209  0.486619  0.887699 -1.112387  0.242534   \n",
       "3079 -0.742924 -0.134855 -0.337486  0.413057  1.205682 -1.366358 -0.096314   \n",
       "3080 -1.257781  0.298899 -0.860130  0.276141  1.046705 -0.745504 -0.342909   \n",
       "3081 -0.801286  0.418368 -0.866499 -0.064372  0.974501 -1.084798  0.472064   \n",
       "3082 -0.707997  0.675630 -0.233938  0.159180  1.000529 -1.665780  0.296066   \n",
       "\n",
       "           7         8         9    ...       758       759       760  \\\n",
       "0    -0.804608 -1.177580  0.156056  ... -0.562810 -0.411340  0.433298   \n",
       "1    -0.050963 -0.216489  1.010718  ...  0.426817 -0.219904 -0.014254   \n",
       "2    -0.201750 -0.855971  0.126374  ...  0.325378 -0.584822 -0.845940   \n",
       "3    -0.645881 -0.857121  0.474149  ... -0.987927 -0.638470 -0.161924   \n",
       "4     0.187629 -0.958486  0.335753  ... -0.653077 -0.492126  0.483244   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3078 -0.489165 -1.174777  0.081529  ... -0.239787 -0.378836 -0.029298   \n",
       "3079 -0.656512 -1.172672  0.480091  ... -0.972684 -0.425606  0.257879   \n",
       "3080 -0.462435 -1.735537  0.148792  ... -0.665857 -0.498027 -0.127219   \n",
       "3081 -0.299250 -0.953439  0.027430  ... -1.174034 -0.653110  0.092794   \n",
       "3082 -0.708535 -1.331825 -0.255058  ... -0.897987 -0.625595  0.092348   \n",
       "\n",
       "           761       762       763       764       765       766       767  \n",
       "0    -0.422068 -0.669982 -0.726443  0.106795  1.173453  0.316311  0.108925  \n",
       "1    -0.705548  0.285813 -0.596890 -0.212060  0.695832 -0.082648  0.383929  \n",
       "2    -0.637423 -0.245427  0.500610  0.191019  0.498423  0.164274 -0.312768  \n",
       "3    -0.339970 -0.626445 -0.612481  0.194379  1.441311 -0.297321  0.020321  \n",
       "4    -0.070685 -0.069700  0.078444  0.408898  1.241559 -0.151853 -0.196577  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3078 -0.404599 -0.395586 -0.504007 -0.001980  1.394140 -0.356526 -0.057205  \n",
       "3079 -0.083000 -0.208448 -0.519165  0.292847  1.117920  0.169528  0.234989  \n",
       "3080 -1.029894 -0.855351 -0.247067  0.405077  1.160195  0.243745  0.261609  \n",
       "3081 -0.495103 -0.692927 -0.327008  0.045146  1.080402  0.078887 -0.075638  \n",
       "3082 -0.311444  0.269258 -0.207657  0.134844  0.806955 -0.166361  0.017548  \n",
       "\n",
       "[3083 rows x 768 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Z_text_df = pd.DataFrame(Z_text_tensor.numpy())\n",
    "Z_text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP 處理數值資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "數值向量與語意向量已成功融合為 Z_full_tensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1978,  0.4914, -0.4858,  ..., -0.0843, -0.0798,  0.0808],\n",
       "        [ 0.2151,  1.0001, -0.0443,  ...,  0.3707, -0.0328, -0.3490],\n",
       "        [ 0.8874,  0.9524,  0.4683,  ...,  0.1201,  0.1975, -0.1585],\n",
       "        ...,\n",
       "        [-1.2578,  0.2989, -0.8601,  ...,  0.0864,  0.0967, -0.2644],\n",
       "        [-0.8013,  0.4184, -0.8665,  ...,  0.5446,  0.1440, -0.5102],\n",
       "        [-0.7080,  0.6756, -0.2339,  ...,  0.0632,  0.0072,  0.0233]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 假設你已載入 DataFrame 為 df，並有這些欄位\n",
    "numeric_cols = [\n",
    "    \"view_count\", \"followers_count\", \"emoji_count\", \"content_length\", \"post_hour\", \"time_elapsed_hours\",\n",
    "    \"has_photo\", \"like_count\", \"reply_count\", \"repost_count\", \"share_count\",\n",
    "    \"has_hashtag\", \"has_url\", \"has_mention\", \"has_exclaim\", \"has_question\"\n",
    "]\n",
    "\n",
    "# 布林欄位轉 int（保險起見）\n",
    "for col in numeric_cols:\n",
    "    if df_semantic[col].dtype == bool:\n",
    "        df_semantic[col] = df_semantic[col].astype(int)\n",
    "\n",
    "# 標準化數值特徵\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(df_semantic[numeric_cols])\n",
    "\n",
    "# 定義簡單的 MLP 模型\n",
    "class NumericMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 初始化與執行模型\n",
    "# 自動偵測你是否有 GPU（用 CUDA），否則就用 CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# 根據你數值欄位的數量設定 input_dim\n",
    "model_num = NumericMLP(input_dim=X_num_scaled.shape[1])\n",
    "# 把模型送到對應裝置（CPU / GPU）\n",
    "model_num.to(device)\n",
    "\n",
    "# 暫時關閉梯度運算，因沒有要訓練模型，僅讓資料過神經網路\n",
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_num_scaled, dtype=torch.float32).to(device)\n",
    "    Z_num_tensor = model_num(X_tensor).cpu()\n",
    "\n",
    "# 合併 Z_text 和 Z_num\n",
    "Z_text_tensor = torch.tensor(Z_text_df.values, dtype=torch.float32)  # [N, 768]\n",
    "Z_full_tensor = torch.cat([Z_text_tensor, Z_num_tensor], dim=1)      # [N, 896]\n",
    "\n",
    "print(\"數值向量與語意向量已成功融合為 Z_full_tensor\")\n",
    "Z_full_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9GcFmuNEsVn"
   },
   "source": [
    "## 清洗數據embbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5H23LR9aF0ny"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3083/3083 [00:00<00:00, 11189.63 examples/s]\n",
      "Map: 100%|██████████| 3083/3083 [01:23<00:00, 37.14 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 全部處理完成，已輸出 threads_with_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# =============== BERT 向量嵌入 ===============\n",
    "df = df.dropna(subset=['content']) #要先處理content空值才能embedding\n",
    "# --- 載入 tokenizer & model ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "# 選擇硬體設備（MPS、CUDA、CPU），自動判斷是否可用 GPU（M1/M2 晶片上的 MPS 或 CUDA），否則 fallback 到 CPU\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# --- 建立 HuggingFace Dataset ---\n",
    "hf_dataset = Dataset.from_pandas(df[[\"content\"]])\n",
    "\n",
    "# --- tokenize function ---\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['content'], truncation=True, padding='max_length', max_length=128)\n",
    "tokenized_dataset = hf_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# --- 取得 [CLS] 向量 ---\n",
    "def extract_embeddings(batch):\n",
    "    inputs = {k: torch.tensor(v).to(model.device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
    "    return {\"embeddings\": embeddings}\n",
    "\n",
    "# --- 批次轉換為 embeddings ---\n",
    "batch_size = 64\n",
    "embeddings_dataset = tokenized_dataset.map(extract_embeddings, batched=True, batch_size=batch_size)\n",
    "\n",
    "# =============== 匯出最終結果 ===============\n",
    "# embeddings_dataset[\"embeddings\"] 是 list of 768-dim vectors\n",
    "embedding_df = pd.DataFrame(embeddings_dataset[\"embeddings\"])\n",
    "final_df = pd.concat([df.reset_index(drop=True), embedding_df], axis=1)\n",
    "\n",
    "# 儲存\n",
    "# final_df.to_csv(\"C:/Users/User/Desktop/louis/threads_with_embeddings.csv\",encoding='utf_8_sig', index=False)\n",
    "final_df.to_csv(\"threads_with_embeddings.csv\",encoding='utf_8_sig', index=False)\n",
    "print(\"✅ 全部處理完成，已輸出 threads_with_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WxP6bcNEywY"
   },
   "source": [
    "## 分詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SovtoK069ox_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/59/tx1m33l955b0h85n6dlwm4440000gn/T/jieba.cache\n",
      "Loading model cost 0.499 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'考完 期中考 ， 成績 都 還沒出 來 ， 小一 女兒 就 自信 對 我 說 ： 「 我 真羨慕 妳 生 一個 天才 ！ 」'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = set(['的', '了', '在', '是', '和', '也', '與', '有', '為', '等'])\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    words = jieba.cut(text)\n",
    "    words_filtered = [word for word in words if word not in stopwords]\n",
    "    return ' '.join(words_filtered)\n",
    "\n",
    "\n",
    "df['processed_content'] = df['content'].apply(tokenize_and_remove_stopwords)\n",
    "df['processed_content'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwCtvYh4E3vU"
   },
   "source": [
    "## 機器學習建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eFdTaLb79ovp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 33538 stored elements and shape (3083, 12873)>\n",
      "  Coords\tValues\n",
      "  (0, 2799)\t0.4663056843152708\n",
      "  (0, 3188)\t0.2940455207159238\n",
      "  (0, 1306)\t0.44284097627197305\n",
      "  (0, 1210)\t0.31028579706458703\n",
      "  (0, 412)\t0.503121337625026\n",
      "  (0, 892)\t0.3880460720366238\n",
      "  (1, 9695)\t0.44774054192975266\n",
      "  (1, 10472)\t0.44774054192975266\n",
      "  (1, 8813)\t0.2568875443528821\n",
      "  (1, 3491)\t0.42597495668833046\n",
      "  (1, 10367)\t0.38876644295524665\n",
      "  (1, 9613)\t0.44774054192975266\n",
      "  (2, 10514)\t0.31144203947399207\n",
      "  (2, 8415)\t0.3288785193471119\n",
      "  (2, 7290)\t0.28537598711095685\n",
      "  (2, 12071)\t0.35868637857163266\n",
      "  (2, 6458)\t0.35868637857163266\n",
      "  (2, 6044)\t0.2990706601225911\n",
      "  (2, 10681)\t0.3288785193471119\n",
      "  (2, 9846)\t0.35868637857163266\n",
      "  (2, 3221)\t0.18120453315645962\n",
      "  (2, 5960)\t0.31144203947399207\n",
      "  (3, 1628)\t0.26164267502394156\n",
      "  (3, 538)\t0.23056465830230907\n",
      "  (3, 2699)\t0.3728698840922293\n",
      "  :\t:\n",
      "  (3080, 3034)\t0.4140307205152917\n",
      "  (3080, 3064)\t0.38762922360836444\n",
      "  (3080, 2872)\t0.36574251223930415\n",
      "  (3081, 1628)\t0.230329717220129\n",
      "  (3081, 692)\t0.2079266247071255\n",
      "  (3081, 2802)\t0.16248268778705707\n",
      "  (3081, 2801)\t0.21355771222235617\n",
      "  (3081, 1395)\t0.22662292335957387\n",
      "  (3081, 2812)\t0.23299934372416703\n",
      "  (3081, 1432)\t0.31634551298701585\n",
      "  (3081, 2872)\t0.25900164914242463\n",
      "  (3081, 2798)\t0.28767358106472024\n",
      "  (3081, 2289)\t0.34501744490931147\n",
      "  (3081, 1560)\t0.34501744490931147\n",
      "  (3081, 2975)\t0.34501744490931147\n",
      "  (3081, 1719)\t0.34501744490931147\n",
      "  (3082, 538)\t0.24991449565436066\n",
      "  (3082, 2827)\t0.2748480556731932\n",
      "  (3082, 2173)\t0.4041625013727274\n",
      "  (3082, 3058)\t0.2836006942769654\n",
      "  (3082, 2101)\t0.25158453952967386\n",
      "  (3082, 1169)\t0.4041625013727274\n",
      "  (3082, 1421)\t0.3221909718872812\n",
      "  (3082, 3166)\t0.3688592851939238\n",
      "  (3082, 2821)\t0.3895103428133763\n"
     ]
    }
   ],
   "source": [
    "# 計算 TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_content'])\n",
    "\n",
    "# 計算 TF\n",
    "tf_vectorizer = CountVectorizer()\n",
    "tf_matrix = tf_vectorizer.fit_transform(df['processed_content'])\n",
    "\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wvN8ioFE8Cz"
   },
   "source": [
    "# 多模型分類實驗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cROM4E4v9otD"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>content_length</th>\n",
       "      <th>lang</th>\n",
       "      <th>scrape_time</th>\n",
       "      <th>post_weekday</th>\n",
       "      <th>post_hour</th>\n",
       "      <th>post_period</th>\n",
       "      <th>viral</th>\n",
       "      <th>post_time</th>\n",
       "      <th>...</th>\n",
       "      <th>emojis</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>scrape_time_origin</th>\n",
       "      <th>has_question</th>\n",
       "      <th>has_exclaim</th>\n",
       "      <th>has_url</th>\n",
       "      <th>has_mention</th>\n",
       "      <th>has_hashtag</th>\n",
       "      <th>time_elapsed_hours</th>\n",
       "      <th>processed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>Thank you God for another day.</td>\n",
       "      <td>30</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 06:27</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T10:58:39.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:27:40.176749+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Thank   you   God   for   another   day .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>百達翡麗？ 沒有下限的網路病態！</td>\n",
       "      <td>16</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 06:27</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-29T12:33:44.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:27:54.964788+08:00</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>百達 翡麗 ？   沒有 下限 網路 病態 ！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ribboworld2021</td>\n",
       "      <td>考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」</td>\n",
       "      <td>40</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T04:39:46.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:09.873641+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "      <td>考完 期中考 ， 成績 都 還沒出 來 ， 小一 女兒 就 自信 對 我 說 ： 「 我 真...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ayofvr</td>\n",
       "      <td>Just be strong. Confident. Hopeful. Intellectu...</td>\n",
       "      <td>69</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T11:25:22.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:24.726576+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Just   be   strong .   Confident .   Hopeful ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jose_ykc</td>\n",
       "      <td>日文輔系老師上課內容之一AiScReam 歌詞導讀</td>\n",
       "      <td>25</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 06:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-29T10:36:19.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 06:28:39.393706+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>日文 輔系 老師 上 課內容 之一 AiScReam   歌詞 導讀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>leighton.williams</td>\n",
       "      <td>First tasting in California</td>\n",
       "      <td>27</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:23</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025年04月29日 07:30</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:23:40.328046+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8.0</td>\n",
       "      <td>First   tasting   in   California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>cape__man</td>\n",
       "      <td>I hoped it would have been better.</td>\n",
       "      <td>34</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:23</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025年04月29日 06:31</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:23:55.063517+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "      <td>I   hoped   it   would   have   been   better .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>simimoonlight</td>\n",
       "      <td>I can’t wait to watch Beyoncé on TikTok tonigh...</td>\n",
       "      <td>51</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:24</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025年04月29日 06:26</td>\n",
       "      <td>...</td>\n",
       "      <td>🫶🏿🥹</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-04-30 00:24:39.870994+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "      <td>I   can ’ t   wait   to   watch   Beyonc é   o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>other98</td>\n",
       "      <td>Tonight, Canada just proved that they have a h...</td>\n",
       "      <td>77</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:24</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025年04月29日 12:31</td>\n",
       "      <td>...</td>\n",
       "      <td>🙌</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-30 00:24:54.669936+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Tonight ,   Canada   just   proved   that   th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>scottiebeam</td>\n",
       "      <td>Yall be fighting on here … i thought threads w...</td>\n",
       "      <td>57</td>\n",
       "      <td>en</td>\n",
       "      <td>2025年04月30日 00:25</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>1</td>\n",
       "      <td>2025年04月29日 01:33</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-30 00:25:09.396585+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Yall   be   fighting   on   here   …   i   tho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3083 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                                            content  \\\n",
       "0                ayofvr                     Thank you God for another day.   \n",
       "1                   NaN                                   百達翡麗？ 沒有下限的網路病態！   \n",
       "2        ribboworld2021           考完期中考，成績都還沒出來，小一女兒就自信的對我說：「我真羨慕妳生了一個天才！」   \n",
       "3                ayofvr  Just be strong. Confident. Hopeful. Intellectu...   \n",
       "4              jose_ykc                          日文輔系老師上課內容之一AiScReam 歌詞導讀   \n",
       "...                 ...                                                ...   \n",
       "3096  leighton.williams                        First tasting in California   \n",
       "3097          cape__man                 I hoped it would have been better.   \n",
       "3098      simimoonlight  I can’t wait to watch Beyoncé on TikTok tonigh...   \n",
       "3099            other98  Tonight, Canada just proved that they have a h...   \n",
       "3100        scottiebeam  Yall be fighting on here … i thought threads w...   \n",
       "\n",
       "      content_length lang        scrape_time post_weekday  post_hour  \\\n",
       "0                 30   en  2025年04月30日 06:27    Wednesday          6   \n",
       "1                 16   Ch  2025年04月30日 06:27    Wednesday          6   \n",
       "2                 40   Ch  2025年04月30日 06:28    Wednesday          6   \n",
       "3                 69   en  2025年04月30日 06:28    Wednesday          6   \n",
       "4                 25   Ch  2025年04月30日 06:28    Wednesday          6   \n",
       "...              ...  ...                ...          ...        ...   \n",
       "3096              27   en  2025年04月30日 00:23    Wednesday          0   \n",
       "3097              34   en  2025年04月30日 00:23    Wednesday          0   \n",
       "3098              51   en  2025年04月30日 00:24    Wednesday          0   \n",
       "3099              77   en  2025年04月30日 00:24    Wednesday          0   \n",
       "3100              57   en  2025年04月30日 00:25    Wednesday          0   \n",
       "\n",
       "     post_period  viral                 post_time  ... emojis emoji_count  \\\n",
       "0        morning      0  2025-04-29T10:58:39.000Z  ...                  0   \n",
       "1        morning      1  2025-04-29T12:33:44.000Z  ...                  0   \n",
       "2        morning      0  2025-04-29T04:39:46.000Z  ...                  0   \n",
       "3        morning      0  2025-04-29T11:25:22.000Z  ...                  0   \n",
       "4        morning      1  2025-04-29T10:36:19.000Z  ...                  0   \n",
       "...          ...    ...                       ...  ...    ...         ...   \n",
       "3096       night      0         2025年04月29日 07:30  ...                  0   \n",
       "3097       night      0         2025年04月29日 06:31  ...                  0   \n",
       "3098       night      1         2025年04月29日 06:26  ...    🫶🏿🥹           3   \n",
       "3099       night      1         2025年04月29日 12:31  ...      🙌           1   \n",
       "3100       night      1         2025年04月29日 01:33  ...                  0   \n",
       "\n",
       "                   scrape_time_origin  has_question  has_exclaim  has_url  \\\n",
       "0    2025-04-30 06:27:40.176749+08:00         False        False    False   \n",
       "1    2025-04-30 06:27:54.964788+08:00          True         True    False   \n",
       "2    2025-04-30 06:28:09.873641+08:00         False         True    False   \n",
       "3    2025-04-30 06:28:24.726576+08:00         False        False    False   \n",
       "4    2025-04-30 06:28:39.393706+08:00         False        False    False   \n",
       "...                               ...           ...          ...      ...   \n",
       "3096 2025-04-30 00:23:40.328046+08:00         False        False    False   \n",
       "3097 2025-04-30 00:23:55.063517+08:00         False        False    False   \n",
       "3098 2025-04-30 00:24:39.870994+08:00         False        False    False   \n",
       "3099 2025-04-30 00:24:54.669936+08:00         False         True    False   \n",
       "3100 2025-04-30 00:25:09.396585+08:00         False        False    False   \n",
       "\n",
       "      has_mention  has_hashtag  time_elapsed_hours  \\\n",
       "0           False        False                 3.0   \n",
       "1           False        False                 1.0   \n",
       "2           False        False                 9.0   \n",
       "3           False        False                 3.0   \n",
       "4           False        False                 3.0   \n",
       "...           ...          ...                 ...   \n",
       "3096        False        False                 8.0   \n",
       "3097        False        False                 9.0   \n",
       "3098        False        False                 9.0   \n",
       "3099        False        False                 3.0   \n",
       "3100        False        False                14.0   \n",
       "\n",
       "                                      processed_content  \n",
       "0             Thank   you   God   for   another   day .  \n",
       "1                               百達 翡麗 ？   沒有 下限 網路 病態 ！  \n",
       "2     考完 期中考 ， 成績 都 還沒出 來 ， 小一 女兒 就 自信 對 我 說 ： 「 我 真...  \n",
       "3     Just   be   strong .   Confident .   Hopeful ....  \n",
       "4                    日文 輔系 老師 上 課內容 之一 AiScReam   歌詞 導讀  \n",
       "...                                                 ...  \n",
       "3096                  First   tasting   in   California  \n",
       "3097    I   hoped   it   would   have   been   better .  \n",
       "3098  I   can ’ t   wait   to   watch   Beyonc é   o...  \n",
       "3099  Tonight ,   Canada   just   proved   that   th...  \n",
       "3100  Yall   be   fighting   on   here   …   i   tho...  \n",
       "\n",
       "[3083 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, random_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 8\n",
    "num_epochs = 5\n",
    "# ========== 參數設定 ==========\n",
    "model_tokenizer_map = {\n",
    "    \"FusionMacBERT\": \"hfl/chinese-macbert-base\",\n",
    "    \"PureMacBERT\": \"hfl/chinese-macbert-base\",\n",
    "    \"NumericOnly\": None,\n",
    "    \"BiLSTMWithNumeric\": \"bert-base-chinese\",\n",
    "    \"MacBERTWithGRU\": \"hfl/chinese-macbert-base\",\n",
    "    \"MacBERTMLPFusion\": \"hfl/chinese-macbert-base\",\n",
    "    \"TextCNNMacBERT\": \"hfl/chinese-macbert-base\",\n",
    "    \"RoBERTa\": \"hfl/chinese-roberta-wwm-ext\",\n",
    "    \"BERTwwmExt\": \"hfl/chinese-bert-wwm-ext\",\n",
    "    \"ERNIE\": \"nghuyong/ernie-3.0-base-zh\",\n",
    "    \"ConvBERT\": \"YituTech/conv-bert-base\"\n",
    "}\n",
    "\n",
    "#tokenizer\n",
    "default_tokenizer_name = model_tokenizer_map[\"FusionMacBERT\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(default_tokenizer_name)\n",
    "\n",
    "#載入資料\n",
    "# df = pd.read_csv(\"C:/Users/User/Desktop/louis/threads_cleaned_v2.csv\", encoding='utf_8_sig')\n",
    "#df = df.dropna(subset=['content', 'view_count']).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibbctyH9CUzu"
   },
   "source": [
    "## Label 分群 （1000以下、1000~10000、10000~100000、100000以上)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5yL13rSfCUzu"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>content_length</th>\n",
       "      <th>lang</th>\n",
       "      <th>scrape_time</th>\n",
       "      <th>post_weekday</th>\n",
       "      <th>post_hour</th>\n",
       "      <th>post_period</th>\n",
       "      <th>viral</th>\n",
       "      <th>post_time</th>\n",
       "      <th>...</th>\n",
       "      <th>scrape_time_origin</th>\n",
       "      <th>has_question</th>\n",
       "      <th>has_exclaim</th>\n",
       "      <th>has_url</th>\n",
       "      <th>has_mention</th>\n",
       "      <th>has_hashtag</th>\n",
       "      <th>time_elapsed_hours</th>\n",
       "      <th>processed_content</th>\n",
       "      <th>view_class</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_jili.25</td>\n",
       "      <td>我要讓掉原本的才能去vip啊啊啊啊😭😭 1</td>\n",
       "      <td>21</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 08:28</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>8</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T06:48:05.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-04-30 08:28:10.692106+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "      <td>我要 讓 掉 原本 才能 去 vip 啊啊啊 啊 😭 😭   1</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lv_auer</td>\n",
       "      <td>拔完智齒第三天了，到底要痛幾天，我都快不能吃東西了😂😂 #哦藉機減肥</td>\n",
       "      <td>34</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 08:07</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>8</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T10:55:49.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-04-30 08:07:36.828332+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>拔 完智齒 第三天 ， 到底 要痛 幾天 ， 我 都 快 不能 吃 東西 😂 😂   # 哦...</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cai.7052</td>\n",
       "      <td>如果有下輩子的話，我一定要當女生，當男生太累太苦了</td>\n",
       "      <td>25</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年05月03日 19:30</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>19</td>\n",
       "      <td>evening</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-03T01:07:29.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-05-03 19:30:28.096090+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>如果 下輩子 話 ， 我 一定 要當 女生 ， 當 男生 太累 太苦</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brandin.podziemski2</td>\n",
       "      <td>拍得我真帥</td>\n",
       "      <td>5</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年05月03日 19:55</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>19</td>\n",
       "      <td>evening</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-03T00:07:39.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-05-03 19:55:44.483391+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>拍得 我 真帥</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emo_97_10_31</td>\n",
       "      <td>🫤在脆上面真的可以找到真心的朋友嗎</td>\n",
       "      <td>17</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 07:50</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>7</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-29T11:39:04.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-04-30 07:50:25.230620+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "      <td>🫤 脆 上面 真的 可以 找到 真心 朋友 嗎</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11342</th>\n",
       "      <td>goooosleep</td>\n",
       "      <td>为什么蝴蝶要飞到十六楼？</td>\n",
       "      <td>12</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月30日 00:08</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025年04月29日 12:03</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-04-30 00:08:40.964074+08:00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "      <td>为什么 蝴蝶 要 飞到 十六 楼 ？</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11343</th>\n",
       "      <td>kickflip_said</td>\n",
       "      <td>我現在看到大家買到票還有互報說VIP區還有票的串文就是一直在破防 辦在期末考前一週我真的要哭出來</td>\n",
       "      <td>48</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月29日 07:24</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>7</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025年04月28日 18:47</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-04-29 07:24:48.150110+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "      <td>我現 看到 大家 買到 票 還有 互報 說 VIP 區還 有票 串文 就是 一直 破防   ...</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11344</th>\n",
       "      <td>lucasbiubiubiubiu</td>\n",
       "      <td>京东外卖真的是一次都没准时过，不是早半个小时就是晚半个小时😡</td>\n",
       "      <td>30</td>\n",
       "      <td>Ch</td>\n",
       "      <td>2025年04月29日 07:14</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>7</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025年04月28日 11:27</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-04-29 07:14:37.311967+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>11.0</td>\n",
       "      <td>京东 外卖 真的 一次 都 没准 时过 ， 不是 早 半个 小时 就是 晚 半个 小时 😡</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11345</th>\n",
       "      <td>clydecole620</td>\n",
       "      <td>Malik Beasley. Michael Ealy. Pop a wheelie. Kn...</td>\n",
       "      <td>56</td>\n",
       "      <td>tl</td>\n",
       "      <td>2025年05月03日 08:00</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>8</td>\n",
       "      <td>morning</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-02T15:29:08.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-05-03 08:00:53.883370+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Malik   Beasley .   Michael   Ealy .   Pop   a...</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11346</th>\n",
       "      <td>hflyizgt</td>\n",
       "      <td>s屬性爆發 shinonome akito</td>\n",
       "      <td>21</td>\n",
       "      <td>st</td>\n",
       "      <td>2025年05月02日 01:38</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-01T09:06:59.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-05-02 01:38:12.735659+08:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>s 屬性 爆發   shinonome   akito</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11347 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    author                                            content  \\\n",
       "0                 _jili.25                              我要讓掉原本的才能去vip啊啊啊啊😭😭 1   \n",
       "1                  lv_auer                 拔完智齒第三天了，到底要痛幾天，我都快不能吃東西了😂😂 #哦藉機減肥   \n",
       "2                 cai.7052                          如果有下輩子的話，我一定要當女生，當男生太累太苦了   \n",
       "3      brandin.podziemski2                                              拍得我真帥   \n",
       "4             emo_97_10_31                                  🫤在脆上面真的可以找到真心的朋友嗎   \n",
       "...                    ...                                                ...   \n",
       "11342           goooosleep                                       为什么蝴蝶要飞到十六楼？   \n",
       "11343        kickflip_said   我現在看到大家買到票還有互報說VIP區還有票的串文就是一直在破防 辦在期末考前一週我真的要哭出來   \n",
       "11344    lucasbiubiubiubiu                     京东外卖真的是一次都没准时过，不是早半个小时就是晚半个小时😡   \n",
       "11345         clydecole620  Malik Beasley. Michael Ealy. Pop a wheelie. Kn...   \n",
       "11346             hflyizgt                              s屬性爆發 shinonome akito   \n",
       "\n",
       "       content_length lang        scrape_time post_weekday  post_hour  \\\n",
       "0                  21   Ch  2025年04月30日 08:28    Wednesday          8   \n",
       "1                  34   Ch  2025年04月30日 08:07    Wednesday          8   \n",
       "2                  25   Ch  2025年05月03日 19:30     Saturday         19   \n",
       "3                   5   Ch  2025年05月03日 19:55     Saturday         19   \n",
       "4                  17   Ch  2025年04月30日 07:50    Wednesday          7   \n",
       "...               ...  ...                ...          ...        ...   \n",
       "11342              12   Ch  2025年04月30日 00:08    Wednesday          0   \n",
       "11343              48   Ch  2025年04月29日 07:24      Tuesday          7   \n",
       "11344              30   Ch  2025年04月29日 07:14      Tuesday          7   \n",
       "11345              56   tl  2025年05月03日 08:00     Saturday          8   \n",
       "11346              21   st  2025年05月02日 01:38       Friday          1   \n",
       "\n",
       "      post_period  viral                 post_time  ...  \\\n",
       "0         morning      0  2025-04-29T06:48:05.000Z  ...   \n",
       "1         morning      0  2025-04-29T10:55:49.000Z  ...   \n",
       "2         evening      0  2025-05-03T01:07:29.000Z  ...   \n",
       "3         evening      0  2025-05-03T00:07:39.000Z  ...   \n",
       "4         morning      0  2025-04-29T11:39:04.000Z  ...   \n",
       "...           ...    ...                       ...  ...   \n",
       "11342       night      0         2025年04月29日 12:03  ...   \n",
       "11343     morning      0         2025年04月28日 18:47  ...   \n",
       "11344     morning      0         2025年04月28日 11:27  ...   \n",
       "11345     morning      0  2025-05-02T15:29:08.000Z  ...   \n",
       "11346       night      0  2025-05-01T09:06:59.000Z  ...   \n",
       "\n",
       "                    scrape_time_origin has_question  has_exclaim  has_url  \\\n",
       "0     2025-04-30 08:28:10.692106+08:00        False        False    False   \n",
       "1     2025-04-30 08:07:36.828332+08:00        False        False    False   \n",
       "2     2025-05-03 19:30:28.096090+08:00        False        False    False   \n",
       "3     2025-05-03 19:55:44.483391+08:00        False        False    False   \n",
       "4     2025-04-30 07:50:25.230620+08:00        False        False    False   \n",
       "...                                ...          ...          ...      ...   \n",
       "11342 2025-04-30 00:08:40.964074+08:00         True        False    False   \n",
       "11343 2025-04-29 07:24:48.150110+08:00        False        False    False   \n",
       "11344 2025-04-29 07:14:37.311967+08:00        False        False    False   \n",
       "11345 2025-05-03 08:00:53.883370+08:00        False        False    False   \n",
       "11346 2025-05-02 01:38:12.735659+08:00        False        False    False   \n",
       "\n",
       "       has_mention  has_hashtag  time_elapsed_hours  \\\n",
       "0            False        False                 9.0   \n",
       "1            False         True                 5.0   \n",
       "2            False        False                 2.0   \n",
       "3            False        False                 3.0   \n",
       "4            False        False                 4.0   \n",
       "...            ...          ...                 ...   \n",
       "11342        False        False                 4.0   \n",
       "11343        False        False                 4.0   \n",
       "11344        False        False                11.0   \n",
       "11345        False        False                 0.5   \n",
       "11346        False        False                 0.5   \n",
       "\n",
       "                                       processed_content  view_class  label  \n",
       "0                       我要 讓 掉 原本 才能 去 vip 啊啊啊 啊 😭 😭   1         low      1  \n",
       "1      拔 完智齒 第三天 ， 到底 要痛 幾天 ， 我 都 快 不能 吃 東西 😂 😂   # 哦...         low      1  \n",
       "2                     如果 下輩子 話 ， 我 一定 要當 女生 ， 當 男生 太累 太苦         low      1  \n",
       "3                                                拍得 我 真帥         low      1  \n",
       "4                                🫤 脆 上面 真的 可以 找到 真心 朋友 嗎         low      1  \n",
       "...                                                  ...         ...    ...  \n",
       "11342                                 为什么 蝴蝶 要 飞到 十六 楼 ？         low      1  \n",
       "11343  我現 看到 大家 買到 票 還有 互報 說 VIP 區還 有票 串文 就是 一直 破防   ...         low      1  \n",
       "11344      京东 外卖 真的 一次 都 没准 时过 ， 不是 早 半个 小时 就是 晚 半个 小时 😡         low      1  \n",
       "11345  Malik   Beasley .   Michael   Ealy .   Pop   a...         low      1  \n",
       "11346                        s 屬性 爆發   shinonome   akito         low      1  \n",
       "\n",
       "[11347 rows x 33 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將 view_count 分成四類：\n",
    "# 0: 小於 1000\n",
    "# 1: 1000 ~ 9999\n",
    "# 2: 10000 ~ 99999\n",
    "# 3: 100000 以上\n",
    "\n",
    "def map_view_class(x):\n",
    "    if x < 1000:\n",
    "        return 'low'\n",
    "    elif x < 10000:\n",
    "        return 'medium'\n",
    "    elif x < 100000:\n",
    "        return 'high'\n",
    "    else:\n",
    "        return 'very_high'\n",
    "\n",
    "df['view_class'] = df['view_count'].apply(map_view_class)\n",
    "\n",
    "# 編碼成數字 label\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['view_class'])\n",
    "\n",
    "df_high = df[df['view_class'] == 'very_high']\n",
    "df_medium = df[df['view_class'] == 'high']\n",
    "df_low = df[df['view_class'] == 'medium']\n",
    "df_very_low = df[df['view_class'] == 'low']\n",
    "\n",
    "# 針對較少的類別進行擴增（假設 high 和 very_low 比較少）\n",
    "df_high_oversampled = pd.concat([df_high] * 3, ignore_index=True)\n",
    "df_very_low_oversampled = pd.concat([df_very_low] * 3, ignore_index=True)\n",
    "\n",
    "# 合併並打亂\n",
    "df = pd.concat([df_medium, df_low, df_high_oversampled, df_very_low_oversampled], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeqEslof2OBB"
   },
   "source": [
    "# Normalization 數值特徵標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "N3pJMu-g9on2"
   },
   "outputs": [],
   "source": [
    "# Normalization 數值特徵標準化\n",
    "base_num_cols = ['like_count', 'share_count', 'repost_count', 'reply_count', 'emoji_count', 'has_photo', 'has_video', 'has_question', 'has_exclaim', 'has_mention', 'has_url', 'has_hashtag', 'content_length']\n",
    "# 找出 one-hot 編碼的欄位（語言類型、發文時段、星期幾等類別欄位）\n",
    "# 使用 StandardScaler 將數值欄位轉換為「標準常態分布」（mean=0, std=1），有助於模型學習穩定。\n",
    "onehot_cols = [col for col in df.columns if col.startswith('lang_') or col.startswith('post_period_') or col.startswith('post_weekday_')]\n",
    "num_cols = base_num_cols + onehot_cols\n",
    "scaler = StandardScaler()\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "# Tokenizer 文本編碼 ：使用事先定義好的 tokenizer（例如 MacBERT、RoBERTa）對貼文進行斷詞、編碼\n",
    "# 將編碼後的結果儲存到 df 中，這兩個欄位會作為 BERT 模型的輸入\n",
    "encodings = tokenizer(df['content'].tolist(), truncation=True, padding='max_length', max_length=128)\n",
    "df['input_ids'] = encodings['input_ids']  # 斷詞後對應的詞彙 ID\n",
    "df['attention_mask'] = encodings['attention_mask']  # 對應位置是否是 padding（0）或實際內容（1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "j32CLLcICUzu"
   },
   "outputs": [],
   "source": [
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df['input_ids'].tolist()\n",
    "        self.attention_mask = df['attention_mask'].tolist()\n",
    "        self.numerics = df[num_cols].values\n",
    "        self.targets = df['target'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'numerics': torch.tensor(self.numerics[idx], dtype=torch.float),\n",
    "            'target': torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsA6q_qf9ojJ"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "4OBPv79a10W0"
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df['input_ids'].tolist()\n",
    "        self.attention_mask = df['attention_mask'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.numerics = df[num_cols].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'numerics': torch.tensor(self.numerics[idx], dtype=torch.float),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_iVSfG82Gpt"
   },
   "source": [
    "#模型架構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "um-YKfVt10Uf"
   },
   "outputs": [],
   "source": [
    "#模型架構\n",
    "# 1. FusionMacBERT：BERT + 數值特徵 concat\n",
    "class FusionMacBERTModel(nn.Module):\n",
    "    def __init__(self, model_name, num_numeric_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size + 64, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        cls_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        num_out = torch.relu(self.numeric_fc(numerics))\n",
    "        combined = torch.cat((cls_output, num_out), dim=1)\n",
    "        return self.classifier(self.dropout(combined))\n",
    "\n",
    "# 2. PureMacBERT：只有文字\n",
    "class PureMacBERTModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics=None):\n",
    "        cls_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        return self.classifier(self.dropout(cls_output))\n",
    "\n",
    "# 3. NumericOnly：只有數值特徵\n",
    "class NumericOnlyModel(nn.Module):\n",
    "    def __init__(self, num_numeric_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_numeric_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, numerics=None):\n",
    "        return self.classifier(numerics)\n",
    "\n",
    "# 4. BiLSTMWithNumeric：LSTM 處理詞嵌入 + 數值特徵\n",
    "class BiLSTMWithNumeric(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_numeric_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2 + 64, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        x = self.embedding(input_ids)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        pooled = lstm_out[:, -1, :]\n",
    "        num_out = torch.relu(self.numeric_fc(numerics))\n",
    "        combined = torch.cat((pooled, num_out), dim=1)\n",
    "        return self.classifier(self.dropout(combined))\n",
    "\n",
    "# 5. MacBERTWithGRU：BERT + GRU + 數值特徵\n",
    "class MacBERTWithGRU(nn.Module):\n",
    "    def __init__(self, model_name, num_numeric_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.gru = nn.GRU(self.bert.config.hidden_size, 128, batch_first=True, bidirectional=True)\n",
    "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
    "        self.classifier = nn.Linear(128*2 + 64, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        gru_out, _ = self.gru(bert_out)\n",
    "        pooled = gru_out[:, -1, :]\n",
    "        num_out = torch.relu(self.numeric_fc(numerics))\n",
    "        combined = torch.cat((pooled, num_out), dim=1)\n",
    "        return self.classifier(self.dropout(combined))\n",
    "\n",
    "# 6. MacBERTMLPFusion：BERT + 數值特徵 -> MLP\n",
    "class MacBERTMLPFusion(nn.Module):\n",
    "    def __init__(self, model_name, num_numeric_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size + num_numeric_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        cls_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        combined = torch.cat((cls_output, numerics), dim=1)\n",
    "        return self.fc(combined)\n",
    "\n",
    "# 7. TextCNNMacBERT：BERT 輸出卷積後 + 數值特徵\n",
    "class TextCNNMacBERT(nn.Module):\n",
    "    def __init__(self, model_name, num_numeric_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, 64, (k, self.bert.config.hidden_size)) for k in [2, 3, 4]])\n",
    "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
    "        self.classifier = nn.Linear(64 * len([2, 3, 4]) + 64, num_classes)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = torch.relu(conv(x)).squeeze(3)\n",
    "        x = torch.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        x = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state.unsqueeze(1)\n",
    "        cnn_out = torch.cat([self.conv_and_pool(x, conv) for conv in self.convs], 1)\n",
    "        num_out = torch.relu(self.numeric_fc(numerics))\n",
    "        combined = torch.cat((cnn_out, num_out), dim=1)\n",
    "        return self.classifier(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "De3g_-Qs2Vux"
   },
   "outputs": [],
   "source": [
    "#訓練與評估\n",
    "def train_and_eval(model, name, preview_count=10):\n",
    "    model = model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    loss_fn = FocalLoss()\n",
    "    # loss_fn = nn.MSELoss()\n",
    "\n",
    "    # 訓練階段\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            output = model(input_ids, attention_mask, numerics)\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # 評估階段\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            output = model(input_ids, attention_mask, numerics)\n",
    "            preds = output.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    mae = mean_absolute_error(all_targets, all_preds)\n",
    "    print(f\"[{name} 評估結果] MSE: {mse:.2f} | MAE: {mae:.2f}\")\n",
    "    print(f\"\\n{name} 評估結果：\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "    '''\n",
    "    preview_shown = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            output = model(input_ids, attention_mask, numerics)\n",
    "            preds = output.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            #印出前幾筆的預測、真實值\n",
    "            if preview_shown < preview_count:\n",
    "                batch_size = input_ids.shape[0]\n",
    "                for i in range(batch_size):\n",
    "                    if preview_shown >= preview_count:\n",
    "                        break\n",
    "                    input_id = input_ids[i].cpu().numpy()\n",
    "                    text = tokenizer.decode(input_id, skip_special_tokens=True)\n",
    "                    print(f\"\\n[{name} 預測] 第 {preview_shown+1} 筆\")\n",
    "                    print(f\"Text: {text}\")\n",
    "                    print(f\"Predicted: {label_encoder.inverse_transform([preds[i]])[0]}\")\n",
    "                    print(f\"Actual:    {label_encoder.inverse_transform([labels[i].cpu().item()])[0]}\")\n",
    "                    preview_shown += 1\n",
    "\n",
    "    print(f\"\\n{name} 評估結果：\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wqejxS910SJ"
   },
   "outputs": [],
   "source": [
    "# 資料分割：資料集切分與取樣\n",
    "dataset = CustomDataset(df)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, len(dataset)-train_size])\n",
    "train_labels = [train_dataset[i]['labels'].item() for i in range(len(train_dataset))]\n",
    "class_counts = pd.Series(train_labels).value_counts().to_dict()\n",
    "weights = [1.0 / class_counts[label] for label in train_labels]\n",
    "sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, sampler=sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# 執行多模型訓練\n",
    "model_variants = {\n",
    "    # \"FusionMacRegressor\": FusionMacBERTRegressor(\"hfl/chinese-macbert-base\", len(num_cols), 3),\n",
    "    \"FusionMacBERT\": FusionMacBERTModel(\"hfl/chinese-macbert-base\", len(num_cols), 4),\n",
    "    \"PureMacBERT\": PureMacBERTModel(\"hfl/chinese-macbert-base\", 4),\n",
    "    \"NumericOnly\": NumericOnlyModel(len(num_cols), 4),\n",
    "    \"BiLSTMWithNumeric\": BiLSTMWithNumeric(tokenizer.vocab_size, 128, 128, len(num_cols), 4),\n",
    "    \"MacBERTWithGRU\": MacBERTWithGRU(\"hfl/chinese-macbert-base\", len(num_cols), 4),\n",
    "    \"MacBERTMLPFusion\": MacBERTMLPFusion(\"hfl/chinese-macbert-base\", len(num_cols), 4),\n",
    "    \"TextCNNMacBERT\": TextCNNMacBERT(\"hfl/chinese-macbert-base\", len(num_cols), 4),\n",
    "    \"RoBERTa\": FusionMacBERTModel(\"hfl/chinese-roberta-wwm-ext\", len(num_cols), 4),\n",
    "    \"BERTwwmExt\": FusionMacBERTModel(\"hfl/chinese-bert-wwm-ext\", len(num_cols), 4),\n",
    "    \"ERNIE\": FusionMacBERTModel(\"nghuyong/ernie-3.0-base-zh\", len(num_cols), 4),\n",
    "    \"ConvBERT\": FusionMacBERTModel(\"YituTech/conv-bert-base\", len(num_cols), 4)\n",
    "}\n",
    "\n",
    "# 逐個模型訓練與輸出結果\n",
    "for name, model in model_variants.items():\n",
    "    tokenizer_name = model_tokenizer_map.get(name, default_tokenizer_name)\n",
    "    if tokenizer_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        encodings = tokenizer(df['content'].tolist(), truncation=True, padding='max_length', max_length=128)\n",
    "        df['input_ids'] = encodings['input_ids']\n",
    "        df['attention_mask'] = encodings['attention_mask']\n",
    "    train_and_eval(model, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WP2MxFtLUhR"
   },
   "source": [
    "1. FusionMacBERT ✅文字 + ✅數值\n",
    "BERT： 使用 MacBERT\n",
    "架構： 把 [CLS] 向量與數值特徵經過 MLP 融合\n",
    "用途： 做為 baseline 融合模型\n",
    "優點： 同時考慮內容語義與貼文統計資料（如按讚數、是否有 hashtag）\n",
    "\n",
    "2. PureMacBERT ✅文字 + ❌數值\n",
    "BERT： 使用 MacBERT\n",
    "架構： 單純使用 [CLS]，後接 linear 層分類\n",
    "用途： 純語言模型 baseline\n",
    "對照： 可用來比較是否有數值輔助提升效果\n",
    "\n",
    "3. NumericOnly ❌文字 + ✅數值\n",
    "模型類型： 只有數值輸入，經過 MLP 做分類\n",
    "用途： 測試「只靠貼文統計資料」能否達到合理分類\n",
    "對照： 可與文字模型或融合模型對比效果\n",
    "\n",
    "4. BiLSTMWithNumeric ✅文字（Embedding+LSTM）+ ✅數值\n",
    "嵌入方式： 使用 nn.Embedding + BiLSTM 處理文字（不是 BERT）\n",
    "融合方式： 將 LSTM 最後時間步 + 數值特徵拼接\n",
    "特別點： 測試「非 Transformer 模型」是否仍具競爭力\n",
    "\n",
    "5. MacBERTWithGRU ✅文字（MacBERT）+ ✅數值\n",
    "文字處理： MacBERT 之後再串 GRU\n",
    "融合方式： GRU 輸出最後一步拼接數值特徵\n",
    "意圖： 想看 BERT+RNN 的表現 vs. 傳統 BERT\n",
    "\n",
    "6. MacBERTMLPFusion ✅文字 + ✅數值\n",
    "處理方式： 文字與數值直接拼接後進入 MLP\n",
    "不同於 FusionMacBERT：\n",
    "沒有額外處理數值特徵（如沒有經過 nn.Linear)\n",
    "更單純的融合設計（屬於 Early Fusion）\n",
    "\n",
    "7. TextCNNMacBERT ✅文字 + ✅數值\n",
    "模型組合：\n",
    "使用 BERT 編碼後丟進 CNN filter (TextCNN)\n",
    "再與數值特徵融合\n",
    "用途： 測試 BERT 結合 CNN 特徵提取是否提升效果\n",
    "有趣點： 有些短文模型（如微博、Threads）對 CNN 特徵抓取敏感\n",
    "\n",
    "8. RoBERTa ✅文字 + ✅數值\n",
    "BERT 替代品： 改用 RoBERTa（中文版本）\n",
    "融合方式： 同 FusionMacBERT\n",
    "實驗目的： 測試不同語言模型對結果的影響（語言模型 ablation）\n",
    "\n",
    "\n",
    "9. BERTwwmExt ✅文字 + ✅數值\n",
    "BERT： 使用 Chinese BERT whole-word-masking 擴展版\n",
    "比較目的： 同上，用於測試不同語言模型特性的影響\n",
    "\n",
    "10. ERNIE ✅文字 + ✅數值\n",
    "BERT： 改用百度的 ERNIE（引入知識增強）\n",
    "適用場景： 當文本與常識有關（如話題、用語）\n",
    "目的： 評估知識型語言模型在社群文本分類的效果\n",
    "\n",
    "11. ConvBERT ✅文字 + ✅數值\n",
    "模型特色： 使用 Convolution + Self-Attention 混合架構的 BERT\n",
    "實驗意義： 試驗非傳統 Self-Attention 模型是否有優勢\n",
    "\n",
    "\n",
    "| 模型名稱              | 說明               | 是否融合 | 文本處理法         | 特殊處理       |\n",
    "| ----------------- | ---------------- | ---- | ------------- | ---------- |\n",
    "| FusionMacBERT     | BERT + 數值特徵      | ✅    | MacBERT       | 自製融合層      |\n",
    "| PureMacBERT       | 純文本模型            | ❌    | MacBERT       | baseline   |\n",
    "| NumericOnly       | 純統計數值            | ❌    | 無             | MLP only   |\n",
    "| BiLSTMWithNumeric | LSTM + 數值        | ✅    | nn.Embedding  | 不使用 BERT   |\n",
    "| MacBERTWithGRU    | BERT + GRU + 數值  | ✅    | MacBERT + GRU | 時序特徵強化     |\n",
    "| MacBERTMLPFusion  | BERT + 數值        | ✅    | MacBERT       | 拼接後進 MLP   |\n",
    "| TextCNNMacBERT    | BERT + CNN + 數值  | ✅    | MacBERT + CNN | 模仿 TextCNN |\n",
    "| RoBERTa           | 換 BERT backbone  | ✅    | RoBERTa       | 模型比較       |\n",
    "| BERTwwmExt        | 換 BERT backbone  | ✅    | BERT-wwm      | 模型比較       |\n",
    "| ERNIE             | 引入知識的 BERT       | ✅    | ERNIE         | 模型比較       |\n",
    "| ConvBERT          | 混合卷積 + 注意力的 BERT | ✅    | ConvBERT      | 模型比較       |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapley value analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_numeric_shap(model, df, num_cols, sample_count=200, background_count=100):\n",
    "    model.eval().cpu()\n",
    "    X = df[num_cols].values.astype(np.float32)\n",
    "\n",
    "    def model_forward(x_numpy):\n",
    "        x_tensor = torch.tensor(x_numpy, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(numerics=x_tensor)\n",
    "            return outputs.numpy()\n",
    "\n",
    "    explainer = shap.Explainer(model_forward, shap.sample(X, background_count))\n",
    "    shap_values = explainer(X[:sample_count])\n",
    "\n",
    "    print(\"🔍 特徵平均貢獻值 (bar chart):\")\n",
    "    shap.plots.bar(shap_values)\n",
    "\n",
    "    print(\"🐝 特徵影響分布 (beeswarm plot):\")\n",
    "    shap.plots.beeswarm(shap_values)\n",
    "\n",
    "    return shap_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericBranchWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.numeric_fc = model.numeric_fc\n",
    "        self.classifier = model.classifier\n",
    "        self.hidden_size = model.bert.config.hidden_size\n",
    "        self.dummy_bert = torch.zeros((1, self.hidden_size))  # 假裝 CLS 向量是 0\n",
    "\n",
    "    def forward(self, numerics):\n",
    "        num_out = torch.relu(self.numeric_fc(numerics))\n",
    "        dummy_bert = self.dummy_bert.expand(numerics.size(0), -1)\n",
    "        combined = torch.cat((dummy_bert, num_out), dim=1)\n",
    "        return self.classifier(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 指定支援 SHAP 的模型名稱清單\n",
    "shap_supported_models = [\"NumericOnly\", \"FusionMacBERT\", \"MacBERTMLPFusion\"]\n",
    "\n",
    "for name, model in model_variants.items():\n",
    "    if name in shap_supported_models:\n",
    "        print(f\"\\n📊 分析模型：{name}\")\n",
    "\n",
    "        # 如果是需要包裝的 MacBERT 融合類型\n",
    "        if name in [\"FusionMacBERT\", \"MacBERTMLPFusion\"]:\n",
    "            wrapper_model = NumericBranchWrapper(model)\n",
    "            shap_vals = analyze_numeric_shap(wrapper_model, df, num_cols)\n",
    "        else:\n",
    "            # 否則就是 NumericOnly 可以直接用\n",
    "            shap_vals = analyze_numeric_shap(model, df, num_cols)\n",
    "\n",
    "        # ✅（選配）儲存圖表：如需存圖檔可解除註解\n",
    "        \"\"\"\n",
    "        plt.figure()\n",
    "        shap.plots.bar(shap_vals, show=False)\n",
    "        plt.savefig(f\"shap_bar_{name}.png\", bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        shap.plots.beeswarm(shap_vals, show=False)\n",
    "        plt.savefig(f\"shap_beeswarm_{name}.png\", bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sd8HLHR6VL1"
   },
   "source": [
    "# 迴歸預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "291iyZhHxpj9"
   },
   "outputs": [],
   "source": [
    "# 原本這樣分類（要拿掉）\n",
    "# df['view_class'] = ...\n",
    "# df['label'] = ...\n",
    "\n",
    "# 直接用原始 view_count 作為 regression target\n",
    "df = df.dropna(subset=[\"content\", \"view_count\"])\n",
    "df['target'] = df['view_count'].apply(parse_count)  # 如果 view_count 不是數字要先轉換\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "dedliSNv7UGU"
   },
   "outputs": [],
   "source": [
    "def train_and_eval_regression(model, name):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # 訓練\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            targets = batch['labels'].float().to(device)  # 重要：labels 必須是 float\n",
    "            preds = model(input_ids, attention_mask, numerics)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # 評估\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            targets = batch['labels'].float().to(device)\n",
    "            preds = model(input_ids, attention_mask, numerics)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    mae = mean_absolute_error(all_targets, all_preds)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "    print(f\"\\n{name}  MSE: {mse:.2f} | MAE: {mae:.2f}| R2: {r2:.2f}\")\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "    return all_targets, all_preds\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5c7CQi1v6wwI"
   },
   "outputs": [],
   "source": [
    "class FusionMacBERTRegressor(nn.Module):\n",
    "    def __init__(self, model_name, num_numeric_features, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
    "        self.regressor = nn.Linear(self.bert.config.hidden_size + 64, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        cls_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        num_out = torch.relu(self.numeric_fc(numerics))\n",
    "        combined = torch.cat((cls_output, num_out), dim=1)\n",
    "        return self.regressor(self.dropout(combined)).squeeze(1)  # (batch,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kTncOradCUz8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "rm -rf ~/.cache/huggingface/transformers/hfl__chinese-macbert-base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ptY9GnoACUz8"
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "'''\n",
    "class CustomDatasetRegression(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df['input_ids'].tolist()\n",
    "        self.attention_mask = df['attention_mask'].tolist()\n",
    "        self.labels = df['label'].astype(float).values   # ← 為回歸任務需轉成 float\n",
    "        self.numerics = df[num_cols].astype(float).values  # ← 確保為 float array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'numerics': torch.tensor(self.numerics[idx], dtype=torch.float),  # ← 修正為 float\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.float)       # ← 修正為 float\n",
    "        }\n",
    "'''\n",
    "class CustomDatasetRegression(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df['input_ids'].tolist()\n",
    "        self.attention_mask = df['attention_mask'].tolist()\n",
    "        self.labels = df['target'].astype(float).values\n",
    "        self.numerics = df[num_cols].astype(float).values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'numerics': torch.tensor(self.numerics[idx], dtype=torch.float),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Rlezwnry2K0h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FusionMacBERTRegressor  MSE: 4410895810.43 | MAE: 10676.62 | R²: -0.03\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = CustomDatasetRegression(df)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "\n",
    "class FusionMacBERTRegressor(nn.Module):\n",
    "    def __init__(self, model_name, num_numeric_features, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.numeric_fc = nn.Linear(num_numeric_features, 64)\n",
    "        self.regressor = nn.Linear(self.bert.config.hidden_size + 64, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        cls_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        num_out = torch.relu(self.numeric_fc(numerics))\n",
    "        combined = torch.cat((cls_output, num_out), dim=1)\n",
    "        return self.regressor(self.dropout(combined)).squeeze(1)\n",
    "\n",
    "\n",
    "def train_and_eval_regression(model, name):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            targets = batch['labels'].float().to(device)\n",
    "\n",
    "            preds = model(input_ids, attention_mask, numerics)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            numerics = batch['numerics'].to(device)\n",
    "            targets = batch['labels'].float().to(device)\n",
    "\n",
    "            preds = model(input_ids, attention_mask, numerics)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    mae = mean_absolute_error(all_targets, all_preds)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f\"\\n{name}  MSE: {mse:.2f} | MAE: {mae:.2f} | R²: {r2:.2f}\")\n",
    "    return all_targets, all_preds\n",
    "\n",
    "\n",
    "model = FusionMacBERTRegressor(\"hfl/chinese-macbert-base\", len(num_cols))\n",
    "all_targets, all_preds = train_and_eval_regression(model, \"FusionMacBERTRegressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmUJQfroFBqt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEyKqKjE9oZy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OeqnHopFJuT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0DyH1z7FJm3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnoabF2fFJgR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaucnuMEFCV-"
   },
   "source": [
    "#**下面都是舊的東西而已~~~~**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvuTn85bUTA5"
   },
   "outputs": [],
   "source": [
    "stopwords = set(['的', '了', '在', '是', '和', '也', '與', '有', '為', '等'])\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    words = jieba.cut(text)\n",
    "    words_filtered = [word for word in words if word not in stopwords]\n",
    "    return ' '.join(words_filtered)\n",
    "\n",
    "df['processed_content'] = df['Content'].apply(tokenize_and_remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nzhynZN7IYy"
   },
   "source": [
    "new_article = \"IC 設計大廠聯發科 (2454-TW) 副董事長暨執行長蔡力行今 (26) 日獲頒潘文淵獎，會後受訪表示，聯發科 3 奈米會在台積電 (2330-TW)(TSM-US) 做，且由於先進製程技術相當複雜，不論要採用或更換都非常困難，雙方會持續緊密合作。外界今日提問不論是輝達 (NVDA-US)、蘋果 (AAPL-US) 等都表示尋求多元的晶圓代工方案，蔡力行回應，聯發科在先進製程持續與台積電緊密合作，英特爾 (INTC-US) 則負責 16 奈米蔡力行也強調，聯發科不會只停在採用 4 奈米，也會採用 3 奈米製程，此外，由於電晶體微縮速度趨緩，儘管技術上可行，但不一定符合經濟效益，因此技術也逐步從平面變成 2D、2.5D，甚至 3D 等，先進封裝的重要性比以前增加。至於跟輝達合作，蔡力行重申，雙方合作仍以汽車為主，輝達布局車用比聯發科早，主要著墨在智慧座艙與 ADAS 系統，雙方有很好的配合，其中，輝達主攻高階、聯發科則瞄準中階，雙方正密切合作開會。\"\n",
    "processed_new_article = tokenize_and_remove_stopwords(new_article)\n",
    "\n",
    "# Random Forest 模型訓練與預測\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 使用相同的數據分割方式\n",
    "X_train_rf, X_val_rf, y_train_rf, y_val_rf = train_test_split(tfidf_matrix, y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train_rf, X_val_rf, y_train_rf, y_val_rf = train_test_split(X_train_rf, y_train_rf, test_size=0.1, random_state=42)\n",
    "\n",
    "# 創建隨機森林模型\n",
    "rand_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 訓練隨機森林模型\n",
    "rand_forest_model.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "# 預測\n",
    "y_val_pred_rf = rand_forest_model.predict(X_val_rf)\n",
    "y_test_pred_rf = rand_forest_model.predict(X_test)\n",
    "\n",
    "# 分類報告\n",
    "print(\"驗證集 Validation Classification Report:\")\n",
    "print(classification_report(y_val_rf, y_val_pred_rf))\n",
    "\n",
    "print(\"\\n測試集 Test Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_rf))\n",
    "\n",
    "# 對照表\n",
    "result_df_val_rf = pd.DataFrame({'Actual': y_val_rf, 'Predicted': y_val_pred_rf})\n",
    "result_df_test_rf = pd.DataFrame({'Actual': y_test, 'Predicted': y_test_pred_rf})\n",
    "\n",
    "print(\"驗證集 Validation Result Comparison:\")\n",
    "print(result_df_val_rf)\n",
    "\n",
    "print(\"\\n測試集 Test Result Comparison:\")\n",
    "print(result_df_test_rf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTn9buro9xlD"
   },
   "outputs": [],
   "source": [
    "new_article = \"IC 設計大廠聯發科 (2454-TW) 副董事長暨執行長蔡力行今 (26) 日獲頒潘文淵獎，會後受訪表示，聯發科 3 奈米會在台積電 (2330-TW)(TSM-US) 做，且由於先進製程技術相當複雜，不論要採用或更換都非常困難，雙方會持續緊密合作。外界今日提問不論是輝達 (NVDA-US)、蘋果 (AAPL-US) 等都表示尋求多元的晶圓代工方案，蔡力行回應，聯發科在先進製程持續與台積電緊密合作，英特爾 (INTC-US) 則負責 16 奈米蔡力行也強調，聯發科不會只停在採用 4 奈米，也會採用 3 奈米製程，此外，由於電晶體微縮速度趨緩，儘管技術上可行，但不一定符合經濟效益，因此技術也逐步從平面變成 2D、2.5D，甚至 3D 等，先進封裝的重要性比以前增加。至於跟輝達合作，蔡力行重申，雙方合作仍以汽車為主，輝達布局車用比聯發科早，主要著墨在智慧座艙與 ADAS 系統，雙方有很好的配合，其中，輝達主攻高階、聯發科則瞄準中階，雙方正密切合作開會。\"\n",
    "\n",
    "processed_new_article = tokenize_and_remove_stopwords(new_article)\n",
    "\n",
    "print(processed_new_article)\n",
    "\n",
    "# 將新文章轉換為 TF-IDF 表示形式\n",
    "new_article_tfidf = tfidf_vectorizer.transform([processed_new_article])\n",
    "\n",
    "# 使用投票分類器進行預測\n",
    "predicted_label_ensemble = voting_classifier.predict(new_article_tfidf)\n",
    "\n",
    "print(f\"新文章預測結果: {predicted_label_ensemble}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjsKVnf0-iiU"
   },
   "outputs": [],
   "source": [
    "# 使用 inverse_transform 將預測的數字編碼轉換回原始標籤\n",
    "predicted_label_original = label_encoder.inverse_transform(predicted_label_ensemble)\n",
    "\n",
    "print(f\"新文章預測結果（原始標籤）: {predicted_label_original}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMJI_upxwRgP"
   },
   "source": [
    "## 實際預測（研究）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gbdm9r7_qFq"
   },
   "outputs": [],
   "source": [
    "# 獲取所有標籤對應的編碼\n",
    "all_labels = label_encoder.classes_\n",
    "\n",
    "print(\"所有標籤對應的編碼:\")\n",
    "for label_code, label in enumerate(all_labels):\n",
    "    print(f\"編碼 {label_code}: 標籤 {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTk9I4ZtEPBP"
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "# 定義停用詞\n",
    "stopwords = set(['的', '了', '在', '是', '和', '也', '與', '有', '為', '等'])\n",
    "\n",
    "# 定義分詞並去除停用詞的函數\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    words = jieba.cut(text)  # 使用 jieba 进行分词\n",
    "    words_filtered = [word for word in words if word not in stopwords]  # 去除停用词\n",
    "    return ' '.join(words_filtered)\n",
    "\n",
    "# 將處理後的內容加入 DataFrame 中\n",
    "df['processed_content'] = df['Content'].apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "# 新文章\n",
    "new_article = \"台股守穩季線，週線三連紅。（資料照） 〔財經頻道／綜合報導〕美國CPI略高於市場預期，美股漲勢暫歇，本週以來，台股經過兩日大漲後，今（13）日指數震盪走低，終場下跌43.34點，以16782.57點作收，守住季線關卡，成交量為2986.08億元，週線上漲262點，呈現三連紅，緯創失守百元大關，AI族群普遍都是收黑，電子類股以矽光子、網通等次族群比較有表現，傳產輪動到營建、造紙、百貨等接棒演出。 前10大成交額個股漲多跌少，除了AI族群收黑，其他都是紅盤居多，廣達跌12元，收226元，成交額182.32億元，排名第1；台積電終場漲3元，收553元，成交額171.29億元，排名第2；矽統終場漲2.75元，收47.7元，成交額146.25億元，排名第3；定穎投控漲3.3元，收103元，成交額95.92億元，排名第4；緯創跌3.4元，收99.1元，成交額93.89億元，排名第5。 請繼續往下閱讀...  技嘉跌13.5元，收271元，成交額89.05億元，排名第6；創意收1695元平盤，成交額86.78億元，排名第7；聯發科上漲27元，收842元，成交額81.63億元，排名第8；裕隆漲1.1元，收85.1元，成交額66.51億元，排名第9；材料-KY漲 5元，收1185元，成交額63.34億元，排名第10。 一手掌握經濟脈動點我訂閱自由財經Youtube頻道 不用抽 不用搶 現在用APP看新聞 保證天天中獎點我下載APP按我看活動辦法 相關新聞\"\n",
    "\n",
    "# 處理新文章\n",
    "processed_new_article = tokenize_and_remove_stopwords(new_article)\n",
    "\n",
    "# 輸出處理後的文章\n",
    "print(processed_new_article)\n",
    "\n",
    "# 將新文章轉換為 TF-IDF 表示形式\n",
    "new_article_tfidf = tfidf_vectorizer.transform([processed_new_article])\n",
    "\n",
    "# 使用投票分類器進行預測\n",
    "predicted_label_ensemble = voting_classifier.predict(new_article_tfidf)\n",
    "\n",
    "# 輸出預測結果\n",
    "print(f\"新文章預測結果: {predicted_label_ensemble}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzB1yP1GFFki"
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "# 定義停用詞\n",
    "stopwords = set(['的', '了', '在', '是', '和', '也', '與', '有', '為', '等'])\n",
    "\n",
    "# 定義分詞並去除停用詞的函數\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    words = jieba.cut(text)  # 使用 jieba 进行分词\n",
    "    words_filtered = [word for word in words if word not in stopwords]  # 去除停用词\n",
    "    return ' '.join(words_filtered)\n",
    "\n",
    "# 將處理後的內容加入 DataFrame 中\n",
    "df['processed_content'] = df['Content'].apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "# 新文章\n",
    "new_article = \"小漲\"\n",
    "\n",
    "# 處理新文章\n",
    "processed_new_article = tokenize_and_remove_stopwords(new_article)\n",
    "\n",
    "# 輸出處理後的文章\n",
    "print(processed_new_article)\n",
    "\n",
    "# 將新文章轉換為 TF-IDF 表示形式\n",
    "new_article_tfidf = tfidf_vectorizer.transform([processed_new_article])\n",
    "\n",
    "# 使用投票分類器進行預測\n",
    "predicted_label_ensemble = voting_classifier.predict(new_article_tfidf)\n",
    "\n",
    "# 輸出預測結果\n",
    "print(f\"新文章預測結果: {predicted_label_ensemble}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
